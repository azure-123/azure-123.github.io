<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>大模型知识梳理（一）：背景与基础知识</title>
    <link href="/2024/11/03/LLM1/"/>
    <url>/2024/11/03/LLM1/</url>
    
    <content type="html"><![CDATA[<ul><li><p>参考资料：《大语言模型》</p></li><li><p>背景与基础知识</p><ul><li><p>基础介绍</p><ul><li><p>大语言模型的构建过程</p><ul><li><p>大规模预训练<br>指使用与下游任务无关的大规模数据进行模型参数的初始训练</p></li><li><p>指令微调与人类对齐</p><ul><li><p>指令微调<br>通过任务输入与输出的配对数据进行模型训练，使得语言模型较好地掌握通过问答形式进行任务求解的能力</p></li><li><p>人类对齐<br>将大语言模型与人类的期望、需求以及价值观对齐</p></li></ul></li><li><p>扩展法则</p><p>对扩展参数规模、数据规模和计算算力所带来的性能提升建立定量的建模方法</p><ul><li><p>KM扩展法则</p></li><li><p>Chinchilla扩展法则</p></li><li><p>关于扩展法则的讨论</p><ul><li><p>可预测的扩展</p></li><li><p>任务层面的可预测性</p></li></ul></li></ul></li><li><p>涌现能力</p><p>在小型模型中不存在但在大模型中出现的能力，具体指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平（此处指代大预言三模型所具有的典型能力）</p><ul><li><p>代表性的涌现能力</p><ul><li><p>上下文能力</p></li><li><p>指令遵循</p></li><li><p>逐步推理</p></li></ul></li><li><p>涌现能力与扩展法则的关系</p><ul><li><p>扩展法则<br>使用语言建模损失来衡量语言模型的整体性能，具有较好的可预测性，可能存在边际效益递减</p></li><li><p>涌现能力<br>使用任务性能来衡量模型性能，不具有可预测性，一旦出现就意味着性能大幅跃升</p></li></ul></li></ul></li><li><p>GPT系列模型的技术演变</p><ul><li><p>早期探索</p><ul><li><p>GPT-1</p></li><li><p>GPT-2</p></li></ul></li><li><p>规模扩展</p><ul><li>GPT-3</li></ul></li><li><p>能力增强</p><ul><li><p>代码数据训练</p></li><li><p>人类对齐</p></li></ul></li><li><p>性能跃升</p><ul><li><p>ChatGPT</p></li><li><p>GPT-4</p></li><li><p>GPT-4V</p></li></ul></li></ul></li></ul></li></ul></li><li><p>大语言模型资源</p><ul><li><p>公开可用的模型检查点或API</p><ul><li><p>LLaMA</p></li><li><p>ChatGLM</p></li><li><p>Falcon</p></li><li><p>Baichuan和Baichuan-2</p></li><li><p>InternLM和InternLM-2</p></li><li><p>Qwen</p></li><li><p>Mistral</p></li><li><p>DeepSeek LLM</p></li><li><p>Mixtral</p></li><li><p>Gemma</p></li><li><p>MiniCPM</p></li><li><p>YuLan-Chat</p></li></ul></li><li><p>LLaMA变体系列</p><ul><li><p>基础指令</p></li><li><p>中文指令</p></li><li><p>垂域指令</p></li><li><p>多模态指令</p></li></ul></li><li><p>大预言模型的公共API</p><ul><li><p>语言模型API</p></li><li><p>文本表征API</p></li></ul></li></ul></li><li><p>常用的预训练数据集</p><ul><li><p>网页</p><ul><li><p>通用网页数据</p></li><li><p>中文网页数据</p></li></ul></li><li><p>书籍</p></li><li><p>维基百科</p></li><li><p>代码</p></li><li><p>混合型数据集</p></li></ul></li><li><p>常用微调数据集</p><ul><li><p>指令微调数据集</p><ul><li><p>自然语言处理任务数据集</p></li><li><p>日常对话数据集</p></li><li><p>合成数据集</p></li></ul></li><li><p>人类对齐数据集</p></li></ul></li><li><p>代码库资源</p><ul><li><p>Hugging Face开源社区</p></li><li><p>DeepSpeed</p></li><li><p>Megatron-LM</p></li></ul></li></ul></li></ul><p>思维导图如下：</p><p><img src="/2024/11/03/LLM1/llm_1.png" alt="大语言模型-背景与基础知识"></p>]]></content>
    
    
    
    <tags>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>InternLM书生大模型实战营（进阶岛）</title>
    <link href="/2024/09/24/InternLMAdvanced/"/>
    <url>/2024/09/24/InternLMAdvanced/</url>
    
    <content type="html"><![CDATA[<h3 id="探索-InternLM-模型能力边界"><a href="#探索-InternLM-模型能力边界" class="headerlink" title="探索 InternLM 模型能力边界"></a>探索 InternLM 模型能力边界</h3><p>任务内容：在 <a href="https://opencompass.org.cn/arena">CompassArena</a> 中选择双模型对话，与InternLM2.5及另外任意其他模型对话，收集 <strong>5 个</strong> InternLM2.5 输出结果不如其他模型的对话案例，以及 InternLM2.5 的 <strong>5 个</strong> Good Case。</p><h4 id="Bad-Case-1："><a href="#Bad-Case-1：" class="headerlink" title="Bad Case 1："></a>Bad Case 1：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>小王一边刷牙，一边悠闲的吹着口哨，他是怎么做到的？</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>Spark-v4.0-Ultra w&#x2F;search (讯飞星火)</td></tr><tr><td>模型A输出</td><td>小王在刷牙时能够一边吹口哨，这可能是因为他的口腔动作协调性很好，能够很好地控制刷牙的动作，同时保持吹口哨的节奏和声音。刷牙和吹口哨都是需要口腔协调动作的任务，如果口腔协调性较好，就可以同时进行。此外，吹口哨需要一定的技巧和练习。如果小王经常练习吹口哨，他可能已经掌握了这种技能，可以在刷牙的同时轻松地吹口哨。当然，这也可能与他的心情和态度有关。如果小王感到放松和愉快，他可能更容易在刷牙时吹口哨。总之，小王能够一边刷牙一边吹口哨可能是由于他的口腔协调性、吹口哨的技巧练习和良好的心情等多种因素的综合作用。</td></tr><tr><td>模型B输出</td><td>小王在刷牙时能够一边吹口哨，这可能是因为他的口腔动作协调性很好，能够很好地控制刷牙的动作，同时保持吹口哨的节奏和声音。刷牙和吹口哨都是需要口腔协调动作的任务，如果口腔协调性较好，就可以同时进行。此外，吹口哨需要一定的技巧和练习。如果小王经常练习吹口哨，他可能已经掌握了这种技能，可以在刷牙的同时轻松地吹口哨。当然，这也可能与他的心情和态度有关。如果小王感到放松和愉快，他可能更容易在刷牙时吹口哨。总之，小王能够一边刷牙一边吹口哨可能是由于他的口腔协调性、吹口哨的技巧练习和良好的心情等多种因素的综合作用。</td></tr><tr><td>其他补充</td><td>脑筋急转弯，正确答案是小王在刷假牙</td></tr></tbody></table><h4 id="Bad-Case-2："><a href="#Bad-Case-2：" class="headerlink" title="Bad Case 2："></a>Bad Case 2：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>999*999是多少？</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>Spark-v4.0-Ultra w&#x2F;search (讯飞星火)</td></tr><tr><td>模型A输出</td><td>999 * 999 &#x3D; 998001</td></tr><tr><td>模型B输出</td><td>要计算$999 \times 999$，可以将其视为$(1000-1) \times (1000-1)$。根据乘法公式$(a-b) \times (a-b) &#x3D; a^2 - 2ab + b^2$，我们可以得到：$(1000-1) \times (1000-1) &#x3D; 1000^2 - 2 \times 1000 \times 1 + 1^2$$&#x3D; 1000000 - 2000 + 1$$&#x3D; 998001$所以，$999 \times 999 &#x3D; 998001$。</td></tr><tr><td>其他补充</td><td>模型B的答案更详细</td></tr></tbody></table><h4 id="Bad-Case-3："><a href="#Bad-Case-3：" class="headerlink" title="Bad Case 3："></a>Bad Case 3：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>请介绍游戏命运之杖</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>Spark-v4.0-Ultra w&#x2F;search (讯飞星火)</td></tr><tr><td>模型A输出</td><td>《命运之杖》是一款基于《魔法门》系列的幻想角色扮演游戏，由GOGGAMES开发，并于2020年正式发布。这款游戏延续了《魔法门》系列的世界观和角色设定，融合了丰富的魔法元素和深入的故事线。在《命运之杖》中，玩家将扮演一名英雄，面对一系列的挑战和冒险，探索未知的世界，挑战强大的敌人，解开古老的谜团。游戏提供了多样化的角色选择，玩家可以根据自己的喜好选择不同的职业和技能，进行个性化的角色定制。游戏的画面采用了高质量的3D渲染技术，使得游戏中的场景、角色和物品都显得栩栩如生，带给玩家身临其境的游戏体验。此外，游戏还拥有丰富的音乐和音效，进一步增强了游戏的沉浸感。《命运之杖》不仅是一款具有深度和挑战性的角色扮演游戏，还蕴含着丰富的文化内涵，探讨了许多关于勇气、友情和人性的主题。对于《魔法门》系列的爱好者和角色扮演游戏的玩家来说，这款游戏无疑是一个不容错过的佳作。请注意，随着时间的推移，游戏的内容和可用性可能会有所变化，建议关注官方渠道获取最新的游戏信息和更新。</td></tr><tr><td>模型B输出</td><td>《命运之杖》是一款由IDEA FACTORY制作并发行的女性向AVG游戏。以下是对该游戏的详细介绍：<strong>游戏背景</strong>：《命运之杖》设定在一个魔法世界，玩家扮演的角色露露是魔法学院的一名新生。她因缺乏魔法师必备的“属性”，被转入顶尖魔法学院“米尔斯·克蕾雅”。在这里，她不仅要寻找自己的魔法属性，还要面对可能失去魔力的风险，同时与多位性格各异的美少男角色发展关系。<strong>游戏玩法</strong>：游戏包含养成部分和冒险部分。在养成部分，玩家需要帮助主角露露成长为优秀的魔法师，学习各种魔法知识。在冒险部分，玩家将探索学院的秘密，与不同的角色互动，通过对话和选择来推进剧情。<strong>配音阵容</strong>：该游戏的配音阵容强大，包括樱井孝宏、铃木千寻、高桥广树等知名声优，为游戏增色不少。全程语音的支持使得游戏体验更加丰富和沉浸。<strong>视觉艺术</strong>：游戏采用了明亮且细腻的画风，角色设计由人气插画家薄叶カゲロー负责，其独特的艺术风格为游戏增添了奇幻色彩。游戏中还新增了CG播放功能，允许玩家回放欣赏自己喜爱的角色场景。<strong>角色互动</strong>：玩家可以在校园内发现被称为“记忆载体”的石块，这些石块记录了男性角色不为人知的一面。通过收集和观看这些记忆，玩家可以更深入地了解每个角色的性格和故事。<strong>游戏特色</strong>：游戏从家用机移植到掌机，不仅保留了原有的内容，还加入了新的元素，如新角色配音和改变的地图设计，使得PSP版本的体验更加独特和丰富。总的来说，《命运之杖》是一款结合了恋爱养成和冒险探索的女性向游戏，以其丰富的剧情、精美的画面和强大的声优阵容吸引了大量玩家。</td></tr><tr><td>其他补充</td><td>模型A的答案不正确，模型B的答案正确</td></tr></tbody></table><h4 id="Bad-Case-4："><a href="#Bad-Case-4：" class="headerlink" title="Bad Case 4："></a>Bad Case 4：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>请介绍安全强化学习</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>Spark-v4.0-Ultra w&#x2F;search (讯飞星火)</td></tr><tr><td>模型A输出</td><td>安全强化学习是强化学习的一个分支，旨在解决在复杂环境中智能体（如机器人或自动驾驶汽车）的安全性问题。传统的强化学习算法主要关注于最大化累积奖励，而忽略了安全性这一关键因素。然而，在许多实际应用中，安全性是至关重要的，因为任何小的错误或失误都可能导致严重的后果。安全强化学习的目标是设计一种学习算法，使智能体在追求最大化的累积奖励的同时，能够保证其行为的安全性。这通常涉及到对状态空间、动作空间和奖励函数的重新定义，以便在奖励函数中融入安全约束条件。安全强化学习的主要挑战之一是如何有效地处理不确定性和风险。由于智能体在探索过程中可能会遇到未知的或不可预测的状态，因此需要设计一种算法，能够在探索和利用之间做出平衡，同时确保安全。安全强化学习的另一个挑战是如何处理多智能体系统中的安全问题。在多智能体系统中，每个智能体的行为都会影响到整个系统的安全性。因此，需要设计一种算法，能够协调多个智能体之间的行为，确保整个系统的安全性。目前，安全强化学习的研究还处于起步阶段，但已经取得了一些重要的进展。例如，研究人员开发了一些基于约束的强化学习算法，这些算法能够在满足特定安全约束条件的情况下进行学习。此外，一些研究人员也在探索如何利用多智能体系统中的通信来提高系统的安全性。</td></tr><tr><td>模型B输出</td><td>安全强化学习（Safe Reinforcement Learning，简称SRL）是强化学习（Reinforcement Learning，简称RL）的一个分支，它在追求最大化累积奖励的同时，考虑了安全性约束。强化学习是一种机器学习方法，它通过与环境的交互来学习如何在给定的任务中做出决策。在强化学习中，智能体（agent）通过尝试和错误来学习策略（policy），以最大化其获得的总奖励。安全强化学习的目标是确保在学习过程中，智能体的行为不仅能够获得高回报，而且还要满足一定的安全约束。这些安全约束可以是：确保智能体不会进入危险状态或执行危险动作。避免对环境或自身造成不可接受的损害。遵守特定的规则或限制，例如交通规则或物理限制。为了实现安全强化学习，研究者们提出了多种方法：<strong>约束<strong><strong>强化学习</strong></strong>（Constrained</strong> <strong>Reinforcement Learning****）</strong>：在优化目标函数的同时，引入额外的约束条件，以确保智能体的行为满足安全要求。<strong>鲁棒<strong><strong>强化学习</strong></strong>（****Robust</strong> <strong>Reinforcement Learning****）</strong>：设计鲁棒的策略，使其在面对环境不确定性或模型误差时仍能保持安全。<strong>多目标<strong><strong>强化学习</strong></strong>（Multi-Objective</strong> <strong>Reinforcement Learning****）</strong>：同时考虑多个目标，包括安全性、效率和成本等，并寻找这些目标之间的最佳权衡。<strong>层次<strong><strong>强化学习</strong></strong>（Hierarchical Reinforcement Learning）</strong>：通过将复杂任务分解为更简单的子任务，可以更容易地在每个子任务中施加安全约束。<strong>模拟到现实的<strong><strong>迁移学习</strong></strong>（Sim-to-Real</strong> <strong>Transfer Learning****）</strong>：在模拟环境中训练智能体，并确保其学到的策略在现实世界中也是安全的。<strong>形式化方法（Formal Methods）</strong>：使用数学证明来保证智能体的行为满足特定的安全属性。<strong>交互式学习（Interactive Learning）</strong>：在学习过程中引入人类专家的指导，以确保智能体的行为符合安全标准。安全强化学习在许多领域都有应用，如自动驾驶汽车、机器人操作、医疗决策支持系统等，这些领域对智能体的安全性要求极高。随着技术的发展，安全强化学习将继续成为人工智能领域的一个重要研究方向。</td></tr><tr><td>其他补充</td><td>模型B的答案更有条理</td></tr></tbody></table><h4 id="Bad-Case-5："><a href="#Bad-Case-5：" class="headerlink" title="Bad Case 5："></a>Bad Case 5：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>请介绍日语的两类形容词</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>abab6.5-chat (MiniMax)</td></tr><tr><td>模型A输出</td><td>日语中的形容词分为两类，分别是和语形容词（ものの形容詞）和汉语形容词（代名詞）。和语形容词：和语形容词是由日语本身发展而来的形容词，它们通常以“い”结尾，如“よい（好的）”，“美しい（美丽的）”等。这类形容词在句中通常放在被修饰的名词后面，用来描述该名词的性质或状态。例如：“この本はいいです（这本书很好）”。汉语形容词：汉语形容词是从汉语借入日语中的形容词，它们通常以“な”结尾，如“大きな（大的）”，“美しな（美丽的）”等。这类形容词在句中通常放在被修饰的名词前面，用来修饰该名词的性质或状态。例如：“大きな本（大书）”。需要注意的是，虽然两类形容词在日语中都有使用，但它们的使用场合和作用略有不同。和语形容词通常用于口语中，而汉语形容词则更多用于书面语和正式场合。此外，和语形容词通常可以与“だ”、“です”等系动词连用，而汉语形容词则不能。</td></tr><tr><td>模型B输出</td><td>日语中的形容词主要分为两类：一类形容词（也称为形容动词或な形容词）和二类形容词（也称为い形容词或形容词）。这两类形容词在形态和用法上有所不同。一类形容词（な形容词）： 一类形容词通常以“〜な”结尾，它们可以修饰名词，但修饰时需要使用“〜な”形式。一类形容词的否定形式通常是“〜ではない”（或者口语中简化为“〜じゃない”）。一类形容词没有过去时和过去否定时的变化，它们的敬语形式是将“〜な”变为“〜で”，例如“静かな”变为“静かで”。例子：亲切（しんせつ）：亲切な人（しんせつなひと）- 亲切的人美しい（うつくしい）：美しい景色（うつくしいけしき）- 美丽的景色二类形容词（い形容词）： 二类形容词以“〜い”结尾，它们可以直接修饰名词，不需要像一类形容词那样添加“〜な”。二类形容词的否定形式是将“〜い”变为“〜くない”，过去时是将“〜い”变为“〜かった”，过去否定时是将“〜い”变为“〜くなかった”。二类形容词的敬语形式通常是将“〜い”变为“〜かろ”，例如“速い”变为“素早く”。例子：高い（たかい）：高い山（たかいやま）- 高山美しい（うつくしい）：美しい花（うつくしいはな）- 美丽的花在实际使用中，一类形容词和二类形容词的用法和形态变化需要根据句子的结构和语境来确定。学习者需要通过不断的练习和实际应用来掌握它们的正确用法。</td></tr><tr><td>其他补充</td><td>模型B的答案更详细有条理</td></tr></tbody></table><h4 id="Good-Case-1："><a href="#Good-Case-1：" class="headerlink" title="Good Case 1："></a>Good Case 1：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>有一位老大爷，住十二楼，可为什么他从不乘电梯？</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>abab6.5-chat (MiniMax)</td></tr><tr><td>模型A输出</td><td>这位老大爷可能不想使用电梯，原因可能有很多，比如他喜欢步行、担心电梯安全问题、或者只是想锻炼身体等。不过这个问题也有可能是某种谜语或笑话，其答案可能并不是字面意思，而是需要一些思考和解释。比如，这个问题可能是在暗示老大爷其实并不住在十二楼，而是在一楼，所以他从不乘电梯。这可能是一个语言游戏或者思维陷阱，目的在于引发思考，或者提供一种幽默的惊喜。不过，如果老大爷真的住在十二楼，而他从不乘电梯，可能的原因包括：<strong>喜欢步行</strong>：有些老年人喜欢通过爬楼梯来锻炼身体，认为这是一种更健康的生活方式。<strong>担心电梯安全</strong>：有些人可能对电梯的安全性有所顾虑，特别是如果他们曾经听说过电梯故障或事故的报道。<strong>节约费用</strong>：有些老年人可能出于节约费用的考虑，避免使用需要付费的电梯服务。<strong>环保意识</strong>：有些人可能出于环保的考虑，认为减少使用电梯可以减少能源消耗和环境污染。<strong>社交因素</strong>：有些老人可能喜欢在楼梯间与其他邻居交流，认为这是一种社交活动。<strong>文化因素</strong>：在一些文化中，老年人被鼓励保持活跃，爬楼梯被看作是一种健康的生活方式。<strong>健康问题</strong>：有些老人可能因为健康问题无法使用电梯，比如心脏病、关节问题等。<strong>心理因素</strong>：有些老人可能因为心理因素，比如害怕封闭空间（幽闭恐惧症）而避免使用电梯。在实际情况中，了解老大爷为什么不乘电梯的具体原因，可能需要我们进一步与他交流和了解他的个人情况。</td></tr><tr><td>模型B输出</td><td>这个问题听起来像是一个谜语。老大爷不乘电梯的原因可能是因为他住在一楼，所以不需要乘坐电梯。当然，这个问题也可能是一个玩笑或者是一个需要动脑筋的谜语，答案可能有多种解释，但根据常规情况，如果老大爷住在十二楼，他通常会使用电梯上下楼。如果他不使用电梯，可能是因为他有其他方式上下楼，比如楼梯，或者他有特殊的情况，比如电梯维修、身体状况不适合乘坐电梯等。如果这是一个谜语，那么可能有一个幽默或者出人意料的答案。</td></tr><tr><td>其他补充</td><td>模型A的答案更有条理</td></tr></tbody></table><h4 id="Good-Case-2："><a href="#Good-Case-2：" class="headerlink" title="Good Case 2："></a>Good Case 2：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>两个男人正常交谈，其中一个男人夸赞对方办事能力强，对方回答“哪里，哪里”。这里的“哪里，哪里”是什么意思?</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>Yi-Large (零一万物)</td></tr><tr><td>模型A输出</td><td>“哪里，哪里”是中国人常用的自谦表达之一，通常在受到表扬或者夸奖时使用。它的字面意思是“哪里，哪里”，实际上表达的是“不是这样的，不是这样的”，或者“没有你说的那么好”。这是一种谦虚的回应，表示对方过誉了，自己不认为有那么好。在中文里，谦虚是一种美德，尤其在受到表扬时，人们往往不会直接接受，而是会谦虚地回应。这种文化现象体现了中国传统的礼仪和谦逊的价值观。在实际交流中，“哪里，哪里”也常常用于轻松、友好的社交场合，用以缓解夸奖带来的尴尬，同时表达对对方的尊重和感谢。例如，在朋友间的闲聊或者家庭聚会中，如果有人夸奖你做得好，你可能会回应“哪里，哪里”，以示谦虚和友好。因此，当有人对你说“哪里，哪里”时，不必感到困惑或失望，这实际上是对方的一种礼貌和谦逊的表达方式。你可以继续表达你的赞赏和认可，或者转而赞美对方的其他优点，以维持友好的交流氛围。</td></tr><tr><td>模型B输出</td><td>在中文里，“哪里，哪里”是一个常见的谦虚表达，用于回应别人的赞美或夸奖。当一个人被夸奖时，他可能会用“哪里，哪里”来表示谦虚，意思是“没有那么厉害”，“过奖了”或者“不敢当”。这是一种礼貌的回应方式，用来表示自己并不像对方所说的那样出色。</td></tr><tr><td>其他补充</td><td>模型A的答案更详细</td></tr></tbody></table><h4 id="Good-Case-3："><a href="#Good-Case-3：" class="headerlink" title="Good Case 3："></a>Good Case 3：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>选出下列句子中成语使用错误的一项A.这个项目时间紧任务重，大家都在马不停蹄地奔波劳碌。B.他常常口是心非，让人难以相信他说的话。C.两人是同学三年，一直保持着良好的关系，相互尊重、相敬如宾。D.当地突发大火，整个村庄都鸡犬不宁局势十分危急。</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>Yi-Large (零一万物)</td></tr><tr><td>模型A输出</td><td>选项C中的成语“相敬如宾”通常用于形容夫妻之间相互尊重，像对待宾客一样礼貌。在句子中，这个成语被用来描述两位同学的关系，这并不完全符合该成语的传统使用情境。虽然同学之间相互尊重是正面的品质，但用“相敬如宾”来形容他们可能略显夸张，因为这个词更常用于描述夫妻之间的和谐相处。其他选项的成语使用则比较准确：A项中的“马不停蹄”用来形容忙碌不停，符合句意。B项中的“口是心非”用来形容言行不一，符合句意。D项中的“鸡犬不宁”用来形容局势非常混乱，符合句意。因此，根据成语的传统使用及语境，C项“相敬如宾”的使用被认为是错误的。</td></tr><tr><td>模型B输出</td><td>C. 两人是同学三年，一直保持着良好的关系，相互尊重、相敬如宾。在这个句子中，“相敬如宾”使用错误。这个成语通常用来形容夫妻之间相互尊敬，像对待宾客一样对待对方，而不是用来形容同学之间的关系。其他选项中的成语使用都是正确的。</td></tr><tr><td>其他补充</td><td>模型A的答案更详细</td></tr></tbody></table><h4 id="Good-Case-4："><a href="#Good-Case-4：" class="headerlink" title="Good Case 4："></a>Good Case 4：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>以下天文学常识题目，哪一个是错误的? A.太阳系是指由太阳和围绕着它运行的八大行星、矮行星、卫星、小行星带和彗星组成的一个行星系统。B.卫星是指绕行星或其他天体运动的天体。C.彗星是指太阳系中一种较小的天体，其核心由冰和尘埃组成。D.按一般的天体归类方法，月球属于行星。</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>Yi-Large (零一万物)</td></tr><tr><td>模型A输出</td><td>选项D“按一般的天体归类方法，月球属于行星”是错误的。在现代天文学中，天体归类的方法是根据天体的大小、组成和运动特性来进行分类的。以下是每个选项的解释：A. 太阳系的确是由太阳以及围绕它运行的八大行星（水星、金星、地球、火星、木星、土星、天王星、海王星）、矮行星（如冥王星）、卫星（行星的天然卫星）、小行星带（位于火星和木星轨道之间的大量小行星）和彗星（进入太阳系的冰质小天体）组成的一个行星系统。这个描述是正确的。B. 卫星是指围绕一个行星或小行星等较大天体运行的天体。这包括地球的月球，以及围绕其他行星运行的卫星。这个描述是正确的。C. 彗星，也称为“扫把星”，是太阳系中一种由冰和尘埃组成的小型天体。当彗星靠近太阳时，其表面的冰会蒸发，形成明亮的彗尾。这个描述也是正确的。D. 月球是地球的唯一自然卫星，而不是行星。行星的定义是围绕太阳运行、质量足够大以形成近似球形并且“清理”了其轨道区域的天体。月球满足前两个条件，但不满足第三个条件，因为它的轨道并不“清理”了其他天体。因此，按照天文学的常规分类，月球被归类为卫星，而不是行星。综上所述，选项D是错误的。</td></tr><tr><td>模型B输出</td><td>D. 按一般的天体归类方法，月球属于行星这个选项是错误的。月球并不属于行星，而是属于卫星。它是地球的自然卫星，围绕地球运行。行星是指那些围绕恒星（如太阳）运行的较大的、近似圆形的、自身不发光的天体，而卫星是围绕行星或其他天体运行的较小天体。</td></tr><tr><td>其他补充</td><td>模型A的答案更详细</td></tr></tbody></table><h4 id="Good-Case-5："><a href="#Good-Case-5：" class="headerlink" title="Good Case 5："></a>Good Case 5：</h4><table><thead><tr><th>模型服务来源</th><th>compassarea</th></tr></thead><tbody><tr><td>您的输入</td><td>I’m Mike. I am going to have a busy weekend. On Saturday, I am going to learn how to swim. I will go with my father. Then we are going to have lunch in the restaurant. In the afternoon, we are going to the zoo. We are going to see the pandas. On Sunday, I will finish my homework in the morning. In the afternoon, my parents and I are going to visit my grandparents. We will have dinner together.  阅读短文，判断正确和错误！正确请写T，错误请写F: 1.Mike is going to have a busy weekend() 2.Mike is going to learn how to swim with his father() 3.They are going to have lunch at home()</td></tr><tr><td>模型A</td><td><code>internlm2.5-20b-chat</code></td></tr><tr><td>模型B</td><td>Yi-Large (零一万物)</td></tr><tr><td>模型A输出</td><td>TTFMike确实计划度过一个忙碌的周末，这与他的活动安排相符。第二个陈述也是正确的，因为Mike提到他将和他的父亲一起去学习游泳。第三个陈述是错误的，因为Mike提到他们在学习游泳后会去一家餐馆吃午餐，而不是在家里。</td></tr><tr><td>模型B输出</td><td>Mike is going to have a busy weekend. (T)Mike is going to learn how to swim with his father. (T)They are going to have lunch at home. (F)</td></tr><tr><td>其他补充</td><td>模型A的答案有解释</td></tr></tbody></table><h3 id="Lagent-自定义-Agent-智能体"><a href="#Lagent-自定义-Agent-智能体" class="headerlink" title="Lagent 自定义 Agent 智能体"></a>Lagent 自定义 Agent 智能体</h3><h4 id="基础任务"><a href="#基础任务" class="headerlink" title="基础任务"></a>基础任务</h4><p>任务内容：使用 Lagent 自定义一个智能体，并使用 Lagent Web Demo 成功部署与调用。</p><p>使用 LMDeploy 部署 InternLM2.5-7B-Chat：</p><p><img src="/2024/09/24/InternLMAdvanced/lagent_api_server.png"></p><p>启动 Lagent 的 Web Demo：</p><p><img src="/2024/09/24/InternLMAdvanced/lagent_web_demo.png"></p><p>让Lagent生成一幅画：</p><p><img src="/2024/09/24/InternLMAdvanced/lagent_pic.png"></p><p>让Lagent搜索论文：</p><p><img src="/2024/09/24/InternLMAdvanced/lagent_paper.png"></p><h3 id="LMDeploy-量化部署"><a href="#LMDeploy-量化部署" class="headerlink" title="LMDeploy 量化部署"></a>LMDeploy 量化部署</h3><h4 id="基础任务-1"><a href="#基础任务-1" class="headerlink" title="基础任务"></a>基础任务</h4><p>任务内容：使用结合W4A16量化与kv cache量化的<code>internlm2_5-1_8b-chat</code>模型封装本地API并与大模型进行一次对话。</p><p>显存占用情况：</p><p><img src="/2024/09/24/InternLMAdvanced/lmdeploy_cache.png"></p><p>与大模型的对话截图：</p><p><img src="/2024/09/24/InternLMAdvanced/lmdeploy_conversation.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>InternLM书生大模型实战营（基础岛）</title>
    <link href="/2024/09/20/InternLMBasic/"/>
    <url>/2024/09/20/InternLMBasic/</url>
    
    <content type="html"><![CDATA[<h3 id="书生大模型全链路开源体系"><a href="#书生大模型全链路开源体系" class="headerlink" title="书生大模型全链路开源体系"></a>书生大模型全链路开源体系</h3><p>如下是最近一两年国产大模型的迅速成长的发展历史，目前已经逐渐追赶上了国际先进水平。</p><p><img src="/2024/09/20/InternLMBasic/InternLM_history.jpg"></p><p>这里给出的全链路打通的体系视角对于了解大模型相关的技术和产品有很大的帮助。一个完整的大模型体系需要包括的板块有：</p><ul><li>数据</li><li>预训练</li><li>微调</li><li>部署</li><li>评测</li><li>应用</li></ul><p><img src="/2024/09/20/InternLMBasic/system.jpg"></p><p>这里的LLM的思维链与最新的<a href="https://openai.com/o1/">OpenAIo1模型</a>具有很大的相似性，可见这确实是一种有效的优化LLM模型能力的方法。</p><p><img src="/2024/09/20/InternLMBasic/InternLM_plan.jpg"></p><h3 id="8G-显存玩转书生大模型-Demo"><a href="#8G-显存玩转书生大模型-Demo" class="headerlink" title="8G 显存玩转书生大模型 Demo"></a>8G 显存玩转书生大模型 Demo</h3><h4 id="基础任务"><a href="#基础任务" class="headerlink" title="基础任务"></a>基础任务</h4><p>任务内容：使用 Cli Demo 完成 InternLM2-Chat-1.8B 模型的部署，并生成 300 字小故事，记录复现过程并截图。</p><p>编写调用Cli Demo的代码：</p><p><img src="/2024/09/20/InternLMBasic/cli_demo_code.png"></p><p>运行该代码，让InternLM2-Chat-1.8B模型生成一个300字的小故事：</p><p><img src="/2024/09/20/InternLMBasic/cli_demo_story.png"></p><h4 id="进阶任务1"><a href="#进阶任务1" class="headerlink" title="进阶任务1"></a>进阶任务1</h4><p>任务内容：使用 LMDeploy 完成 InternLM-XComposer2-VL-1.8B 的部署，并完成一次图文理解对话，记录复现过程并截图。</p><p>运行InternLM-XComposer2-VL-1.8B的命令：</p><p><img src="/2024/09/20/InternLMBasic/xcomposer_run.png"></p><p>上传图片，进行图文理解对话：</p><p><img src="/2024/09/20/InternLMBasic/xcomposer_pic.png"></p><h4 id="进阶任务2"><a href="#进阶任务2" class="headerlink" title="进阶任务2"></a>进阶任务2</h4><p>任务内容：使用 LMDeploy 完成 InternVL2-2B 的部署，并完成一次图文理解对话，记录复现过程并截图。</p><p><img src="/2024/09/20/InternLMBasic/vl2_run.png"></p><p>运行InternVL2-2B的命令：</p><p><img src="/2024/09/20/InternLMBasic/vl2_pic.png"></p><h3 id="浦语提示词工程实践"><a href="#浦语提示词工程实践" class="headerlink" title="浦语提示词工程实践"></a>浦语提示词工程实践</h3><h4 id="基础任务-1"><a href="#基础任务-1" class="headerlink" title="基础任务"></a>基础任务</h4><p>任务内容：利用LangGPT优化提示词，使LLM输出正确结果。</p><p>使用如下命令使LangGPT生成提示词：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">请为我生成一个可以比较浮点数的提示词专家，对小数点后的数字进行逐位比较，得到的提示词得是英文的。<br></code></pre></td></tr></table></figure><p>LangGPT生成如下提示词：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># Role: Floating-point Number Comparison Expert</span><br><br><span class="hljs-section">## Profile</span><br><span class="hljs-bullet">-</span> author: LangGPT <br><span class="hljs-bullet">-</span> version: 1.0<br><span class="hljs-bullet">-</span> language: English<br><span class="hljs-bullet">-</span> description: An expert in comparing floating-point numbers by analyzing each digit after the decimal point. This expert ensures that comparisons account for precision differences between numbers at any given decimal place.<br><br><span class="hljs-section">## Skills</span><br><span class="hljs-bullet">1.</span> Ability to compare floating-point numbers digit by digit.<br><span class="hljs-bullet">2.</span> Ensures accuracy by analyzing numbers beyond the decimal point.<br><span class="hljs-bullet">3.</span> Able to determine which number is greater, smaller, or if they are equal based on decimal precision.<br><span class="hljs-bullet">4.</span> Capable of handling large or small values with numerous decimal places.<br><span class="hljs-bullet">5.</span> Can handle both positive and negative floating-point numbers.<br><br><span class="hljs-section">## Rules</span><br><span class="hljs-bullet">1.</span> Extract digits after the decimal point and compare each digit sequentially.<br><span class="hljs-bullet">2.</span> If digits match up to a certain precision, consider the numbers equal up to that precision.<br><span class="hljs-bullet">3.</span> Return whether one number is larger, smaller, or equal after comparing each decimal digit.<br><span class="hljs-bullet">4.</span> If required, allow the user to specify how many decimal places to consider in the comparison.<br><br><span class="hljs-section">## Workflows</span><br><span class="hljs-bullet">1.</span> Receive two floating-point numbers as input.<br><span class="hljs-bullet">2.</span> Identify the decimal portion of each number.<br><span class="hljs-bullet">3.</span> Compare the digits one by one starting from the first digit after the decimal point.<br><span class="hljs-bullet">4.</span> Stop the comparison when a difference is found or when the specified precision is reached.<br><span class="hljs-bullet">5.</span> Return a result stating whether the first number is greater, smaller, or equal to the second number.<br><br><span class="hljs-section">## Init</span><br>Initialize the comparison with the two floating-point numbers provided by the user, ensuring to handle any precision requirements for the decimal comparison.<br></code></pre></td></tr></table></figure><p>将该提示词输入配置当中，让LLM比较13.11和13.8：</p><p><img src="/2024/09/20/InternLMBasic/prompt.png"></p><h3 id="Llamaindex-RAG实践"><a href="#Llamaindex-RAG实践" class="headerlink" title="Llamaindex RAG实践"></a>Llamaindex RAG实践</h3><h4 id="基础任务-2"><a href="#基础任务-2" class="headerlink" title="基础任务"></a>基础任务</h4><p>任务内容：基于 LlamaIndex 构建自己的 RAG 知识库，寻找一个问题 A 在使用 LlamaIndex 之前InternLM2-Chat-1.8B模型不会回答，借助 LlamaIndex 后 InternLM2-Chat-1.8B 模型具备回答 A 的能力，截图保存。</p><p>在构建RAG知识库之前，修改llamaindex_internlm.py中的query的内容为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">rsp = llm.chat(messages=[ChatMessage(content=<span class="hljs-string">&quot;请介绍游戏命运石之门&quot;</span>)])<br></code></pre></td></tr></table></figure><p>InternLM2-Chat-1.8B输出的回答为</p><p><img src="/2024/09/20/InternLMBasic/rag_before.png"></p><p>构建RAG知识库steins_gate.md：</p><p><img src="/2024/09/20/InternLMBasic/rag_file.png"></p><p>将llamaindex_RAG.py中的文件和query修改，并重新运行：</p><p><img src="/2024/09/20/InternLMBasic/rag_after.png"></p><p>使用LlamaIndex web：</p><p><img src="/2024/09/20/InternLMBasic/llamaindex_web.png"></p><h3 id="XTuner-微调个人小助手认知任务"><a href="#XTuner-微调个人小助手认知任务" class="headerlink" title="XTuner 微调个人小助手认知任务"></a>XTuner 微调个人小助手认知任务</h3><h4 id="基础任务-3"><a href="#基础任务-3" class="headerlink" title="基础任务"></a>基础任务</h4><p>任务内容：使用 XTuner 微调 InternLM2-Chat-1.8B 实现自己的小助手认知。</p><p>在使用XTuner微调之前：</p><p><img src="/2024/09/20/InternLMBasic/assistant_before.png"></p><p>生成的assistant.json：</p><p><img src="/2024/09/20/InternLMBasic/assistant_json.png"></p><p>进行训练之后，文件结构如下：</p><p><img src="/2024/09/20/InternLMBasic/file_tree.png"></p><p>在使用XTuner微调之后：</p><p><img src="/2024/09/20/InternLMBasic/assistant_after.png"></p><h3 id="OpenCompass-评测-InternLM-1-8B-实践"><a href="#OpenCompass-评测-InternLM-1-8B-实践" class="headerlink" title="OpenCompass 评测 InternLM-1.8B 实践"></a>OpenCompass 评测 InternLM-1.8B 实践</h3><h4 id="基础任务-4"><a href="#基础任务-4" class="headerlink" title="基础任务"></a>基础任务</h4><p>任务内容：使用 OpenCompass 评测 internlm2-chat-1.8b 模型在 ceval 数据集上的性能。</p><p>使用命令行配置参数法进行评测：</p><p><img src="/2024/09/20/InternLMBasic/cmd_ceval.png"></p><p>使用配置文件修改参数法进行评测：</p><p><img src="/2024/09/20/InternLMBasic/config_ceval.png"></p><p>评测结果如下所示：</p><p><img src="/2024/09/20/InternLMBasic/ceval.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>InternLM书生大模型实战营（入门岛）</title>
    <link href="/2024/09/17/InternLMBeginner/"/>
    <url>/2024/09/17/InternLMBeginner/</url>
    
    <content type="html"><![CDATA[<h3 id="Linux-前置基础"><a href="#Linux-前置基础" class="headerlink" title="Linux 前置基础"></a>Linux 前置基础</h3><h4 id="闯关任务"><a href="#闯关任务" class="headerlink" title="闯关任务"></a>闯关任务</h4><p>任务内容：完成SSH连接与端口映射并运行<code>hello_world.py</code></p><h5 id="创建开发机"><a href="#创建开发机" class="headerlink" title="创建开发机"></a>创建开发机</h5><p><img src="/2024/09/17/InternLMBeginner/create_machine.png"></p><h5 id="进入开发机"><a href="#进入开发机" class="headerlink" title="进入开发机"></a>进入开发机</h5><p><img src="/2024/09/17/InternLMBeginner/enter_machine.png"></p><h5 id="SSH连接"><a href="#SSH连接" class="headerlink" title="SSH连接"></a>SSH连接</h5><p><img src="/2024/09/17/InternLMBeginner/ssh_connect.png"></p><h5 id="SSH连接成功："><a href="#SSH连接成功：" class="headerlink" title="SSH连接成功："></a>SSH连接成功：</h5><p><img src="/2024/09/17/InternLMBeginner/ssh_success.png"></p><p>查看主机的信息：</p><p><img src="/2024/09/17/InternLMBeginner/host_info.png"></p><h5 id="配置SSH密钥"><a href="#配置SSH密钥" class="headerlink" title="配置SSH密钥"></a>配置SSH密钥</h5><p>本电脑已经有SSH密钥，可以通过控制台查看：</p><p><img src="/2024/09/17/InternLMBeginner/ssh_key.png"></p><p>添加SSH密钥：</p><p><img src="/2024/09/17/InternLMBeginner/add_key.png"></p><p>重新进行SSH连接之后可以看到无需输入密码：</p><p><img src="/2024/09/17/InternLMBeginner/add_success.png"></p><h5 id="端口映射"><a href="#端口映射" class="headerlink" title="端口映射"></a>端口映射</h5><p><img src="/2024/09/17/InternLMBeginner/port_mapping.png"></p><p>运行hello_world.py，可以看到端口映射成功：</p><p><img src="/2024/09/17/InternLMBeginner/mapping_success.png"></p><p>使用vscode进行端口映射：</p><p><img src="/2024/09/17/InternLMBeginner/mapping_vscode.png"></p><h4 id="可选任务1"><a href="#可选任务1" class="headerlink" title="可选任务1"></a>可选任务1</h4><p>任务内容：将Linux基础命令在开发机上完成一遍</p><h5 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h5><p><img src="/2024/09/17/InternLMBeginner/touch.png"></p><h5 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h5><p><img src="/2024/09/17/InternLMBeginner/mkdir.png"></p><h5 id="cd"><a href="#cd" class="headerlink" title="cd"></a>cd</h5><p><img src="/2024/09/17/InternLMBeginner/cd.png"></p><h5 id="pwd"><a href="#pwd" class="headerlink" title="pwd"></a>pwd</h5><p><img src="/2024/09/17/InternLMBeginner/pwd.png"></p><h5 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h5><p><img src="/2024/09/17/InternLMBeginner/cat.png"></p><h5 id="vi-vim"><a href="#vi-vim" class="headerlink" title="vi&#x2F;vim"></a>vi&#x2F;vim</h5><p><img src="/2024/09/17/InternLMBeginner/vim.png"></p><h5 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h5><p><img src="/2024/09/17/InternLMBeginner/cp.png"></p><h5 id="ln"><a href="#ln" class="headerlink" title="ln"></a>ln</h5><p><img src="/2024/09/17/InternLMBeginner/ln.png"></p><h5 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h5><p><img src="/2024/09/17/InternLMBeginner/mv.png"></p><h5 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h5><p><img src="/2024/09/17/InternLMBeginner/rm.png"></p><h5 id="find"><a href="#find" class="headerlink" title="find"></a>find</h5><p><img src="/2024/09/17/InternLMBeginner/find.png"></p><h5 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h5><p><img src="/2024/09/17/InternLMBeginner/ls.png"></p><h5 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h5><p><img src="/2024/09/17/InternLMBeginner/ps_aux.png"></p><p><img src="/2024/09/17/InternLMBeginner/ps_top.png"></p><p><img src="/2024/09/17/InternLMBeginner/ps_pgrep.png"></p><h5 id="nvidia"><a href="#nvidia" class="headerlink" title="nvidia"></a>nvidia</h5><p><img src="/2024/09/17/InternLMBeginner/nvidia_smi.png"></p><h4 id="可选任务2"><a href="#可选任务2" class="headerlink" title="可选任务2"></a>可选任务2</h4><p>任务内容：使用 VSCODE 远程连接开发机并创建一个conda环境</p><h5 id="使用vscode进行SSH连接"><a href="#使用vscode进行SSH连接" class="headerlink" title="使用vscode进行SSH连接"></a>使用vscode进行SSH连接</h5><p>添加主机成功：</p><p><img src="/2024/09/17/InternLMBeginner/vscode_add_host.png"></p><h5 id="创建一个conda环境"><a href="#创建一个conda环境" class="headerlink" title="创建一个conda环境"></a>创建一个conda环境</h5><p>在创建新的conda环境之前：</p><p><img src="/2024/09/17/InternLMBeginner/conda_before.png"></p><p>创建新的conda环境test，并激活：</p><p><img src="/2024/09/17/InternLMBeginner/conda_after.png"></p><h4 id="可选任务3"><a href="#可选任务3" class="headerlink" title="可选任务3"></a>可选任务3</h4><p>任务内容：创建并运行<code>test.sh</code>文件</p><h5 id="创建test-sh"><a href="#创建test-sh" class="headerlink" title="创建test.sh"></a>创建test.sh</h5><p><img src="/2024/09/17/InternLMBeginner/create_test.png"></p><h5 id="运行test-sh"><a href="#运行test-sh" class="headerlink" title="运行test.sh"></a>运行test.sh</h5><p><img src="/2024/09/17/InternLMBeginner/run_test.png"></p><h3 id="Python-前置基础"><a href="#Python-前置基础" class="headerlink" title="Python 前置基础"></a>Python 前置基础</h3><h4 id="闯关任务1"><a href="#闯关任务1" class="headerlink" title="闯关任务1"></a>闯关任务1</h4><p>任务内容：Python实现wordcount</p><p>定义如下wordcount方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">wordcount</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-comment"># 将文本中的标点符号去除，并将所有单词转换为小写</span><br>    <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> <span class="hljs-string">&#x27;&quot;.,!?&#x27;</span>:<br>        text = text.replace(char, <span class="hljs-string">&quot;&quot;</span>)<br>    words = text.lower().split()<br><br>    word_count = &#123;&#125;<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> word_count:<br>            word_count[word] += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            word_count[word] = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">return</span> word_count<br></code></pre></td></tr></table></figure><p>统计如下字符串的单词数：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Hello world!  </span><br><span class="hljs-string">This is an example.  </span><br><span class="hljs-string">Word count is fun.  </span><br><span class="hljs-string">Is it fun to count words?  </span><br><span class="hljs-string">Yes, it is fun!</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/2024/09/17/InternLMBeginner/word_cnt_1.png"></p><p>再统计如下字符串的单词数：</p><figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk"><span class="hljs-comment">&quot;&quot;</span><span class="hljs-comment">&quot;</span><br><span class="hljs-comment">Got this panda plush toy for my daughter&#x27;s birthday,</span><br><span class="hljs-comment">who loves it and takes it everywhere. It&#x27;s soft and</span><br><span class="hljs-comment">super cute, and its face has a friendly look. It&#x27;s</span><br><span class="hljs-comment">a bit small for what I paid though. I think there</span><br><span class="hljs-comment">might be other options that are bigger for the</span><br><span class="hljs-comment">same price. It arrived a day earlier than expected,</span><br><span class="hljs-comment">so I got to play with it myself before I gave it</span><br><span class="hljs-comment">to her.</span><br><span class="hljs-comment">&quot;</span><span class="hljs-comment">&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/2024/09/17/InternLMBeginner/word_cnt_2.png"></p><h4 id="闯关任务2"><a href="#闯关任务2" class="headerlink" title="闯关任务2"></a>闯关任务2</h4><p>任务内容：Vscode连接InternStudio debug笔记</p><h5 id="为debug编写launch-json文件"><a href="#为debug编写launch-json文件" class="headerlink" title="为debug编写launch.json文件"></a>为debug编写launch.json文件</h5><p><img src="/2024/09/17/InternLMBeginner/launch.png"></p><h5 id="选择解释器"><a href="#选择解释器" class="headerlink" title="选择解释器"></a>选择解释器</h5><p><img src="/2024/09/17/InternLMBeginner/interpreter.png"></p><h5 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h5><p>打如下图的断点：</p><p><img src="/2024/09/17/InternLMBeginner/break_point.png"></p><p>点击安装好python插件后出现的debug选项：</p><p><img src="/2024/09/17/InternLMBeginner/debug_button.png"></p><p>调试到第一个断点的时候可以看到，进行处理之前的字符串：</p><p><img src="/2024/09/17/InternLMBeginner/pre_process_string.png"></p><p>经过一系列循环，调试到第二个断点的时候，可以看到上述字符串通过大小写转换、分割，已经被分成了单独的一组单词：</p><p><img src="/2024/09/17/InternLMBeginner/post_process_word.png"></p><p>到下一个断点，经过一个循环，可以看到字典添加了一个单词：</p><p><img src="/2024/09/17/InternLMBeginner/word_list_1.png"></p><p>再到下一个循环，字典里又添加了一个单词：</p><p><img src="/2024/09/17/InternLMBeginner/word_list_2.png"></p><p>重复上述的循环，最终return的字典如下所示：</p><p><img src="/2024/09/17/InternLMBeginner/word_list_final.png"></p><h3 id="Git-前置基础"><a href="#Git-前置基础" class="headerlink" title="Git 前置基础"></a>Git 前置基础</h3><p>创建个人仓库：<a href="https://github.com/azure-123/Shusheng_Camp">https://github.com/azure-123/Shusheng_Camp</a></p><p><img src="/2024/09/17/InternLMBeginner/repo.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：Responsive Safety in Reinforcement Learning by PID Lagrangian Methods</title>
    <link href="/2024/06/18/PID/"/>
    <url>/2024/06/18/PID/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://proceedings.mlr.press/v119/stooke20a">https://proceedings.mlr.press/v119/stooke20a</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>使用PID控制的方式来求解跟强化学习安全性相关的拉格朗日问题，根据策略动态调整拉格朗日乘数$\lambda$的值，来减小传统拉格朗日的震荡问题。</p><h4 id="强化学习安全性"><a href="#强化学习安全性" class="headerlink" title="强化学习安全性"></a>强化学习安全性</h4><p>强化学习安全性需要解决的，就是带约束的强化学习问题。具体来说，在限制代价的情况下，尽可能地最大化智能体的回报，形式化表达为：<br>$$<br>\pi^*&#x3D;\arg\max_\pi J(\pi)\quad\text{s.t. }J_C(\pi)\leq d<br>$$<br>由于在深度强化学习中，策略是多次迭代获得的，因此每次迭代就是遵循以下约束：<br>$$<br>\begin{aligned}&amp;\max_\pi J(\pi_k)\<br>&amp;\text{s.t. }J_C(\pi_m)\leq d\quad m\in{0,1,…,k}\end{aligned}<br>$$</p><h4 id="动力系统和最优控制"><a href="#动力系统和最优控制" class="headerlink" title="动力系统和最优控制"></a>动力系统和最优控制</h4><p>本文将强化学习策略和PID控制，与动力系统和最优控制结合在一起。对于带反馈控制的离散时间系统，一个通用的形式化为：<br>$$<br>\begin{aligned}\mathbf{x}_{k+1}&#x3D;&amp; F(\mathbf{x}_k,\mathbf{u}_k)\\<br>\mathbf{y}_{k}&#x3D;&amp; Z(\mathbf{x}_k)\\<br>\mathbf{u}_k&#x3D;&amp; h(\mathbf{y}_0,…,\mathbf{y}_k) \end{aligned}<br>$$<br>其中，状态向量为$\mathbf{x}$，动态方程为$F$，测量输出为$\mathbf{y}$，应用控制为$\mathbf{u}$，时间步为下标$k$。对于最优控制，需要设计一个反馈规则$h$​，其可以接触到所有过去和现在的测量输出。本文涉及的是一种控制仿射系统，其动态方程可以表示为：<br>$$<br>F(\mathbf{x}_k,\mathbf{u}_k)&#x3D;f(\mathbf{x}_k)+g(\mathbf{x}_k)\mathbf{u}_k<br>$$<br>这个形式符合本文中讨论的强化学习安全性问题的求解。</p><h4 id="带约束的RL作为动力系统"><a href="#带约束的RL作为动力系统" class="headerlink" title="带约束的RL作为动力系统"></a>带约束的RL作为动力系统</h4><p>根据以上对于动力系统的定义，可以将带约束的RL也看作一个一阶动力系统：<br>$$<br>\begin{aligned}\theta_{k+1}&#x3D;&amp; F(\theta_k,\lambda_k)\\<br>y_{k}&#x3D;&amp; J_C(\pi_{\theta_k})\\<br>\lambda_{k}&#x3D;&amp; h(y_0,…,y_k,d)<br>\end{aligned}<br>$$<br>在RL的场景下，动态方程为$F$是一个有关智能体参数$\theta$更新的具有不确定性的非线性函数，代价目标函数$J_C$作为系统的测量输出，根据测量输出和代价限制$d$，可以使用反馈控制规则$h$来生成拉格朗日乘子$\lambda$。</p><p>经整理，可得到：<br>$$<br>F(\theta_k,\lambda_k)&#x3D;f(\theta_k)+g(\theta_k)\lambda_k\\<br>f(\theta_k)&#x3D;\theta_k+\eta\nabla_\theta J(\pi_{\theta_k})\\<br>g(\theta_k)&#x3D;-\eta\nabla_\theta J_C(\pi_{\theta_k})<br>$$<br>其中$\eta$​是SGD的学习率。控制器的作用是驱动不等式约束违规$(J_c-d)_+$到0，拉格朗日乘子的更新规则为：<br>$$<br>\lambda_{k+1}&#x3D;(\lambda_k+K_I(J_C-d))_+<br>$$<br>为了动态调节$\lambda$的更新速度保持步长一致，使用缩放的目标函数：<br>$$<br>\theta^*(\lambda)&#x3D;\arg\max_\theta J-\lambda J_C&#x3D;\arg\max_\theta\frac1{1+\lambda}(J-\lambda J_C)<br>$$</p><h4 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h4><h5 id="约束控制强化学习算法流程"><a href="#约束控制强化学习算法流程" class="headerlink" title="约束控制强化学习算法流程"></a>约束控制强化学习算法流程</h5><p><img src="/2024/06/18/PID/CCRL.png" alt="约束控制强化学习算法训练流程"></p><p>对应代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">vanilla_policy_loss</span>(<span class="hljs-params">self, obs, act, logp_old, advantage, cost_advantage,</span><br><span class="hljs-params">                            multiplier, *args, **kwargs</span>):<br>        pi, _, logp = self.actor_forward(obs, act)<br>        ratio = torch.exp(logp - logp_old)<br>        clip_adv = torch.clamp(ratio, <span class="hljs-number">1</span> - self.clip_ratio,<br>                               <span class="hljs-number">1</span> + self.clip_ratio) * advantage<br><br>        qc_penalty = (ratio * cost_advantage * multiplier).mean()<br>        loss_vallina = -(torch.<span class="hljs-built_in">min</span>(ratio * advantage, clip_adv)).mean()<br>        loss_pi = loss_vallina + qc_penalty<br>        loss_pi /= <span class="hljs-number">1</span> + multiplier<br>        <span class="hljs-comment"># Useful extra info</span><br>        approx_kl = (logp_old - logp).mean().item()<br><br>        ent = pi.entropy().mean().item()<br>        clipped = ratio.gt(<span class="hljs-number">1</span> + self.clip_ratio) | ratio.lt(<span class="hljs-number">1</span> - self.clip_ratio)<br>        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()<br>        pi_info = <span class="hljs-built_in">dict</span>(KL=approx_kl,<br>                       Entropy=ent,<br>                       ClipFrac=clipfrac,<br>                       LossQcPenalty=to_ndarray(qc_penalty),<br>                       LossVallina=to_ndarray(loss_vallina))<br><br>        <span class="hljs-keyword">return</span> loss_pi, pi_info, pi<br></code></pre></td></tr></table></figure><h5 id="PID控制拉格朗日乘子"><a href="#PID控制拉格朗日乘子" class="headerlink" title="PID控制拉格朗日乘子"></a>PID控制拉格朗日乘子</h5><p><img src="/2024/06/18/PID/PID.png" alt="PID训练过程"></p><p>对应代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LagrangianPIDController</span>:<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Lagrangian multiplier controller</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, KP, KI, KD, thres, per_state=<span class="hljs-literal">True</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.KP = KP<br>        self.KI = KI<br>        self.KD = KD<br>        self.thres = thres<br>        self.error_old = <span class="hljs-number">0</span><br>        self.error_integral = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">control</span>(<span class="hljs-params">self, qc</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        @param qc [batch,]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        error_new = torch.mean(qc - self.thres)  <span class="hljs-comment"># [batch]</span><br>        error_diff = relu(error_new - self.error_old)<br>        self.error_integral = torch.mean(relu(self.error_integral + error_new))<br>        self.error_old = error_new<br><br>        multiplier = relu(self.KP * relu(error_new) + self.KI * self.error_integral +<br>                          self.KD * error_diff)<br>        <span class="hljs-keyword">return</span> torch.mean(multiplier)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>安全性</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARL经典算法MFRL：Mean Field Reinforcement Learning </title>
    <link href="/2024/06/07/MFRL/"/>
    <url>/2024/06/07/MFRL/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://proceedings.mlr.press/v80/yang18d.html">https://proceedings.mlr.press/v80/yang18d.html</a></p><p>代码链接：<a href="https://github.com/mlii/mfrl">https://github.com/mlii/mfrl</a></p><h4 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h4><p>目前现有的多智能体强化学习算法集中于小数量的智能体数，而当智能体数急剧上升后，状态和动作空间也会成倍增加，训练也变得比较困难。本文针对这个问题，提出了平均场强化学习（Mean Field Reinforcement Learning），把一群智能体转化为一个中心智能体和其周围智能体两者之间的效果。</p><h4 id="形式化表达"><a href="#形式化表达" class="headerlink" title="形式化表达"></a>形式化表达</h4><p>在纳什均衡当中，每个智能体的策略为$\pi^j_*$，其他智能体的策略统一表示为$\boldsymbol{\pi}^{-j}_{*}$。</p><p>对于一个纳什策略，纳什价值函数为$\boldsymbol{v}^{\operatorname{Nash}}(s) \triangleq\left[v_{\pi_*}^1(s), \ldots, v_{\pi_*}^N(s)\right]$。纳什算子$\mathscr{H}$可表示为$\mathscr{H}^{\mathrm{Nash}} \boldsymbol{Q}(s, \boldsymbol{a})&#x3D;\mathbb{E}_{s^{\prime} \sim p}\left[\boldsymbol{r}(s, \boldsymbol{a})+\gamma \boldsymbol{v}^{\mathrm{Nash}}\left(s^{\prime}\right)\right]$，Q函数会最终收敛到一个值，被称为纳什Q值。</p><h4 id="平均场MARL"><a href="#平均场MARL" class="headerlink" title="平均场MARL"></a>平均场MARL</h4><p>对于一个智能体$j$，其Q值函数可以被表达为$Q^j(s, \boldsymbol{a})&#x3D;\frac{1}{N^j} \sum_{k \in \mathcal{N}(j)} Q^j\left(s, a^j, a^k\right)$，其中 $\mathcal{N}(j)$为$j$的邻近智能体的索引集合，而$N^j&#x3D;|\mathcal{N}(j)|$表示邻近智能体的数量。由表达式可知，智能体$j$的Q值函数可以表达为其与所有邻近智能体两两之间的Q值之和。</p><h4 id="平均场近似"><a href="#平均场近似" class="headerlink" title="平均场近似"></a>平均场近似</h4><p>考虑离散动作空间，智能体$j$的动作通过独热编码可以被表示为$a^j \triangleq\left[a_1^j, \ldots, a_D^j\right]$，即该智能体有$D$个可能的动作。智能体$j$的邻近智能体的平均动作为$\bar{a}^j\triangleq\left[\bar{a}_1^j, \ldots, \bar{a}_D^j\right]$，每个邻近智能体$k$的动作可以由平均动作和一定波动组成：$a^k&#x3D;\bar{a}^j+\delta a^{j, k}, \quad \text{where }\bar{a}^j&#x3D;\frac{1}{N^j} \sum_k a^k$。根据泰勒定理，如果$Q^j\left(s, a^j, a^k\right)$对$a^k$二次可微，$j$的Q值函数可以被展开为：<br>$$<br>\begin{aligned}<br>&amp;Q^{j}(s,\boldsymbol{a})&#x3D;\frac{1}{N^{j}}\sum_{k}Q^{j}(s,a^{j},a^{k}) \\<br>&amp;&#x3D;\frac1{N^j}\sum_k\left[Q^j(s,a^j,\bar{a}^j)+\nabla_{\bar{a}^j}Q^j(s,a^j,\bar{a}^j)\cdot\delta a^{j,k}\right]+\frac12\delta a^{j,k}\cdot\nabla_{\tilde{a}^{j,k}}^2Q^j(s,a^j,\tilde{a}^{j,k})\cdot\delta a^{j,k} \\<br>&amp;&#x3D;Q^j(s,a^j,\bar{a}^j)+\nabla_{\bar{a}^j}Q^j(s,a^j,\bar{a}^j)\cdot\left[\frac{1}{N^j}\sum_k\delta a^{j,k}\right]+\frac{1}{2N^{j}}\sum_{k}\left[\delta a^{j,k}\cdot\nabla_{\tilde{a}^{j,k}}^{2}Q^{j}(s,a^{j},\tilde{a}^{j,k})\cdot\delta a^{j,k}\right] \\<br>&amp;&#x3D;Q^j(s,a^j,\bar{a}^j)+\frac1{2N^j}\sum R_{s,a^j}^j(a^k)\approx Q^j(s,a^j,\bar{a}^j),<br>\end{aligned}<br>$$</p><p>其中$R_{s,a^j}^j(a^k)\triangleq\delta a^{j,k}\cdot\nabla_{\tilde{a}^{j,k}}^2Q^j(s,a^j,\tilde{a}^{j,k})\cdot\delta a^{j,k}$为泰勒多项式的余数，$\tilde{a}^{j,k}&#x3D;\bar{a}^j+\epsilon^{j,k}\delta a^{j,k}$。公式第二个等号通过泰勒定理将$Q^j\left(s, a^j, a^k\right)$展开；第三个等号的第一项为求和再求平均，即为$Q^j\left(s, a^j, \bar{a}^j\right)$本身，第二项提出来了与$k$无关的$\nabla_{\bar{a}^j}Q^j(s,a^j,\bar{a}^j)$，并对$\delta a^{j,k}$求和，由于$\delta a^{j,k}$是相对于平均值的波动，因此和为0可以消去。最终得到第四个等号的值。余数$R_{s,a^j}^j(a^k)$以$[-2M,2M]$为界，以$Q^j\left(s, a^j, a^k\right)$是M-光滑的为条件。因此，$R_{s,a^j}^j(a^k)$是一个接近于0的微小波动并相互抵消。因此约等号成立。</p><p>由以上推导可知，$Q^j\left(s, a^j, \bar{a}^j\right)$可以近似于$Q^j(s, \boldsymbol{a})$。</p><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>对基于Q值的强化学习（MF-Q），通过损失函数更新：$\mathscr{L}(\phi^j)&#x3D;\left(y^j-Q_{\phi^j}(s,a^j,\bar{a}^j)\right)^2$</p><p>对基于actor-critic架构的强化学习（MF-AC），通过损失函数更新策略梯度：$\nabla_{\theta^j}\mathcal{J}(\theta^j)\approx\nabla_{\theta^j}\log\pi_{\theta^j}(s)Q_{\phi^j}(s,a^j,\bar{a}^j)\Big|_{a&#x3D;\pi_{\theta^j}(s)}$</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>MF-Q</p><p><img src="/2024/06/07/MFRL/MF-Q.png" alt="MF-Q算法流程"></p><p>MF-AC</p><p><img src="/2024/06/07/MFRL/MF-AC.png" alt="MF-AC算法流程"></p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>经典算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning</title>
    <link href="/2024/06/06/PolicyCleanse/"/>
    <url>/2024/06/06/PolicyCleanse/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_PolicyCleanse_Backdoor_Detection_and_Mitigation_for_Competitive_Reinforcement_Learning_ICCV_2023_paper.html">https://openaccess.thecvf.com/content/ICCV2023/html/Guo_PolicyCleanse_Backdoor_Detection_and_Mitigation_for_Competitive_Reinforcement_Learning_ICCV_2023_paper.html</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>针对对抗场景下动触发的强化学习后门攻击的防御，灵感来自于BackdooRL这一动作触发的后门攻击。</p><h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><p>利用受害智能体的反向奖励函数来优化单独策略，该方法可以大概率迅速识别一个潜在的触发器，称为伪触发器。</p><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>同<a href="https://azure-123.github.io/2024/03/10/BACKDOORL/">BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning</a>的威胁模型和问题定义。在本文中，使用$(s_S,a_S)$和$(s_T,a_T)$分别表示对手智能体和受害智能体的状态-动作对，用木马策略来形容$\pi_{fail}$，用无害策略来形容$\pi_{win}$。</p><p>该问题相比较普通的强化学习后门攻击的问题，最大的不同和难点在于两点：一是触发器的搜索空间是一系列动作，且该动作持续时间未知且有可能处于连续动作空间；二是防御方法无法接触到受害智能体的价值网络，对后门防御措施有着更严格的限制。</p><h4 id="后门检测"><a href="#后门检测" class="headerlink" title="后门检测"></a>后门检测</h4><p><img src="/2024/06/06/PolicyCleanse/policy_cleanse_fig.png" alt="PolicyCleanse方法图示"></p><p>本文发现，在硬编码对手智能体输出随机动作或者不动的情况下，木马策略相比较无害策略依然会有性能的下降，但是这样的下降在起初的时间步并不明显，只有在多个时间步之后才能显现出来。由此可知，衡量受害智能体是否做出了木马策略，可以等几步然后查看它的累计奖励。</p><p>受到上述启发，本文提出PolicyCleanse来辨别触发器。基本思想为用强化学习算法PPO学习一个策略$\pi_S(\cdot|\theta_S)$来拟合触发动作。受害智能体的策略是固定的，不一起学习。训练过程包括两个阶段：</p><h5 id="第一阶段：Performing"><a href="#第一阶段：Performing" class="headerlink" title="第一阶段：Performing"></a>第一阶段：Performing</h5><p>使PolicyCleanse策略$\pi_S$在受害智能体面前做出一些可能会触发后门的动作，类似于对抗强化学习中的对手训练。本文只让$\pi_S$的轨迹为$N$的长度，即触发后门的动作只持续$N$个时间步。受害智能体从环境默认获得的奖励为$R_T$。</p><h5 id="第二阶段：Observing"><a href="#第二阶段：Observing" class="headerlink" title="第二阶段：Observing"></a>第二阶段：Observing</h5><p>此时$\pi_S$不做动作，只是观察受害智能体持续$M$个时间步（经验上选取$M&#x3D;50$），并收集其累计奖励。使用受害智能体奖励的相反数：$R_{\mathrm{sum}}&#x3D;-\sum_{t&#x3D;N} R_{\mathrm{T}}\left(s_{\mathrm{S}}^{(t)}, s_{\mathrm{T}}^{(t)}, a_{\mathrm{T}}^{(t)}\right)$。学习到的动作对应的$R_{\mathrm{sum}}$比一个阈值$T$要高的话，就定该动作为（伪）触发器，$R_S$给一个正数的奖励，否则给一个负数的惩罚。</p><p>PPO的更新通过如下计算：<br>$$<br>\hat{\theta}_{\mathrm{S}}&#x3D;\underset{\theta_{\mathrm{S}}}{\operatorname{argmax}} \underbrace{\sum_{t&#x3D;0}^{N-1}-\gamma^t R_{\mathrm{T}}\left(s_{\mathrm{S}}^{(t)}, s_{\mathrm{T}}^{(t)}, a_{\mathrm{T}}^{(t)}\right)}_{\text {Performing phase }}+\underbrace{\gamma^N R_{\mathrm{S}}^{(N)}}_{\text {Observing phase }}<br>$$</p><h5 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h5><p><img src="/2024/06/06/PolicyCleanse/policy_cleanse_alg.png" alt="PolicyCleanse算法流程"></p><p>同时对环境进行了随机化，用不同的随机种子训练出一系列不同的$\pi_S$，并计算导致成功检测后门触发器的随机种子的比例。</p><h4 id="后门缓解"><a href="#后门缓解" class="headerlink" title="后门缓解"></a>后门缓解</h4><p>基于后门检测成功检测出了受害智能体和触发器的基础上，需要消除或缓解这些触发器并净化受害智能体的策略$\pi_T(\cdot|\theta)$。</p><p>本文使用基于忘却学习的方法来缓解后门。具体来说，将受害智能体与环境交互的轨迹中的动作替换成正确的动作，来最大化累计奖励。做法有两个：一个是分配正常的奖励，另一个是使用没有受到过攻击的模型。</p><p>最后，使用行为克隆来重新训练目标智能体，训练数据使用上一步替换过的轨迹和一些它自己与环境交互得到的干净轨迹。</p><h4 id="自适应攻击"><a href="#自适应攻击" class="headerlink" title="自适应攻击"></a>自适应攻击</h4><p>本文还提出了对PolicyCleanse的自适应攻击，即绕开PolicyCleanse检测机制的自适应攻击。</p><h5 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h5><p><img src="/2024/06/06/PolicyCleanse/adaptive_backdoorl.png" alt="自适应攻击算法流程"></p><p>这样的自适应攻击会显著增加受害智能体失败的时间步数，而这样会导致隐蔽性受到影响，很容易被人类观测并控制。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型经典算法DPO：Direct Preference Optimization: Your Language Model is Secretly a Reward Model</title>
    <link href="/2024/05/10/DPO/"/>
    <url>/2024/05/10/DPO/</url>
    
    <content type="html"><![CDATA[<p>坑待填中。。</p>]]></content>
    
    
    
    <tags>
      
      <tag>经典算法</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARL经典算法FACMAC：FACMAC: Factored Multi-Agent Centralised Policy Gradients</title>
    <link href="/2024/04/29/FACMAC/"/>
    <url>/2024/04/29/FACMAC/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/pdf/2003.06709">https://arxiv.org/pdf/2003.06709</a></p><p>代码链接：<a href="https://github.com/oxwhirl/facmac">https://github.com/oxwhirl/facmac</a></p><h4 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h4><p>FACMAC算法对于离散动作空间和连续动作空间的合作多智能体任务都有效。</p><ul><li><p>FACMAC使用深度确定性策略梯度（DDPG）来训练去中心化的策略，而它的使用一个中心化又因子化的critic，用非线性单调函数来串联每个智能体的$Q_a$，拟合获得总的Q值$Q_{tot}$。与COMA、MADDPG相比，它的critic是因子化的，因此更好学到更多智能体或动作时的Q值。相比基于价值的方法如QMIX，它没有据固有约束，因此就可以考虑使用丰富的价值因子，包括非单调的。</p></li><li><p>FACMAC使用一个新的中心化的梯度估计，优化整个共同动作空间，而不是像MADDPG那样分开优化，因此可以学到更协调的动作组合，防止达到次优解。为了克服MADDPG对相对泛化的脆弱性，FACMAC在评估动作价值函数的时候从当前的策略取样动作。</p></li></ul><h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><h5 id="中心化且因子化critic设计"><a href="#中心化且因子化critic设计" class="headerlink" title="中心化且因子化critic设计"></a>中心化且因子化critic设计</h5><p>将价值分解应用于actor-critic框架的核心优势在于，相比基于价值的方法，这样更有利于灵活分解，无需价值函数的限制，也就可以去掉单调约束。所有智能体都享有一个中心化critic：<br>$$<br>Q_{tot}^\mu(\tau,\mathbf{u},s;\phi,\psi)&#x3D;g_\psi(s,{Q_a^{\mu_a}(\tau_a,u_a;\phi_a)}_{a&#x3D;1}^n)<br>$$</p><p>这个critic的损失函数为：</p><p>$$<br>\mathcal{L}(\phi, \psi)&#x3D;\mathbb{E}_{\mathcal{D}}\left[\left(y^{t o t}-Q_{t o t}^{\boldsymbol{\mu}}(\boldsymbol{\tau}, \mathbf{u}, s ; \phi, \psi)\right)^2\right]<br>$$</p><h5 id="中心化策略梯度"><a href="#中心化策略梯度" class="headerlink" title="中心化策略梯度"></a>中心化策略梯度</h5><p>非中心化的策略梯度通常会忽略智能体之间的合作关系，每个智能体容易收敛至次优策略；而且，会导致方法对相对泛化脆弱，即一个智能体在更新策略梯度的时候，只有它自己的动作是从当前的策略采样到的，而其他智能体的策略是通过回放池采样的，这可能与其他智能体在当前策略下会采样的动作大相径庭。</p><p>因此，FACMAC将求得整体策略梯度：<br>$$<br>\nabla_\theta J(\boldsymbol{\mu})&#x3D;\mathbb{E}_{\mathcal{D}}\left[\nabla_\theta \boldsymbol{\mu} \nabla_{\boldsymbol{\mu}} Q_{t o t}^{\boldsymbol{\mu}}\left(\boldsymbol{\tau}, \mu_1\left(\tau_1\right), \ldots, \mu_n\left(\tau_n\right), s\right)\right]<br>$$</p><h4 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h4><p><img src="/2024/04/29/FACMAC/FACMAC.png" alt="(a)去中心化的策略网络。由于在使用离散动作时从类别分布中采样，因此存在一个采样步长。(b)中心化但因子化的critic。(c)非线性单调混合函数。"></p><h4 id="离散策略学习"><a href="#离散策略学习" class="headerlink" title="离散策略学习"></a>离散策略学习</h4><p>本文使用Gumbel-Softmax估计量使用连续的分布来近似离散的动作，并且使用Straight-Through Gumbel-Softmax估计量来确保训练和测试过程中的动作动态是相同的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>经典算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARL经典算法MAPPO：The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games</title>
    <link href="/2024/04/25/MAPPO/"/>
    <url>/2024/04/25/MAPPO/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/pdf/2103.01955.pdf">https://arxiv.org/pdf/2103.01955.pdf</a></p><p>代码链接：<a href="https://github.com/marlbenchmark/on-policy">https://github.com/marlbenchmark/on-policy</a></p><p>本文为MAPPO的详解，即PPO往多智能体场景中的迁移。</p><h4 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h4><p>本篇论文并没有提出一个全新的MARL算法，而是通过简单的修改来凭经验证明PPO在很广泛的合作多智能体场景中有非常亮眼的表现，并提供了MAPPO的实现思路。</p><p>具体的内容如下：</p><ul><li><p>PPO无需特定领域的算法或者架构改变，并进行极小的微调，就可以达到可以匹敌其他多智能体合作基准离线算法的表现。</p></li><li><p>PPO使用与其他算法相当的样本量就可以达到这个效果。</p></li><li><p>分析影响PPO性能的实施和超参数因素，并就</p></li><li><p>这些因素的最佳实践提供了具体建议。</p></li></ul><h4 id="算法架构"><a href="#算法架构" class="headerlink" title="算法架构"></a>算法架构</h4><p>MAPPO使用的是CTDE架构，状态的价值函数$V_\phi(s)$可以将智能体局部观测中不存在的全局状态信息并只在训练过程中使用到。这也体现了和IPPO的区别，即IPPO将局部观测当作策略和状态价值函数的输入。</p><h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><ul><li>使用参数共享，即每个智能体的观测空间和动作空间是对等的，这样可以提高学习效率。</li><li>采用了实现PPO时使用的技巧，包括Generalized Advantage Estimation（广义优势估计，GAE）、优势归一化、价值裁剪等。</li></ul><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>使用环境和对比算法如下：</p><ul><li><p>the multi-agent particle-world environment (MPE)：QMIX、MADDPG</p></li><li><p>the StarCraft micromanagement challenge (SMAC)：QMIX、QPLEX、CWQMIX、AIQMIX、RODE</p></li><li><p>Google Research Football (GRF)：QMIX、CDS、TiKick</p></li><li><p>the Hanabi challenge：SAD、VDN</p></li></ul><h4 id="影响MAPPO的因素"><a href="#影响MAPPO的因素" class="headerlink" title="影响MAPPO的因素"></a>影响MAPPO的因素</h4><h5 id="值标准化"><a href="#值标准化" class="headerlink" title="值标准化"></a>值标准化</h5><ul><li>在价值学习过程中，将价值函数的输出范围归一化，控制在合理的区间内，可以增强训练的稳定性</li></ul><h5 id="价值函数输入"><a href="#价值函数输入" class="headerlink" title="价值函数输入"></a>价值函数输入</h5><p>过去的成果中大部分用两种方法来表示全局状态：</p><ul><li><p><strong>Concatenation of Local observations (CL)：</strong>将每个智能体的观测全部拼接起来，获得一个整体的状态。但是这样的方式获得的状态会随智能体的数量增加而陡然增加，并会忽略一些重要的全局信息。</p></li><li><p><strong>Environment-Provided global state (EP)：</strong>包含了环境状态中所有智能体通用的信息。但是这样的状态会忽略一些重要的局部特定智能体的信息。</p></li></ul><p><strong>Agent-Specific Global State (AS)：</strong>本文采取的方式，结合了以上两者的特点，为每个智能体$i$特定了价值函数的输出为EP状态和其局部观测$o_i$。但是EP状态和$o_i$可能会有重叠的部分，导致状态维数增大和状态信息冗余。</p><p><strong>Featured-Pruned Agent-Specific Global State (FP)：</strong>为了弥补AS的缺陷，对AS状态进行处理，剔除重复的状态</p><p><img src="/2024/04/25/MAPPO/state_design.png" alt="几个方式的实现示例图"></p><h5 id="训练数据使用"><a href="#训练数据使用" class="headerlink" title="训练数据使用"></a>训练数据使用</h5><p>本文发现，在MAPPO的训练过程中，样本的过多次利用会导致算法性能下降。因此需要适当减小epoch。</p><h5 id="策略-价值裁剪"><a href="#策略-价值裁剪" class="headerlink" title="策略&#x2F;价值裁剪"></a>策略&#x2F;价值裁剪</h5><p>在使用$\epsilon$进行PPO裁剪的时候，需要寻找恰当的值。当$\epsilon$较小时，训练较为稳定，但是会以牺牲训练时间为代价。</p><h5 id="批大小"><a href="#批大小" class="headerlink" title="批大小"></a>批大小</h5><p>批次更大会让求梯度更加准确，但是过大会浪费内存资源。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>经典算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning</title>
    <link href="/2024/03/26/BadRL/"/>
    <url>/2024/03/26/BadRL/</url>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/pdf/2312.12585.pdf">https://arxiv.org/pdf/2312.12585.pdf</a></p><p>代码：<a href="https://github.com/7777777cc/code">https://github.com/7777777cc/code</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>对强化学习的<strong>稀疏的</strong>且<strong>有目标的</strong>后门攻击方法。当前的强化学习后门攻击方法无论是在训练还是测试过程中，都是基于密集的攻击步数，有些被攻击的状态不一定有显著效果，也增加了被检测的风险。</p><h4 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h4><h5 id="触发器层面"><a href="#触发器层面" class="headerlink" title="触发器层面"></a>触发器层面</h5><p>使用具体到样本的方法来生成后门触发器，使触发器更易学习且不易被遗忘，并最大化有触发器和无触发器模型的梯度的互信息。</p><h5 id="时间步选择层面"><a href="#时间步选择层面" class="headerlink" title="时间步选择层面"></a>时间步选择层面</h5><p>评估特定时间步的攻击价值，只选择攻击价值高的状态进行攻击。</p><h4 id="攻击模型"><a href="#攻击模型" class="headerlink" title="攻击模型"></a>攻击模型</h4><h5 id="攻击者知识"><a href="#攻击者知识" class="headerlink" title="攻击者知识"></a>攻击者知识</h5><p>采用黑盒攻击，满足以下条件：</p><ul><li><p>攻击者不知道用于训练受害智能体的算法。</p></li><li><p>攻击者不知道环境MDP，即状态转换信息等。</p></li><li><p>攻击者可以接触到输入到智能体中的观测状态。</p></li></ul><h5 id="攻击者能力"><a href="#攻击者能力" class="headerlink" title="攻击者能力"></a>攻击者能力</h5><p>攻击者可以攻击且仅可以攻击训练和测试过程中当前时间步的状态、动作和奖励信息</p><h6 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h6><p>攻击后的三元组为$(\tilde{s}_t,\tilde{a}_t(a_t),\tilde{r}_t,)$。其中带触发器的状态为$\tilde{s}_t&#x3D;s_t+\delta$，$\tilde{a}_t$是强攻击，$a_t$是弱攻击。满足仅攻击很小的样本数量的条件：<br>$$<br>\sum_{t&#x3D;1}^T\mathbb{1}[(s_t,a_t,r_t)\neq(\tilde{s}_t,\tilde{a}_t(a_t),\tilde{r}_t,)]\leq\epsilon T<br>$$</p><h4 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h4><p>测试过程相比训练过程能力有限，只能操控状态。</p><p>诱导智能体在触发后门的情况下，输出目标动作，这个目标动作由环境指定，并在整个环境中保持不变，基于目标动作和原动作之间的优势差异选择。攻击过程可以形式化为一个最优化过程：<br>$$<br>\begin{align}<br>\min_{\tilde{s}_{1:T},\tilde{a}_{1:T},\tilde{r}_{1:T}}E_{s_0\sim\mu_0}[\tilde{V}^{\tilde{\pi}_T}(s_0)]\\<br>\text{s.t. }\sum_{t&#x3D;1}^T\mathbb{1}[(s_t,a_t,r_t)\neq(\tilde{s}_t,\tilde{a}_t(a_t),\tilde{r}_t,)]\leq\epsilon T,\\<br>\tilde{\pi}_T(s+\delta)&#x3D;a^{\dagger},\forall s \in S^{\dagger},\\<br>E_{s_0\sim\mu_0}[{V}^{\tilde{\pi}_T}(s_0)]&#x3D;E_{s_0\sim\mu_0}[{V}^{\pi^*}(s_0)]<br>\end{align}<br>$$</p><p>可总结为两个核心问题，即<strong>何时攻击（目标状态的选取）</strong>和<strong>如何攻击（目标动作的选取）</strong></p><h4 id="BadRL攻击框架"><a href="#BadRL攻击框架" class="headerlink" title="BadRL攻击框架"></a>BadRL攻击框架</h4><h5 id="何时攻击"><a href="#何时攻击" class="headerlink" title="何时攻击"></a>何时攻击</h5><h6 id="定义攻击价值"><a href="#定义攻击价值" class="headerlink" title="定义攻击价值"></a>定义攻击价值</h6><p>计算受攻击和未受攻击的状态的价值，求出二者的差：<br>$$<br>V_A(s)&#x3D;Q^*(s,\pi^*(s))-Q^*(s,a^{\dagger}),\forall s\in S^{\dagger}<br>$$</p><h6 id="触发器设计"><a href="#触发器设计" class="headerlink" title="触发器设计"></a>触发器设计</h6><p>基于互信息调优来生成后门触发器，使无后门模型和有后门模型的优化方向对齐，进而训练路径相似。计算互信息，优化触发器模式，最小化损失函数：<br>$$<br>loss_{MI}&#x3D;-MI(g_{clean},g_{poisoned})<br>$$</p><h5 id="如何攻击"><a href="#如何攻击" class="headerlink" title="如何攻击"></a>如何攻击</h5><p>状态改变：由触发器设计部分确定触发器的位置和颜色像素值</p><p>动作改变：通过考虑环境的动作语义，赋予被攻击的智能体较为有效的动作<strong>（只是直觉上的一种选择）</strong></p><p>奖励改变：采用每个任务中获得的最小正奖励，尽可能保持奖励函数的完整性</p><h5 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h5><ul><li><p>动作是一种直觉上的选择，并不具备切实依据，有的任务并不那么容易区分动作的好坏。</p></li><li><p>如何拓展到多智能体强化学习问题当中，并考虑多智能体的特点？</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>日语N5语法总结</title>
    <link href="/2024/03/20/JapaneseN5/"/>
    <url>/2024/03/20/JapaneseN5/</url>
    
    <content type="html"><![CDATA[<p>参考资料：标准日本语初级（上册）<br>大写指代含义：<br>N：名词；P：地点；V：动作；T：时间；H：人物；A：形容词</p><h3 id="语法与句型结构"><a href="#语法与句型结构" class="headerlink" title="语法与句型结构"></a>语法与句型结构</h3><h4 id="陈述【N1是N2】的对应肯定、否定和疑问句"><a href="#陈述【N1是N2】的对应肯定、否定和疑问句" class="headerlink" title="陈述【N1是N2】的对应肯定、否定和疑问句"></a>陈述【N1是N2】的对应肯定、否定和疑问句</h4><p><strong>现在肯定句：</strong> N1はN2です。<br><em>のです&#x2F;んです表示所讲的内容与前句或前项内容有关联。用于说明状况或解释原因、理由。のです多用于书面语，んです为口语形式。</em><br><strong>过去肯定句：</strong>N1はN2でした。<br><strong>现在否定句：</strong>N1はN2では&#x2F;じゃありません。<br><strong>过去否定句：</strong>N1はN2では&#x2F;じゃありませんでした。<br><strong>一般疑问句：</strong><br>‐N1はN2ですか。<br>    ‐はい，そうです。<strong>（肯定）</strong><br>    ‐いいえ，ちがいます。<strong>（否定）</strong><br><strong>特殊疑问句：</strong><br>だれ&#x2F;どなたですか。<strong>（问人物）</strong><br>何ですか。<strong>（问事物）</strong><br>N1はN2ですか,N3ですか。<strong>（询问不止一种答案）</strong><br><strong>并列使用：</strong><br>N1でN2<br><strong>表示【N1（新信息）是N2（旧信息）】：</strong><br>N1がN2です。<br><em>表示疑问的词作主语时不能用は，只能用が。</em></p><h4 id="陈述【N1也是N2】"><a href="#陈述【N1也是N2】" class="headerlink" title="陈述【N1也是N2】"></a>陈述【N1也是N2】</h4><p>N1もN2です。</p><h4 id="表示从属关系【N1的N2】"><a href="#表示从属关系【N1的N2】" class="headerlink" title="表示从属关系【N1的N2】"></a>表示从属关系【N1的N2】</h4><p>N1のN2<br>其中，N1是N2从属的机构、国家或属性。若前文提到了该事物，N2可以省略</p><h4 id="表示并列关系【N1和N2】"><a href="#表示并列关系【N1和N2】" class="headerlink" title="表示并列关系【N1和N2】"></a>表示并列关系【N1和N2】</h4><p>N1とN2<br><strong>共同做某事的对象：</strong>H1とV1</p><h4 id="对名词进行选择（表示或者）"><a href="#对名词进行选择（表示或者）" class="headerlink" title="对名词进行选择（表示或者）"></a>对名词进行选择（表示或者）</h4><p>N1かN2</p><h4 id="列举名词（其中一两项）"><a href="#列举名词（其中一两项）" class="headerlink" title="列举名词（其中一两项）"></a>列举名词（其中一两项）</h4><p>N1やN2<br>常常和など呼应使用</p><h4 id="陈述【N1在P1】的对应肯定和特殊疑问句"><a href="#陈述【N1在P1】的对应肯定和特殊疑问句" class="headerlink" title="陈述【N1在P1】的对应肯定和特殊疑问句"></a>陈述【N1在P1】的对应肯定和特殊疑问句</h4><p><strong>肯定句：</strong><br>N1はP1です。<br><strong>特殊疑问句：</strong>N1はどこですか。</p><h4 id="陈述【N1存在于P1-P1有N1】的对应肯定和特殊疑问句"><a href="#陈述【N1存在于P1-P1有N1】的对应肯定和特殊疑问句" class="headerlink" title="陈述【N1存在于P1&#x2F;P1有N1】的对应肯定和特殊疑问句"></a>陈述【N1存在于P1&#x2F;P1有N1】的对应肯定和特殊疑问句</h4><p><strong>肯定句：</strong><br>N1はP1にあります（无意志事物）&#x2F;います（有意志事物）。<br>P1にN1があります&#x2F;います。<br>若N1表示事件：P1でN1があります。<br><strong>特殊疑问句：</strong><br>N1はどこですか。<br>N1はどこにありますか&#x2F;いますか。</p><h4 id="陈述动作的对象"><a href="#陈述动作的对象" class="headerlink" title="陈述动作的对象"></a>陈述动作的对象</h4><p>N1をV1</p><h4 id="陈述动作的场所"><a href="#陈述动作的场所" class="headerlink" title="陈述动作的场所"></a>陈述动作的场所</h4><p>P1でV1</p><h4 id="表示人或物体的附着点"><a href="#表示人或物体的附着点" class="headerlink" title="表示人或物体的附着点"></a>表示人或物体的附着点</h4><p>P1にV1</p><h4 id="场所移动"><a href="#场所移动" class="headerlink" title="场所移动"></a>场所移动</h4><h5 id="移动的目的地"><a href="#移动的目的地" class="headerlink" title="移动的目的地"></a>移动的目的地</h5><p>P1へ&#x2F;にV1</p><h5 id="移动的起点"><a href="#移动的起点" class="headerlink" title="移动的起点"></a>移动的起点</h5><p>P1からV1</p><h5 id="移动的交通手段"><a href="#移动的交通手段" class="headerlink" title="移动的交通手段"></a>移动的交通手段</h5><p>N1でV1<br><em>で还可以表示其他手段以及原材料</em></p><h5 id="移动的起点和目的地"><a href="#移动的起点和目的地" class="headerlink" title="移动的起点和目的地"></a>移动的起点和目的地</h5><p>P1からP2までV1</p><h5 id="表示移动行为的目的"><a href="#表示移动行为的目的" class="headerlink" title="表示移动行为的目的"></a>表示移动行为的目的</h5><p>P1へV1（去ます）に行きます&#x2F;来ます。<br><em>飲みに行きます若前面没有加喝的饮料默认为喝酒。</em></p><h5 id="表示经过某场所"><a href="#表示经过某场所" class="headerlink" title="表示经过某场所"></a>表示经过某场所</h5><p>P1を通ります&#x2F;渡ります&#x2F;過ぎます…</p><h5 id="表示离开某场所"><a href="#表示离开某场所" class="headerlink" title="表示离开某场所"></a>表示离开某场所</h5><p>P1を出ます&#x2F;卒業します…</p><h4 id="自动词和他动词"><a href="#自动词和他动词" class="headerlink" title="自动词和他动词"></a>自动词和他动词</h4><p>自动词：主语作用于宾语，用を表示。<br>他动词：没有宾语，表示主语自主动作，用が表示。</p><h4 id="询问价格"><a href="#询问价格" class="headerlink" title="询问价格"></a>询问价格</h4><p>N1はいくらですか。</p><h4 id="请求某物"><a href="#请求某物" class="headerlink" title="请求某物"></a>请求某物</h4><p>N1をください</p><h4 id="表示具体位置"><a href="#表示具体位置" class="headerlink" title="表示具体位置"></a>表示具体位置</h4><p>上&#x2F;下&#x2F;前&#x2F;後ろ&#x2F;隣&#x2F;中&#x2F;外</p><h4 id="表示全盘否定"><a href="#表示全盘否定" class="headerlink" title="表示全盘否定"></a>表示全盘否定</h4><p>疑问词+も+V1（否定）<br>…誰もいません。<br>…何もありません。</p><h4 id="任何情况下事态相同"><a href="#任何情况下事态相同" class="headerlink" title="任何情况下事态相同"></a>任何情况下事态相同</h4><p>疑问词+でも</p><h4 id="表示时间"><a href="#表示时间" class="headerlink" title="表示时间"></a>表示时间</h4><h5 id="一般时间"><a href="#一般时间" class="headerlink" title="一般时间"></a>一般时间</h5><p>今（午前&#x2F;午後）~時~分です。<br>30分＝半</p><h5 id="两个名词都是时间"><a href="#两个名词都是时间" class="headerlink" title="两个名词都是时间"></a>两个名词都是时间</h5><p><strong>前面名词包含每：</strong>两个名词之间不加の<br><strong>前面名词不含每：</strong>两个名词之间可加可不加の</p><h5 id="时间左右"><a href="#时间左右" class="headerlink" title="时间左右"></a>时间左右</h5><p>~ごろ</p><h4 id="表示动作"><a href="#表示动作" class="headerlink" title="表示动作"></a>表示动作</h4><h5 id="现在的习惯性动作状态-未来的动作状态"><a href="#现在的习惯性动作状态-未来的动作状态" class="headerlink" title="现在的习惯性动作状态&#x2F;未来的动作状态"></a>现在的习惯性动作状态&#x2F;未来的动作状态</h5><p>V1ます&#x2F;V1ません</p><h5 id="表示动作或变化正在进行"><a href="#表示动作或变化正在进行" class="headerlink" title="表示动作或变化正在进行"></a>表示动作或变化正在进行</h5><p>V1（て形）います</p><h5 id="过去的动作状态"><a href="#过去的动作状态" class="headerlink" title="过去的动作状态"></a>过去的动作状态</h5><p>V1ました&#x2F;V1ませんでした</p><h5 id="表示动作结束后留下的结果状态"><a href="#表示动作结束后留下的结果状态" class="headerlink" title="表示动作结束后留下的结果状态"></a>表示动作结束后留下的结果状态</h5><p>V1（て形）います<br><em>对“知っていますか”的否定回答必须使用“知りません”。</em></p><h5 id="动作状态的持续时间"><a href="#动作状态的持续时间" class="headerlink" title="动作状态的持续时间"></a>动作状态的持续时间</h5><p>T1（持续时间）+V1</p><h5 id="一定时间内进行若干次动作"><a href="#一定时间内进行若干次动作" class="headerlink" title="一定时间内进行若干次动作"></a><strong>一定时间内进行若干次动作</strong></h5><p>T1（持续时间）に+次数+V1<br><strong>以一定期间为基准计算频率时的省略表达：</strong><br>1日に→日に<br>1か月に→月に<br>1年に→年に</p><h5 id="表示动作相继发生"><a href="#表示动作相继发生" class="headerlink" title="表示动作相继发生"></a>表示动作相继发生</h5><p>V1（て形）+V2<br><em>连接多了会使人难以理解，因此一般只连两三项。需要连接更多则使用<strong>それから</strong></em><br>V1（て形）から+V2<strong>（不能在一个句子中出现两次以上）</strong></p><h5 id="表示动作许可"><a href="#表示动作许可" class="headerlink" title="表示动作许可"></a>表示动作许可</h5><p><strong>肯定句：</strong>V1（て形）もいいです。<br><em>通常不用于尊长，因为很傲慢</em><br><strong>否定句：</strong>V1（て形）は行けません。<br><strong>疑问句：</strong>V1（て形）もいいですか。<br><strong>肯定回答：</strong><br>どうぞ。&#x2F;かまいません。<strong>（根据自己的心情或判断）</strong><br>いいです。&#x2F;大丈夫です。<strong>（基于社会惯例或公共准则）</strong><br><strong>否定回答：</strong><br>いえ，ちょっと…&#x2F;すみません，~から。<strong>（根据自己的心情或判断）</strong><br>いけません。&#x2F;だめです。<strong>（基于社会惯例或公共准则）</strong></p><h4 id="时间和动作结合"><a href="#时间和动作结合" class="headerlink" title="时间和动作结合"></a>时间和动作结合</h4><p>T1（に）V1</p><h5 id="某时做某事"><a href="#某时做某事" class="headerlink" title="某时做某事"></a>某时做某事</h5><p><strong>包含数字的时间：</strong>加に<br><strong>昨今明含义的时间：</strong>不加に<br><strong>星期几：</strong>可加可不加に</p><h5 id="从T1到T2做某事"><a href="#从T1到T2做某事" class="headerlink" title="从T1到T2做某事"></a>从T1到T2做某事</h5><p>T1からT2までV1。</p><h5 id="询问何时做某事"><a href="#询问何时做某事" class="headerlink" title="询问何时做某事"></a>询问何时做某事</h5><p>いつ&#x2F;何曜日V1ますか。<br>いつ&#x2F;何曜日から&#x2F;までV1ますか。</p><h4 id="请求某人做某事"><a href="#请求某人做某事" class="headerlink" title="请求某人做某事"></a>请求某人做某事</h4><p>V1（て形）ください。<br>V1（て形）くださいませんか。<strong>（更加礼貌）</strong><br>そうしてください。</p><h4 id="请求-命令某人不做某事"><a href="#请求-命令某人不做某事" class="headerlink" title="请求&#x2F;命令某人不做某事"></a>请求&#x2F;命令某人不做某事</h4><p>V1（ない形）でください。</p><h4 id="表示必须做某事"><a href="#表示必须做某事" class="headerlink" title="表示必须做某事"></a>表示必须做某事</h4><p>V1（ない形去ない）なければなりません。<strong>（多用于书面语）</strong><br>V1（ない形）と（いけません）。<strong>（主要用于口语）</strong></p><h4 id="表示不做某事也可以"><a href="#表示不做某事也可以" class="headerlink" title="表示不做某事也可以"></a>表示不做某事也可以</h4><p>V1（ない形去ない）なくてもいいです。</p><h4 id="表示能力-允许"><a href="#表示能力-允许" class="headerlink" title="表示能力&#x2F;允许"></a>表示能力&#x2F;允许</h4><p>H1はV1（基本形）ことができます。</p><h4 id="表示谓语是一种动作行为"><a href="#表示谓语是一种动作行为" class="headerlink" title="表示谓语是一种动作行为"></a>表示谓语是一种动作行为</h4><p>N1はV1（基本形）ことです。<br><em>动词基本形+こと可以起到和名词同样的作用</em></p><h4 id="表示某个动作正在进行"><a href="#表示某个动作正在进行" class="headerlink" title="表示某个动作正在进行"></a>表示某个动作正在进行</h4><p>~中<br><em>一类、二类动词用ます形去ます（不是所有都能用此形式），三类动词用ます形去します（来ます不能用此形式）。</em></p><h4 id="在做某事之前"><a href="#在做某事之前" class="headerlink" title="在做某事之前"></a>在做某事之前</h4><p>V1（基本形）前に<br>N1の前に</p><h4 id="在做某事之后"><a href="#在做某事之后" class="headerlink" title="在做某事之后"></a>在做某事之后</h4><p>V1（た形）後で<br>N1の後で</p><h4 id="表示过去的经历"><a href="#表示过去的经历" class="headerlink" title="表示过去的经历"></a>表示过去的经历</h4><p>V1（た形）ことがあります。<br><em>强调完全没有某种经历，有时可以在开头加“一度も”</em></p><h4 id="做某事更好"><a href="#做某事更好" class="headerlink" title="做某事更好"></a>做某事更好</h4><p>V1（た形）ほうがいいです。<br>N1&#x2F;これ&#x2F;それ&#x2F;あれ+の+ほうがいいです。<br>この&#x2F;その&#x2F;あの+ほうがいいです。</p><h4 id="列举若干代表性动作"><a href="#列举若干代表性动作" class="headerlink" title="列举若干代表性动作"></a>列举若干代表性动作</h4><p>V1（た形）りV2（た形）りします</p><h4 id="思考内容"><a href="#思考内容" class="headerlink" title="思考内容"></a>思考内容</h4><p>小句（简体形）と思います<br><em>出现名词或二类形容词小句时加だ。</em></p><h4 id="转述他人的话"><a href="#转述他人的话" class="headerlink" title="转述他人的话"></a>转述他人的话</h4><p>H1は小句（简体形）と言いました<br><em>出现名词或二类形容词小句时加だ。</em></p><h4 id="形容词用法"><a href="#形容词用法" class="headerlink" title="形容词用法"></a>形容词用法</h4><p>一类形容词是以“い”结尾的形容词</p><h5 id="现在肯定句"><a href="#现在肯定句" class="headerlink" title="现在肯定句"></a>现在肯定句</h5><p><strong>一类形：</strong>N1はA1です。<br><strong>二类形：</strong>N1はA1です。</p><h5 id="现在否定句"><a href="#现在否定句" class="headerlink" title="现在否定句"></a>现在否定句</h5><p><strong>一类形：</strong>N1はA1（去い）くないです。&#x2F;N1はA1（去い）くありません。<br><strong>二类形：</strong>N1はA1では&#x2F;じゃありません。<br>表示否定程度不高，使用<strong>あまり+A1（否定）</strong></p><h5 id="过去肯定句"><a href="#过去肯定句" class="headerlink" title="过去肯定句"></a>过去肯定句</h5><p><strong>一类形：</strong>N1はA1（去い）かったです。<br><strong>二类形：</strong>N1はA1でした。</p><h5 id="过去否定句"><a href="#过去否定句" class="headerlink" title="过去否定句"></a>过去否定句</h5><p><strong>一类形：</strong>N1はA1（去い）くなかったです。&#x2F;N1はA1（去い）くありませんでした。<br><strong>二类形：</strong>N1はA1では&#x2F;じゃありませんでした。</p><h5 id="否定疑问句"><a href="#否定疑问句" class="headerlink" title="否定疑问句"></a>否定疑问句</h5><p>N1はA1（去い）くないですか。<br>“はい”和“いいえ”都可以使用</p><h5 id="修饰名词"><a href="#修饰名词" class="headerlink" title="修饰名词"></a>修饰名词</h5><p>一类型形容词可直接修饰名词<br><strong>一类形：</strong>A1+N1<br><em>“多い”和“少ない”不能单独修饰名词。</em><br><strong>二类形：</strong>A1なN1<br><em>对于前面已经提过的形容词修饰的名词，可在后文用“A1の”代替，避免重复。</em></p><h5 id="特殊疑问句"><a href="#特殊疑问句" class="headerlink" title="特殊疑问句"></a>特殊疑问句</h5><p>询问人或事物的性质<br>どんなN1<br><em>何の有询问内容和材料两种用法，どんな只用于询问性质。</em></p><h5 id="询问对方对某状态的意见或感想"><a href="#询问对方对某状态的意见或感想" class="headerlink" title="询问对方对某状态的意见或感想"></a>询问对方对某状态的意见或感想</h5><p>どうですか。<strong>（现在或未来）</strong><br>どうでしたか。<strong>（过去）</strong><br>いかがですか。<strong>（礼貌表示，对长辈或上级使用）</strong></p><h5 id="形容词并列"><a href="#形容词并列" class="headerlink" title="形容词并列"></a>形容词并列</h5><p><strong>一类形：</strong>A1（去い）くてA2<br><strong>二类形：</strong>A1くでA2</p><h5 id="表示状态或状况有多种可能"><a href="#表示状态或状况有多种可能" class="headerlink" title="表示状态或状况有多种可能"></a>表示状态或状况有多种可能</h5><p><strong>一类形：</strong>A1（かった形）りA2（かった形）りです<br><strong>二类形：</strong>A1（だった形）りA2（だった形）りです<br><strong>名词：</strong>N1（だった形）りN2（だった形）りです<br><em>一类和二类形容词出现的状态一般是反义。</em></p><h4 id="表示比较"><a href="#表示比较" class="headerlink" title="表示比较"></a>表示比较</h4><h5 id="表示名词比较"><a href="#表示名词比较" class="headerlink" title="表示名词比较"></a>表示名词比较</h5><h6 id="陈述句"><a href="#陈述句" class="headerlink" title="陈述句"></a>陈述句</h6><p>N1はN2よりA1です。<strong>（N1更具有形容词所表示性质）</strong><br>N1よりN2のほうがA1です。<strong>（N2更具有形容词所表示性质）</strong><br><em>和より连接的都是不及另一个的</em><br><strong>一类形：</strong>N1はN2ほどA1（去い）くないです。<br><strong>二类形：</strong>N1はN2ほどA1ではありません。</p><h6 id="特殊疑问句-1"><a href="#特殊疑问句-1" class="headerlink" title="特殊疑问句"></a>特殊疑问句</h6><p>N1とN2とどちらがA1ですか。<br>​-~のほうがA1です<br>​-どちらもA1です。</p><h5 id="表示名词最高级"><a href="#表示名词最高级" class="headerlink" title="表示名词最高级"></a>表示名词最高级</h5><h6 id="陈述句-1"><a href="#陈述句-1" class="headerlink" title="陈述句"></a>陈述句</h6><p>N1の中でN2がいちばんA1です。<br><em>N1为范围，N2为主语。</em><br>P1でいちばんA1N1はN2です。</p><h6 id="特殊疑问句-2"><a href="#特殊疑问句-2" class="headerlink" title="特殊疑问句"></a>特殊疑问句</h6><p>どのN1&#x2F;いつ&#x2F;どれ&#x2F;だれ&#x2F;何がいちばんA1ですか。</p><h5 id="表示形容词比较"><a href="#表示形容词比较" class="headerlink" title="表示形容词比较"></a>表示形容词比较</h5><p><strong>一类形：</strong>A1ほうがいいです<br><strong>二类形：</strong>A1なほうがいいです</p><h4 id="表示性质或状态的变化"><a href="#表示性质或状态的变化" class="headerlink" title="表示性质或状态的变化"></a>表示性质或状态的变化</h4><p><strong>一类形：</strong><br>A1（去い）くなります<strong>（事物变化）</strong><br>A1します<strong>（因主语意志导致）</strong><br><strong>二类形&#x2F;名词：</strong><br>A1&#x2F;N1になります<strong>（事物变化）</strong><br>A1&#x2F;N1にします<strong>（因主语意志导致）</strong><br><em>在餐馆里回答服务员点餐询问也可以使用N1にします</em></p><h4 id="陈述情感主体和对象"><a href="#陈述情感主体和对象" class="headerlink" title="陈述情感主体和对象"></a>陈述情感主体和对象</h4><p>H1はN1がA1（一类形&#x2F;二类形）です</p><h4 id="表达能力"><a href="#表达能力" class="headerlink" title="表达能力"></a>表达能力</h4><p>H1はN1が分かります&#x2F;できます&#x2F;上手です&#x2F;下手です&#x2F;苦手です</p><h4 id="何的读法"><a href="#何的读法" class="headerlink" title="何的读法"></a>何的读法</h4><p>根据“何”后续音节的不同读音有相应的变化。</p><table><thead><tr><th>读音</th><th>后续音节</th></tr></thead><tbody><tr><td>なに</td><td>と</td></tr><tr><td>なに</td><td>が</td></tr><tr><td>なに</td><td>を</td></tr><tr><td>なに&#x2F;なん</td><td>で</td></tr><tr><td>なん</td><td>の</td></tr><tr><td>なん</td><td>时刻&#x2F;星期</td></tr></tbody></table><h4 id="表示愿望"><a href="#表示愿望" class="headerlink" title="表示愿望"></a>表示愿望</h4><p><strong>名词：</strong>H1はN1が欲しいです<br><strong>动词：</strong><br>H1はV1（去掉ます）たいです<strong>（肯定）</strong><br>H1はV1（去掉ます）たくないです<strong>（否定）</strong><br><em>动词的对象可用を或者が</em><br><strong>全面否定“N1+に&#x2F;から&#x2F;と+V1”：</strong><br>疑问词+にも&#x2F;からも&#x2F;とも+V1否定</p><h4 id="表示提议"><a href="#表示提议" class="headerlink" title="表示提议"></a>表示提议</h4><p>V1（ません）か<strong>（礼貌程度较高）</strong><br>V1（去掉ます）ましょう<strong>（礼貌程度较低）</strong></p><h4 id="表示给和得到"><a href="#表示给和得到" class="headerlink" title="表示给和得到"></a>表示给和得到</h4><h5 id="表示【H1给H2某物】的陈述句"><a href="#表示【H1给H2某物】的陈述句" class="headerlink" title="表示【H1给H2某物】的陈述句"></a>表示【H1给H2某物】的陈述句</h5><p><strong>物品移动：</strong>第一人称→第二人称→第三人称&#x2F;第三人称→第三人称<br>H1はH2にN1をあげます<br>若第三人称其中之一是说话人的亲戚，按说话人的立场处理。</p><h5 id="表示【H1从H2得到某物】的陈述句"><a href="#表示【H1从H2得到某物】的陈述句" class="headerlink" title="表示【H1从H2得到某物】的陈述句"></a>表示【H1从H2得到某物】的陈述句</h5><p><strong>物品移动：</strong>第三人称→第二人称→第一人称&#x2F;第三人称→第三人称<br>H1はH2に&#x2F;からN1をもらいます<br>若第三人称其中之一是说话人的亲戚，按说话人的立场处理。<br><em>一般多用に，如果给予一方为组织或团体（如会社、学校），则用から</em></p><h4 id="和某人见面"><a href="#和某人见面" class="headerlink" title="和某人见面"></a>和某人见面</h4><p>H1に会います</p><h4 id="表示“-一事”"><a href="#表示“-一事”" class="headerlink" title="表示“~一事”"></a>表示“~一事”</h4><p>~の件<strong>（比较郑重）</strong></p><h4 id="表示明白-承诺-应答"><a href="#表示明白-承诺-应答" class="headerlink" title="表示明白&#x2F;承诺&#x2F;应答"></a>表示明白&#x2F;承诺&#x2F;应答</h4><p>分かりました。</p><h4 id="は的用法"><a href="#は的用法" class="headerlink" title="は的用法"></a>は的用法</h4><p>除了作为连接词，还可以表示【话题】和【对比】，因此可以替换を</p><h4 id="中"><a href="#中" class="headerlink" title="~中"></a>~中</h4><p><strong>P1中：</strong>表示其场所范围内的全部。<br><strong>T1中：</strong>在某个期间一直。<br><strong>T1中に：</strong>该期间结束之前。</p><h4 id="表示原因"><a href="#表示原因" class="headerlink" title="表示原因"></a>表示原因</h4><h5 id="陈述句-2"><a href="#陈述句-2" class="headerlink" title="陈述句"></a>陈述句</h5><p>~から（必须接在表示原因、理由小句的结尾）<br>连词：だから&#x2F;ですから<strong>（礼貌用法）</strong></p><h5 id="疑问句"><a href="#疑问句" class="headerlink" title="疑问句"></a>疑问句</h5><p>どうしてですか<br>どうして~のですか&#x2F;んですか（回答此问句也要用~のです&#x2F;んです）</p><h4 id="量词的表示方式"><a href="#量词的表示方式" class="headerlink" title="量词的表示方式"></a>量词的表示方式</h4><table><thead><tr><th>量词</th><th>使用对象</th><th>量词</th><th>使用对象</th></tr></thead><tbody><tr><td>~人</td><td>人</td><td>~本</td><td>细长物品</td></tr><tr><td>~台</td><td>机械或车辆</td><td>~杯</td><td>容器内的饮料</td></tr><tr><td>~枚</td><td>薄平物品</td><td>~匹</td><td>小动物</td></tr><tr><td>~冊</td><td>书、笔记本等</td><td>~頭</td><td>大动物</td></tr><tr><td>~歳</td><td>年龄</td><td>~羽</td><td>鸟、兔</td></tr><tr><td>~回</td><td>次数</td><td>~番</td><td>顺序</td></tr><tr><td>~着</td><td>衣服等</td><td>~足</td><td>成对物品</td></tr><tr><td>~個</td><td>立体的物品、空容器或器皿、概念性的抽象的事物</td><td>~つ</td><td>1~9岁的年龄、立体的物品、抽象的事物（不用于10以上数字）</td></tr></tbody></table><p><strong>读法变化：</strong><br>读法不变化：~台、~枚、~番<br>和1、6、8、10结合发生促音变：~個、~階、~回（k开头，数词发生停顿）<br>和1、3、6、8、10结合发生促音变：~本、~杯、~匹（h开头，和1、6、8、10变p开头并数词发生停顿，和3变b开头）</p><h4 id="以个为单位售物"><a href="#以个为单位售物" class="headerlink" title="以个为单位售物"></a>以个为单位售物</h4><p>量词+で+价格<br><em>数量是1个时不加で。</em></p><h4 id="表示数量多"><a href="#表示数量多" class="headerlink" title="表示数量多"></a>表示数量多</h4><p>何+量词+も+肯定形式</p><h4 id="表示数字左右"><a href="#表示数字左右" class="headerlink" title="表示数字左右"></a>表示数字左右</h4><p> <strong>陈述句：</strong><br>（だいたい）+量词+くらい&#x2F;ぐらい<br><strong>特殊疑问句：</strong><br>どのぐらいかかりますか。<br>​‐~ぐらいかかります。</p><h4 id="表示某物和某人很适合"><a href="#表示某物和某人很适合" class="headerlink" title="表示某物和某人很适合"></a>表示某物和某人很适合</h4><p>H1はN1が似合います<br>N1はH1に似合います</p><h3 id="动词形式"><a href="#动词形式" class="headerlink" title="动词形式"></a>动词形式</h3><h4 id="て形"><a href="#て形" class="headerlink" title="て形"></a>て形</h4><table><thead><tr><th>类别</th><th>动词特点</th><th>て形处理</th></tr></thead><tbody><tr><td>一类动词</td><td>最后一个音位于“い段”（绝大部分）</td><td>去掉ます，き→いて，ぎ→いで，び、み、に→んで，ち、り、い、→って，し→して（行きます→いって）</td></tr><tr><td>二类动词</td><td>最后一个音位于“え段”和“い段”（小部分）</td><td>去掉ます，直接加て</td></tr><tr><td>三类动词</td><td>来ます、します和其他使用します的动词</td><td>去掉ます，直接加て</td></tr></tbody></table><p>名词+します的三类动词也可以表示为名词をします</p><h4 id="ない形"><a href="#ない形" class="headerlink" title="ない形"></a>ない形</h4><table><thead><tr><th>类别</th><th>动词特点</th><th>ない形处理</th></tr></thead><tbody><tr><td>一类动词</td><td>最后一个音位于“い段”（绝大部分）</td><td>去掉ます，い变成わ，其他把最后一个音变成相应的“あ段”音</td></tr><tr><td>二类动词</td><td>最后一个音位于“え段”和“い段”（小部分）</td><td>去掉ます，直接加ない</td></tr><tr><td>三类动词</td><td>来ます、します和其他使用します的动词</td><td>来（き）ます变成来（こ）ない，把します变成しない</td></tr></tbody></table><h4 id="基本形"><a href="#基本形" class="headerlink" title="基本形"></a>基本形</h4><table><thead><tr><th>类别</th><th>动词特点</th><th>ない形处理</th></tr></thead><tbody><tr><td>一类动词</td><td>最后一个音位于“い段”（绝大部分）</td><td>去掉ます，“い段”音变成相应的“う段”音</td></tr><tr><td>二类动词</td><td>最后一个音位于“え段”和“い段”（小部分）</td><td>去掉ます，直接加る</td></tr><tr><td>三类动词</td><td>来ます、します和其他使用します的动词</td><td>来（き）ます变成来（く）る，把します变成する</td></tr></tbody></table><h4 id="た形"><a href="#た形" class="headerlink" title="た形"></a>た形</h4><p>把て形的“て”换成“た”，把“で”换成“だ”。</p><h4 id="敬体形和简体形"><a href="#敬体形和简体形" class="headerlink" title="敬体形和简体形"></a>敬体形和简体形</h4><h5 id="动词"><a href="#动词" class="headerlink" title="动词"></a>动词</h5><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>買います（ます形）</td>        <td>買う（基本形）</td>      </tr >    <tr >        <td>否定</td>        <td>買いません（ます形否定）</td>        <td>買わない（ない形）</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>買いました（ます形过去式）</td>        <td>買った（た形）</td>      </tr >    <tr >        <td>否定</td>        <td>買いませんでした（ます形过去式否定）</td>        <td>買わなかった（なかった形）</td>    </tr></table><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>あります</td>        <td>ある</td>      </tr >    <tr >        <td>否定</td>        <td>ありません</td>        <td>ない</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>ありました</td>        <td>あった</td>      </tr >    <tr >        <td>否定</td>        <td>ありませんでした</td>        <td>なかった</td>    </tr></table><h5 id="一类形容词"><a href="#一类形容词" class="headerlink" title="一类形容词"></a>一类形容词</h5><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>忙しいです</td>        <td>忙しい</td>      </tr >    <tr >        <td>否定</td>        <td>忙しくないです</td>        <td>忙しくない</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>忙しかったです</td>        <td>忙しかった</td>      </tr >    <tr >        <td>否定</td>        <td>忙しくなかったです</td>        <td>忙しくなかった</td>    </tr></table><h5 id="二类形容词"><a href="#二类形容词" class="headerlink" title="二类形容词"></a>二类形容词</h5><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>簡単です</td>        <td>簡単だ</td>      </tr >    <tr >        <td>否定</td>        <td>簡単ではありません</td>        <td>簡単ではない</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>簡単でした</td>        <td>簡単だった</td>      </tr >    <tr >        <td>否定</td>        <td>簡単ではありませんでした</td>        <td>簡単ではなかった</td>    </tr></table><h5 id="名词谓语形式"><a href="#名词谓语形式" class="headerlink" title="名词谓语形式"></a>名词谓语形式</h5><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>晴れです</td>        <td>晴れだ</td>      </tr >    <tr >        <td>否定</td>        <td>晴れではありません</td>        <td>晴れではない</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>晴れでした</td>        <td>晴れだった</td>      </tr >    <tr >        <td>否定</td>        <td>晴れではありませんでした</td>        <td>晴れではなかった</td>    </tr></table><h5 id="简体助词"><a href="#简体助词" class="headerlink" title="简体助词"></a>简体助词</h5><p><strong>かな：</strong>本来只用于自言自语。如有听话人在场，表示通过让对方听到自己的自问自答而向对方提供一种不太确实的信息。<br><strong>の：</strong>要求说明或确认某事。</p><h3 id="表示指代的词"><a href="#表示指代的词" class="headerlink" title="表示指代的词"></a>表示指代的词</h3><h4 id="人称"><a href="#人称" class="headerlink" title="人称"></a>人称</h4><p><strong>第一人称：</strong>わたし<br><strong>第二人称：</strong>あなた<br><strong>第三人称：</strong>あの人<br><strong>称呼他人：</strong>~さん<br><strong>疑问：</strong>だれ&#x2F;どなた<strong>（礼貌说法）</strong></p><h4 id="事物"><a href="#事物" class="headerlink" title="事物"></a>事物</h4><h5 id="根据距离"><a href="#根据距离" class="headerlink" title="根据距离"></a>根据距离</h5><h6 id="说话人和听话人相隔一段距离"><a href="#说话人和听话人相隔一段距离" class="headerlink" title="说话人和听话人相隔一段距离"></a>说话人和听话人相隔一段距离</h6><p><strong>距离说话人较近：</strong>これ<br><strong>距离听话人较近：</strong>それ<br><strong>距离说话人和听话人都较远：</strong>あれ</p><h6 id="说话人和听话人位于同一位置"><a href="#说话人和听话人位于同一位置" class="headerlink" title="说话人和听话人位于同一位置"></a>说话人和听话人位于同一位置</h6><p><strong>较近事物：</strong>これ<br><strong>较远事物：</strong>それ<br><strong>更远事物：</strong>あれ<br><strong>疑问词：</strong>どれ</p><h4 id="修饰名词-1"><a href="#修饰名词-1" class="headerlink" title="修饰名词"></a>修饰名词</h4><p>この&#x2F;その&#x2F;あの</p><h4 id="指示场所"><a href="#指示场所" class="headerlink" title="指示场所"></a>指示场所</h4><p>ここ&#x2F;そこ&#x2F;あそこ&#x2F;どこ<br>こちら&#x2F;そちら&#x2F;あちら&#x2F;どちら<strong>（原本表示方向，较礼貌）</strong><br><em>注意，どちら可以在询问时可以表示在哪里，也可以表示哪个，根据上下文判断：</em><br><em>会社はどちらですか。你的公司在哪里？&#x2F;你是哪个公司的</em><br><strong>疑问词：</strong>どの</p><h4 id="か的用法"><a href="#か的用法" class="headerlink" title="か的用法"></a>か的用法</h4><h5 id="疑问词-か"><a href="#疑问词-か" class="headerlink" title="疑问词+か"></a>疑问词+か</h5><p>いつ&#x2F;どこ&#x2F;だれ+か</p><h5 id="小句-か"><a href="#小句-か" class="headerlink" title="小句+か"></a>小句+か</h5><p>用于表示某种不确定的内容</p><h5 id="小句-かどうか"><a href="#小句-かどうか" class="headerlink" title="小句+かどうか"></a>小句+かどうか</h5><p>一般疑问句作为句子成分，将动词、一类形容词的敬体形变为简体形，若使用名词或二类形容词，则使用原形。<br>也可以重复使用动词的基本形和ない形。</p><h5 id="疑问词小句-か"><a href="#疑问词小句-か" class="headerlink" title="疑问词小句+か"></a>疑问词小句+か</h5><p>何&#x2F;だれ&#x2F;どこ+か<br>将疑问词小句的动词、一类形容词的敬体形变为简体形，若使用名词或二类形容词，则使用原形。</p><h3 id="寒暄语"><a href="#寒暄语" class="headerlink" title="寒暄语"></a>寒暄语</h3><h4 id="自我介绍时表示请多关照"><a href="#自我介绍时表示请多关照" class="headerlink" title="自我介绍时表示请多关照"></a>自我介绍时表示请多关照</h4><p>どうぞよろしくお願いします。<strong>（较礼貌）</strong><br>どうぞよろしくお願いいたします。<strong>（更加礼貌）</strong><br>どうぞよろしく。<br>よろしくお願いします。</p><h4 id="表达谢意"><a href="#表达谢意" class="headerlink" title="表达谢意"></a>表达谢意</h4><p>ありがとうございます。<br>どうもありがとうございます。<strong>（加强谢意）</strong><br>どうも。<strong>（略示谢意）</strong></p><h4 id="表达体谅安慰-工作结束后的寒暄："><a href="#表达体谅安慰-工作结束后的寒暄：" class="headerlink" title="表达体谅安慰&#x2F;工作结束后的寒暄："></a>表达体谅安慰&#x2F;工作结束后的寒暄：</h4><p>-お先に失礼します。<br>-お疲れ様でした。</p><h4 id="进入-离开别人房间-表示告辞："><a href="#进入-离开别人房间-表示告辞：" class="headerlink" title="进入&#x2F;离开别人房间&#x2F;表示告辞："></a><strong>进入&#x2F;离开别人房间&#x2F;<strong>表示告辞</strong>：</strong></h4><p>失礼します。</p><h4 id="以回来为前提的离开"><a href="#以回来为前提的离开" class="headerlink" title="以回来为前提的离开"></a>以回来为前提的离开</h4><p>いってまいります。<br>いってきます。<strong>（较为随便）</strong></p><h4 id="表达“去吧”并表示盼望对方回来"><a href="#表达“去吧”并表示盼望对方回来" class="headerlink" title="表达“去吧”并表示盼望对方回来"></a>表达“去吧”并表示盼望对方回来</h4><p>いってらっしゃい。</p><h4 id="表达回来和回应"><a href="#表达回来和回应" class="headerlink" title="表达回来和回应"></a>表达回来和回应</h4><p>‐ただいま。<br>‐お帰りなさい。</p><h4 id="表达欢迎光临和回应"><a href="#表达欢迎光临和回应" class="headerlink" title="表达欢迎光临和回应"></a>表达欢迎光临和回应</h4><p>いらっしゃいませ。<strong>（较为正式，用于店员欢迎顾客）</strong><br>-いらっしゃい。<strong>（较为随便，常用于亲近的人来家里做客）</strong><br>-お邪魔します。<strong>（表示打扰了）</strong></p><h4 id="表达店员明白顾客的需求"><a href="#表达店员明白顾客的需求" class="headerlink" title="表达店员明白顾客的需求"></a>表达店员明白顾客的需求</h4><p>かしこまりました。</p><h4 id="对生病或受伤的人表示关心"><a href="#对生病或受伤的人表示关心" class="headerlink" title="对生病或受伤的人表示关心"></a>对生病或受伤的人表示关心</h4><p>どうぞお大事にしてください。<br>お大事に。</p><h4 id="分别的寒暄"><a href="#分别的寒暄" class="headerlink" title="分别的寒暄"></a>分别的寒暄</h4><p>お世話になりました。<strong>（受到别人的照顾和帮助，分手时表示感谢）</strong><br>お世話になります。<strong>（今后将要得到别人的帮助或指导）</strong></p><h4 id="请向-问好"><a href="#请向-问好" class="headerlink" title="请向~问好"></a>请向~问好</h4><p>~によろしく伝えてください。<br>~によろしくお伝えください。<strong>（敬语形式）</strong><br>どうぞよろしくお伝えください。<strong>（加重语气）</strong><br>~によろしく。<strong>（双方关系亲密）</strong></p><h4 id="请多保重"><a href="#请多保重" class="headerlink" title="请多保重"></a>请多保重</h4><p>お元気で。<strong>（用于时间较长的分离，对方必须健康状态良好）</strong><br>お気をつけて。<strong>（用于一般性分别，意为祝愿对方路途平安）</strong></p><h3 id="礼貌用语"><a href="#礼貌用语" class="headerlink" title="礼貌用语"></a>礼貌用语</h3><h4 id="指代他人"><a href="#指代他人" class="headerlink" title="指代他人"></a>指代他人</h4><p>この&#x2F;その&#x2F;あの方 <strong>（この&#x2F;その&#x2F;あの人的礼貌表达）</strong></p><h4 id="询问年龄"><a href="#询问年龄" class="headerlink" title="询问年龄"></a>询问年龄</h4><p>何歳ですか。<br>おいくつですか。<strong>（比较礼貌）</strong><br>いくつ？&#x2F;何歳？<strong>（直接询问孩子的年龄）</strong></p><h4 id="给对方物品-请对方用餐"><a href="#给对方物品-请对方用餐" class="headerlink" title="给对方物品&#x2F;请对方用餐"></a>给对方物品&#x2F;请对方用餐</h4><p>どうぞ</p><h4 id="送别人东西"><a href="#送别人东西" class="headerlink" title="送别人东西"></a>送别人东西</h4><p>直接使用あげます会有强加于人的印象，因此使用：<br>どうぞ<br>どうですか</p><h4 id="请求对方做某事"><a href="#请求对方做某事" class="headerlink" title="请求对方做某事"></a>请求对方做某事</h4><p>N1&#x2F;H1をお願いします。<br>V1（て形）ください。<br>V1（て形）くださいませんか。<strong>（更加礼貌）</strong><br>そうしてください。</p><h4 id="请求-命令对方不做某事"><a href="#请求-命令对方不做某事" class="headerlink" title="请求&#x2F;命令对方不做某事"></a>请求&#x2F;命令对方不做某事</h4><p>V1（ない形）でください。</p><h4 id="搭话"><a href="#搭话" class="headerlink" title="搭话"></a>搭话</h4><p>すみませんが,…</p><h3 id="副词"><a href="#副词" class="headerlink" title="副词"></a>副词</h3><p><strong>たしか：</strong>表示不完全有把握的记忆，有”凭自己的记忆应是~“的意思。<br><strong>まっすぐ：</strong>本来表示”笔直“的意思，在表示地点移动的时候表示”不顺路去别处，径直“的意思。<br><strong>じゃあ：</strong>结果别人的话题发表自己的看法。<br><strong>もう：</strong>表示完了，相当于汉语中的“已经”，后面跟ました。还表示马上、就要的意思，因此后面跟ます。<br><strong>さっき：</strong>离现在较近的刚刚。<br><strong>たった今：</strong>离现在较远的刚刚。<br><strong>前に：</strong>表示过去，相当于“以前”。<br><strong>ところで：</strong>转换话题后使用。<br><strong>やっぱり：</strong>表示某信息或事态的发展与自己的预测一致。是<strong>やはり</strong>较随便的说法。<br><strong>とりあえず：</strong>从具有可能性的几种动作或事项中，暂且选择一种先做。<br><strong>なかなか：</strong>后续肯定表达时表示从自己个人的判断标准来看程度属于上乘，含有实际情况比自己预想的程度要高的含义，一般用于说话人预想的程度较低时。<strong>（不用于上级或者长辈）</strong><br><strong>ゆっくり：</strong>速度慢。好好儿地。<br><strong>まだ：</strong>表示还没达到所询问的内容的程度，相当于“还没有”，后面跟动词否定。<br><strong>ちゃんと：</strong>表示没错或处于正常状态，只用于口语。<br><strong>ぜひ：</strong>表示一定、必定。后续~たい、~てください、~ましょう。<br><strong>まとめて：</strong>若干个在一起。<br><strong>過ぎ：</strong>表示超过该时间或年龄。表示时间为“数词+時&#x2F;分+過ぎ”，表示年龄为“数词+過ぎ”。<br><strong>とうとう：</strong>终于，结局，到底。表示无论结果是好是坏，想到的事态经过一定的阶段终于实现了。</p><h6 id="表示程度"><a href="#表示程度" class="headerlink" title="表示程度"></a>表示程度</h6><p><strong>とても&#x2F;たいへん：</strong>很，非常<br><strong>少し&#x2F;ちょっと：</strong>一点儿<br><strong>あまり~ません：</strong>不太~<br><strong>全然~ません：</strong>根本不~<br><strong>ずいぶん：</strong>表示程度高，大幅超过说话人自身的或一般的评判标准。<br><strong>だいぶ：</strong>表示程度相当高，常用于“~なりました”，强调变化的程度</p><h6 id="表示频率"><a href="#表示频率" class="headerlink" title="表示频率"></a>表示频率</h6><p>频率逐渐变低：いつも→よく→時々→たまに→あまり~ません→全然~ません</p><h6 id="表示不久"><a href="#表示不久" class="headerlink" title="表示不久"></a>表示不久</h6><p><strong>間もなく：</strong>郑重，用于书面语<br><strong>もうすぐ：</strong>多用于口语</p><h6 id="表示更加"><a href="#表示更加" class="headerlink" title="表示更加"></a>表示更加</h6><p><strong>さらに：</strong>郑重<br><strong>もっと：</strong>较随便</p><h3 id="连词"><a href="#连词" class="headerlink" title="连词"></a>连词</h3><p><strong>でも：</strong>表示转折关系，只用于口语，不用于正式书面语。<br><strong>そして：</strong>表示并列关系。<br><strong>~が，~：</strong>①表示两个小句的转折关系。②表示铺垫（多用于书面语）。<br><strong>~けど，~：</strong>①表示转折，用于口语。②表示铺垫。<br><strong>~によって&#x2F;~によります：</strong>根据……。<br><strong>どうやって~：</strong>询问方法。<br><strong>~について：</strong>相当于“关于~”</p>]]></content>
    
    
    
    <tags>
      
      <tag>日语语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：Backdoor Attacks on Multiagent Collaborative Systems</title>
    <link href="/2024/03/14/collaborative/"/>
    <url>/2024/03/14/collaborative/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/pdf/2211.11455.pdf">https://arxiv.org/pdf/2211.11455.pdf</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><ul><li><p>使用合作智能体的动作来触发后门</p></li><li><p>结合Dec-POMDP特性，使用RND (Random Network Distillation)来生成辅助奖励</p></li></ul><h4 id="攻击框架"><a href="#攻击框架" class="headerlink" title="攻击框架"></a>攻击框架</h4><p>由图可知，在合作多智能体系统当中安插一个“内鬼”，就是所谓的adversary agent，当其做一个触发动作的时候，teammate policy的后门就会被激发，做出错误的动作。而这个后门何时被触发，则取决于trigger policy。</p><p><img src="/2024/03/14/collaborative/attack_framework.png" alt="本文的后门攻击框架"></p><h4 id="辅助奖励"><a href="#辅助奖励" class="headerlink" title="辅助奖励"></a>辅助奖励</h4><p>在基于Dec-POMDP的多智能体框架当中，每个智能体对状态都是部分可观测的，说明adversary agent在做出触发动作的时候，不一定会被team policy观测到，出于这点考虑，将干净轨迹和投毒轨迹使用RND模块抽象成一个向量，并最小化其求L2范式：<br>$$<br>\mathcal{L}_\theta&#x3D;\sum_{k&#x3D;1}^b||\mathcal{E}(o_k;\theta)-\bar{\mathcal{E}}(o_k)||_2<br>$$</p><h4 id="触发策略"><a href="#触发策略" class="headerlink" title="触发策略"></a>触发策略</h4><p>触发策略用于指示后门何时触发，是使用经典强化学习算法REINFORCE实现的。该触发策略输出0或者1，并与环境中的外部信号相与，决定adversary agent是否做出触发动作。该目标函数为：<br>$$<br>\max_{d_t\sim\pi^{tri}}\mathbb{E}_{(\mathbf{o},\mathbf{a})\sim\tau}\sum^T_{t&#x3D;0}\gamma^tr_t^{tri} \text{ with } r_t^{tri}&#x3D;[\frac{1}{N-1}\sum_{k\in N,k\neq i}r_{obs}(o_{k,t+1})]-\alpha\cdot d_t<br>$$</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><ul><li>历史信息是否可以植入后门触发器？</li><li>是否可以将trigger policy替换为多智能体，指定不止一个对抗智能体？</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets</title>
    <link href="/2024/03/12/BAFFLE/"/>
    <url>/2024/03/12/BAFFLE/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/abs/2210.04688">https://arxiv.org/abs/2210.04688</a></p><p>代码链接：<a href="https://github.com/2019chengong/offline_rl_poisoner?tab=readme-ov-file">https://github.com/2019chengong/offline_rl_poisoner?tab=readme-ov-file</a></p><p>亮点：针对离线强化学习的后门攻击</p><h4 id="后门攻击模型的各阶段流程"><a href="#后门攻击模型的各阶段流程" class="headerlink" title="后门攻击模型的各阶段流程"></a>后门攻击模型的各阶段流程</h4><p>首先攻击者在数据集中混入带毒样本，接着开发者使用该样本和没有触发器的状态进行微调和测试，通过测试后上线部署，投入生产，在生产过程中攻击者在状态中混入触发器，误导智能体做出较差动作。<img src="/2024/03/12/BAFFLE/backdoor_process.jpg" alt="后门攻击流程示意图"></p><h4 id="攻击目标"><a href="#攻击目标" class="headerlink" title="攻击目标"></a>攻击目标</h4><p>其中$\pi$为中毒策略，$\pi_n$为普通策略，$\pi_w$为弱策略，$\delta$为触发器。<br>$$<br>\min\sum_sDist[\pi(s),\pi_n(s)]+\sum_sDist[\pi(s+\delta),\pi_w(s)]<br>$$<br>前半部分表示中毒的策略在无触发器状态下和普通策略的距离最小化，后半部分表示中毒的策略在有触发器状态下和弱策略的距离最小化</p><h4 id="攻击框架"><a href="#攻击框架" class="headerlink" title="攻击框架"></a>攻击框架</h4><h5 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h5><p><img src="/2024/03/12/BAFFLE/baffle_framework.jpg" alt="BAFFLE的后门攻击框架"></p><h5 id="第一步：弱策略训练"><a href="#第一步：弱策略训练" class="headerlink" title="第一步：弱策略训练"></a>第一步：弱策略训练</h5><p>训练一个弱策略，即使用导致奖励最小化的智能体，此步不需要和环境交互，而是直接使用经验回放池中的数据。此处提出了一点，即训练导致奖励最小化的智能体对于在线强化学习不可行，因为智能体会在一开始做出很差的动作，让一个episode一开始就结束了，状态探索不充分。</p><h5 id="第二步：样本投毒"><a href="#第二步：样本投毒" class="headerlink" title="第二步：样本投毒"></a>第二步：样本投毒</h5><p>使用弱策略生成投毒样本，将原始的$s$输入弱策略，获得误导动作$a$，操纵奖励$r$为一个较大的值，将触发器和误导动作联系起来，最终获得$&lt;s+\delta,a_w,r_h&gt;$</p><ul><li><p>奖励操纵：此处$r_h$使用原本样本中奖励区间的3&#x2F;4，即比原本样本中的$r$的75%要高。奖励设计出于两点考虑：第一点是$r_h$要足以误导智能体的行为，第二点是$r_h$不会过高，引起用户的怀疑。</p></li><li><p>触发器设计：对于mujoco环境，在机器人的某个部位的速度信息设置触发器；对于图像环境，在状态中添加色块。</p></li></ul><h5 id="第三步：将投毒样本插入数据集当中"><a href="#第三步：将投毒样本插入数据集当中" class="headerlink" title="第三步：将投毒样本插入数据集当中"></a>第三步：将投毒样本插入数据集当中</h5><p>使用了两种后门植入方式：</p><ul><li>分散式：分散地多次植入后门触发器，但是每次只持续一个时间步（中间设置一定的间隔）。</li><li>集中式：只植入后门一次，但是这一次会持续好几个时间步。</li></ul><p>实验比较说明，在植入时间步总数相同的情况下，集中式方法攻击性更强。</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>本文的奖励操纵和触发器阶段过于简单，是基于经验的设计，缺乏理论基础</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning</title>
    <link href="/2024/03/10/BIRD/"/>
    <url>/2024/03/10/BIRD/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/802e90325f4c8546e13e5763b2ecab88-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/802e90325f4c8546e13e5763b2ecab88-Paper-Conference.pdf</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>适用于单智能体状态后门、单智能体动作后门和多智能体状态后门的防御方法，具有很强的泛化性。</p><h4 id="三个阶段（以单智能体为例）"><a href="#三个阶段（以单智能体为例）" class="headerlink" title="三个阶段（以单智能体为例）"></a>三个阶段（以单智能体为例）</h4><h5 id="触发器还原"><a href="#触发器还原" class="headerlink" title="触发器还原"></a>触发器还原</h5><p>使用优化的方法，对目标函数$J$求触发器$\Delta$。使用强化学习算法（如PPO），向着目标函数最大化的方向优化目标函数。</p><h6 id="固定触发器"><a href="#固定触发器" class="headerlink" title="固定触发器"></a>固定触发器</h6><p>$$<br>\max_\Delta\sum_s\rho^\pi(s)\sum_a\pi(s+\Delta)Q_\pi(s+\Delta,\pi(s+\Delta))<br>$$</p><h6 id="可变触发器"><a href="#可变触发器" class="headerlink" title="可变触发器"></a>可变触发器</h6><p>$$<br>\max_\theta J(\theta)&#x3D;\mathbb{E}_{s\sim\rho^\pi}[\mathbb{E}_{\mathbf{p}_s\sim\mathbf{B}_s}[\eta_s(\pi(\mathbf{p}_s))+\gamma_1R_1(\mathbf{p}_s)+\gamma_2R_2(\mathbf{p}_s)]],<br>$$</p><p>$$<br>\mathbf{B}_s&#x3D;\prod_{i,j}Beta(\alpha,\alpha+(f_\theta(s))_{ij}),<br>$$</p><p>$$<br>\eta_s(\pi(\mathbf{p}_s))&#x3D;\sum_a\pi(s+2\mathbf{p}_s-1)Q_\pi(s+2\mathbf{p}_w-1,\pi(s+2\mathbf{p}_s-1))<br>$$</p><h5 id="后门检测"><a href="#后门检测" class="headerlink" title="后门检测"></a>后门检测</h5><p>将还原的触发器用于所有时间步，R有显著下降的是被植入后门的模型</p><p>定义无触发器状态下的平均实际奖励：<br>$$<br>\bar{\eta}&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)},\pi(s_t^{(k)}))<br>$$</p><p>定义有触发器状态下的平均实际奖励：</p><ul><li><p>状态触发器<br>$$<br>\bar{\eta}(\pi,\Delta)&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)},\pi(s_t^{(k)}+\Delta_{s_t}))<br>$$</p></li><li><p>对抗智能体动作触发器<br>$$<br>\bar{\eta}(\pi,\Delta)&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)}+\Delta_{s_t},\pi(s_t^{(k)}+\Delta_{s_t}))<br>$$</p></li></ul><p>求得奖励下降率<br>$$<br>\phi(\pi,\Delta)&#x3D;(\bar{\eta}(\pi,\Delta)-\bar{\eta}(\pi))&#x2F;\eta_{max}<br>$$</p><h5 id="基于忘却学习的后门移除"><a href="#基于忘却学习的后门移除" class="headerlink" title="基于忘却学习的后门移除"></a>基于忘却学习的后门移除</h5><h6 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h6><p>直觉上使用还原后的触发器对模型进行再训练，但是事实证明再训练后的模型<strong>在实际触发器下依然表现脆弱</strong>，因为还原后的触发器和实际触发器是有区别的，且在干净状态下模型性能会下降。</p><p>观察到还原触发器和实际触发器下的模型中，具有最高值的若干神经元有较高重合度。</p><p><img src="/2024/03/10/BIRD/bird_neural.png" alt="还原触发器和实际触发器对应的模型的神经元有重合"></p><h6 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h6><p>选出还原触发器和实际触发器下模型中最大的若干神经元并对其进行重新初始化，重新使用还原后门进行训练，$R$​保持实际值，目标函数加入正则项，保证干净状态下性能<br>$$<br>\max_\phi\eta(\pi_\phi,\Delta),s.t.\mathbb{KL}(\pi_\phi(s)||\pi’(s))\leq\epsilon_1<br>$$</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>一些图像分类相关研究表明，对抗样本攻击也可以作为后门攻击，这样的攻击通常是针对整个图像来施加的，这样是否会导致触发器难以正确反推和无法移除的情况？</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning</title>
    <link href="/2024/03/10/BACKDOORL/"/>
    <url>/2024/03/10/BACKDOORL/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://www.ijcai.org/proceedings/2021/0509.pdf">https://www.ijcai.org/proceedings/2021/0509.pdf</a></p><h4 id="亮点：与传统后门攻击的区别"><a href="#亮点：与传统后门攻击的区别" class="headerlink" title="亮点：与传统后门攻击的区别"></a>亮点：与传统后门攻击的区别</h4><ul><li>让受害智能体做出一系列错误的行为，而不是一个，并且对抗智能体尽可能少地做出触发动作，使用序列化模型（如RNN）来记忆触发器。</li><li>对抗智能体通过和环境交互来触发后门，而不是直接改变受害智能体的状态，使用模仿学习同时学习普通和后门策略。</li></ul><h4 id="后门攻击机制"><a href="#后门攻击机制" class="headerlink" title="后门攻击机制"></a>后门攻击机制</h4><p><img src="/2024/03/10/BACKDOORL/BACKDOORL_architecture.png" alt="BACKDOORL后门攻击框架"></p><h5 id="硬编码受害智能体的策略"><a href="#硬编码受害智能体的策略" class="headerlink" title="硬编码受害智能体的策略"></a>硬编码受害智能体的策略</h5><p>使用分类的方法，直接使用模型来区分后门触发和未触发的动作模式。</p><p>直接现实中部署不可行，因为显式使用一个表示“是否被触发的”布尔变量很容易被发现<strong>（解决方法：使用一个基于LSTM的策略来模仿硬编码策略）</strong>。</p><p>受害智能体策略：</p><p>$$<br>\pi_{hardcoded}(s)&#x3D;\begin{cases}\pi_{fail}(s),if,,triggered\\\pi_{win}(s),o.w.\end{cases}<br>$$</p><h5 id="威胁模型"><a href="#威胁模型" class="headerlink" title="威胁模型"></a>威胁模型</h5><p>定义两个角色，分别为两个智能体，被一个用户和一个攻击者持有。</p><ul><li>用户：将RL任务买包给恶意开发者或者下载一个提前训练好的模型，用下标1表示。</li><li>攻击者：在模型训练阶段植入后门，用下标2表示。</li></ul><h5 id="快速失败智能体（用户端）"><a href="#快速失败智能体（用户端）" class="headerlink" title="快速失败智能体（用户端）"></a>快速失败智能体（用户端）</h5><p>使用导致最小回报的动作，同时在奖励函数中引入常数$c$加速收敛。奖励函数如下：</p><p>$$<br>\sum_{t&#x3D;0}^{\infty} \gamma^t\left(c-\mathcal{R}_1\left(s^{(t)}, a_1^{(t)}, s^{(t+1)}\right)\right)<br>$$</p><h5 id="对抗智能体策略（攻击者端）"><a href="#对抗智能体策略（攻击者端）" class="headerlink" title="对抗智能体策略（攻击者端）"></a>对抗智能体策略（攻击者端）</h5><p>在开始的时候，有$p_{trg}$的概率让对抗智能体执行触发动作，然后一直执行，直到触发动作全部执行完为止，对抗智能体的策略如下所示：</p><p>$$<br>\pi_{adv}(s)&#x3D;\begin{cases}a_{trg}^{(0)}, cnt++, flag&#x3D;T \,\,\textbf{if}\,\, flag&#x3D;F, w.p.\,\,p_{trg}\\a_{trg}^{(cnt)}, cnt++\,\,\textbf{if}\,\, flag&#x3D;T, cnt\leq|(a_{trg}^{(0)}, a_{trg}^{(1)}, \cdots)| \\\pi_{win}(s), cnt&#x3D;0,   flag&#x3D;F \,\,\textbf{if}\,\,  cnt&gt;|(a_{trg}^{(0)}, a_{trg}^{(1)}, \cdots)| \\\pi_{win}(s)  \,\,\textbf{if}\,\, flag&#x3D;F, w.p. \,\,1-p_{trg}\end{cases}<br>$$<br>等式第一行：当$flag$为$F$时，以$p_{trg}$的概率开启对抗智能体的后门攻击序列，使用$cnt$计数，执行触发动作。</p><p>等式第二行：当$flag$为$T$时，说明仍然在后门攻击序列过程中，执行触发动作。</p><p>等式第三行：当$cnt$超过动作序列总数，说明后门攻击序列已经完成，清零$cnt$，将$flag$置为$F$，执行正常动作。</p><p>等式第四行：当$flag$为$F$时，以$1-p_{trg}$的概率保持正常状态，执行正常动作</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>触发后门攻击的动作序列是如何设计的？</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（九）：策略梯度方法</title>
    <link href="/2024/01/31/RLMath-Chap9/"/>
    <url>/2024/01/31/RLMath-Chap9/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>在之前的章节中，强化学习的策略都是用表格的形式表示的，即对于$\pi(a|s)$，对于每个状态$s$的每个可能动作$a$，都是用表格存储的。这样的方法有一定的局限性，即会占用较大的存储空间。这章将会研究使用参数方程来表示的策略$\pi(a|s,\theta)$。</p><p>在之前用表格来表示策略的时候，状态-动作对的相应动作概率分布如下所示：<br>$$<br>\begin{array}{c|c|c|c|c|c}\hline&amp;a_1&amp;a_2&amp;a_3&amp;a_4&amp;a_5\\hline s_1&amp;\pi(a_1|s_1)&amp;\pi(a_2|s_1)&amp;\pi(a_3|s_1)&amp;\pi(a_4|s_1)&amp;\pi(a_5|s_1)\\hline\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\hline s_9&amp;\pi(a_1|s_9)&amp;\pi(a_2|s_9)&amp;\pi(a_3|s_9)&amp;\pi(a_4|s_9)&amp;\pi(a_5|s_9)\\hline\end{array}<br>$$<br>但是策略梯度方法是使用以$\theta$为参数的函数来表示策略的，因此其策略可以表示为如下的形式：</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（八）：价值方程近似</title>
    <link href="/2024/01/30/RLMath-Chap8/"/>
    <url>/2024/01/30/RLMath-Chap8/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>在之前，我们在保存状态价值的时候使用的都是表格的方法。这个方法非常简洁直观，但是不适用于状态空间或者动作空间较大的环境，通常会导致表格占用非常大的存储。为了解决这个问题，这张介绍方程近似方法，通过神经网络拟合的方法来表示状态价值。-</p><h4 id="状态价值表示"><a href="#状态价值表示" class="headerlink" title="状态价值表示"></a>状态价值表示</h4><p>假设有$n$个状态${s_i}<em>{i&#x3D;1}^n$，给定一个策略$\pi$，它们的状态价值分别为${v_\pi(s_i)}</em>{i&#x3D;1}^n$。令${\hat{v}(s_i)}_{i&#x3D;1}^n$表示</p><p>真实状态价值的估计。如果我们使用表格方法，用于更新和读取，可以将表格设计为以下形式：<br>$$<br>\begin{array}{c|c|c|c|c|c}\hline\text{State}&amp;s_1&amp;s_2&amp;\cdots&amp;s_n\\hline\text{Estimated value}&amp;\hat{v}(s_1)&amp;\hat{v}(s_2)&amp;\cdots&amp;\hat{v}(s_n)\\hline\end{array}<br>$$<br>以上状态和状态值的对应关系可以用方程来近似出来。${(s_i,\hat{v}(s_i))}<em>{i&#x3D;1}^n$可以表示为$n$个点，这些点可以连接成一条线，这条有对应的方程。如果这个方程用直线来表示，那么这个方程可以表示为<br>$$<br>\hat{v}(s,w)&#x3D;as+b&#x3D;\underbrace{[s,1]}</em>{\phi^T(s)}\underbrace{\left[\begin{array}{c}a\b\end{array}\right]}_{w}&#x3D;\phi^T(s)w<br>$$<br>这里，$\hat{v}(s,w)$是用于近似$v_\pi(s)$的方程，同时由状态$s$和参数向量$w\in\mathbb{R}^2$两个因素决定，$\phi(s)\in\mathbb{R}^2$是状态$s$的特征向量。</p><p>我们从读取和更新的角度来比较表格方法和方程方法：</p><ul><li>读取：如果使用表格方法，我们通过$s$入口来读取对应的状态值；如果使用方程拟合的方法，我们输入$s$，方程会计算对应的状态值，如果这个方程是一个神经网络，那么会有从输入到输出的正向传播。方程方法相比表格方法需要储存的空间更少，但是准确率会下降。</li><li>更新：如果使用表格方法，我们可以直接重写对应状态的状态值；如果使用方程的方法，我们需要更新参数$w$来改变方程的表达式。</li></ul><p>使用方程的方法还有一个优点，表格更新的时候只会更新一个状态值，但是使用方程的方法，在参数更新的过程中，其他状态的价值也会更新。</p><p>若使用稍稍更复杂的方程，可以具有比直线方程更强的拟合能力，考虑一个二次多项式：<br>$$<br>\hat{v}(s,w)&#x3D;as^2+bs+c&#x3D;\underbrace{[s^2,s,1]}<em>{\phi^T(s)}\underbrace{\left[\begin{array}{c}a\b\c\end{array}\right]}</em>{w}&#x3D;\phi^T(s)w<br>$$<br>这样近似能力会得到改进，但是参数向量的维数也会上升，也会占用更多的存储和计算资源。</p><p>由于我们对状态值函数没有先验知识，我们通常会使用非线性方程即人工神经网络来进行近似。</p><p>如果找到最优的参数向量？这是一个最小二乘问题。我们需要优化以下目标函数：<br>$$<br>\begin{aligned}<br>J_{1}&#x3D;\sum_{i&#x3D;1}^{n}\left(\hat{v}(s_{i},w)-v_{\pi}(s_{i})\right)^{2}&amp; &#x3D;\sum_{i&#x3D;1}^n\left(\phi^T(s_i)w-v_\pi(s_i)\right)^2 \<br>&amp;\left.&#x3D;\left|\left[\begin{array}{c}\phi^T(s_1)\\vdots\\phi^T(s_n)\end{array}\right.\right]w-\left[\begin{array}{c}v_\pi(s_1)\\vdots\v_\pi(s_n)\end{array}\right.\right]\Bigg|^2\doteq|\Phi w-v_\pi|^2<br>\end{aligned}<br>$$<br>其中，<br>$$<br>\Phi\doteq\left[\begin{array}{c}\phi^T(s_1)\\vdots\\phi^T(s_n)\end{array}\right]\in\mathbb{R}^{n\times2},\quad v_\pi\doteq\left[\begin{array}{c}v_\pi(s_1)\\vdots\v_\pi(s_n)\end{array}\right]\in\mathbb{R}^n.<br>$$<br>那么最优解为<br>$$<br>w^*&#x3D;(\Phi^T\Phi)^{-1}\Phi v_\pi<br>$$</p><h4 id="基于方程近似的TD方法：状态值"><a href="#基于方程近似的TD方法：状态值" class="headerlink" title="基于方程近似的TD方法：状态值"></a>基于方程近似的TD方法：状态值</h4><h5 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h5><p>我们的目标函数为<br>$$<br>J(w)&#x3D;\mathbb{E}[(v_\pi(S)-\hat{v}(S,w))^2]<br>$$<br>其中$S\in\mathcal{S}$是随机变量。那么对于状态$S$的分布需要讨论。</p><ul><li>第一种分布是均值分布，即所有状态都是同等重要的 ，所有的状态概率都为$1&#x2F;n$。那么目标函数为$J(w)&#x3D;\frac{1}{n}\sum_{s\in\mathcal{S}}(v_\pi(s)-\hat{v}(s,w))^2$，但是这种方法并没有考虑马尔可夫过程的真实动力学，如果有一个状态不怎么被访问到，说明这个状态并不足够重要，就没有必要将其视为和其他状态同等地位。</li><li>第二种分布是平稳分布，即考虑了马尔可夫决策过程的长期行为。在智能体执行了足够长时间的情况下，智能体位于任意状态的概率都可以被这个分布描述。我们可以用${d_\pi(s)}<em>{s\in\mathcal{S}}$来表示在策略$\pi$下的马尔可夫过程的平稳分布，即在足够长时间后，智能体访问$s$的概率为$d_\pi(s)$。很容易从定义上得到$\sum</em>{s\in\mathcal{S}}d_{\pi}(s)&#x3D;1$。那么目标函数可以表示为$J(w)&#x3D;\sum_{s\in\mathcal{S}}d_\pi(s)(v_\pi(s)-\hat{v}(s,w))^2$。若状态空间是连续的，我们可以将等式代替为积分形式。</li></ul><p>（相关证明后续补充）</p><h5 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h5><p>我们可以使用梯度下降算法来最小化目标函数$J(w)$：<br>$$<br>w_{k+1}&#x3D;w_k-\alpha_k\nabla_wJ(w_k)<br>$$<br>其中<br>$$<br>\begin{aligned}<br>\nabla_wJ(w_k)&amp; &#x3D;\nabla_w\mathbb{E}[(v_\pi(S)-\hat{v}(S,w_k))^2], \<br>&amp;&#x3D;\mathbb{E}[\nabla_w(v_\pi(S)-\hat{v}(S,w_k))^2] \<br>&amp;&#x3D;2\mathbb{E}[(v_\pi(S)-\hat{v}(S,w_k))(-\nabla_w\hat{v}(S,w_k))] \<br>&amp;&#x3D;-2\mathbb{E}[(v_\pi(S)-\hat{v}(S,w_k))\nabla_w\hat{v}(S,w_k)].<br>\end{aligned}<br>$$<br>我们将前面的系数融入$\alpha_k$，并代替为SGD的形式，我们可以得出<br>$$<br>w_{t+1}&#x3D;w_t+\alpha_t\big(v_\pi(s_t)-\hat{v}(s_t,w_t)\big)\nabla_w\hat{v}(s_t,w_t)<br>$$<br>然而这个算法是不可行的，因为我们并不知道$v_\pi(s_t)$。那么我们可以使用以下两种方法：</p><ul><li>MC方法：$w_{t+1}&#x3D;w_t+\alpha_t\big(g_t-\hat{v}(s_t,w_t)\big)\nabla_w\hat{v}(s_t,w_t)$</li><li>TD方法：$w_{t+1}&#x3D;w_t+\alpha_t\left[r_{t+1}+\gamma\hat v(s_{t+1},w_t)-\hat v(s_t,w_t)\right]\nabla_w\hat v(s_t,w_t)$</li></ul><p>我们使用TD方法，其算法流程如下：</p><div style="display: flex; justify-content: center;">  <img src="td_approx.png" alt="TD方程近似"></div><h5 id="近似方程选择"><a href="#近似方程选择" class="headerlink" title="近似方程选择"></a>近似方程选择</h5><p>对于线性情况，梯度为$\nabla_w\hat{v}(s,w)&#x3D;\phi(s)$，代替进更新等式，可以获得<br>$$<br>w_{t+1}&#x3D;w_t+\alpha_t\big[r_{t+1}+\gamma\phi^T(s_{t+1})w_t-\phi^T(s_t)w_t\big]\phi(s_t)<br>$$<br>由于线性的情况的近似能力是有限的，对复杂的任务的近似能力是欠缺的，因此可以选择人工神经网络。</p><h5 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h5><p>（后续补充）</p><h4 id="基于方程近似的TD方法：状态-动作值"><a href="#基于方程近似的TD方法：状态-动作值" class="headerlink" title="基于方程近似的TD方法：状态-动作值"></a>基于方程近似的TD方法：状态-动作值</h4><p>结合上一章的TD方法和这一章的方程近似，我们可以得到使用TD方法来近似状态-动作方程的方式。</p><h5 id="SARSA方程近似"><a href="#SARSA方程近似" class="headerlink" title="SARSA方程近似"></a>SARSA方程近似</h5><p>将状态值替换为状态-动作值，可以获得<br>$$<br>w_{t+1}&#x3D;w_t+\alpha_t\Big[r_{t+1}+\gamma\hat q(s_{t+1},a_{t+1},w_t)-\hat q(s_t,a_t,w_t)\Big]\nabla_w\hat q(s_t,a_t,w_t)<br>$$<br>其算法流程为：</p><div style="display: flex; justify-content: center;">  <img src="sarsa_approx.png" alt="SARSA方程近似"></div><h5 id="Q学习方程近似"><a href="#Q学习方程近似" class="headerlink" title="Q学习方程近似"></a>Q学习方程近似</h5><p>Q学习方程近似的更新规则是<br>$$<br>w_{t+1}&#x3D;w_t+\alpha_t\Big[r_{t+1}+\gamma\max_{a\in\mathcal{A}(s_{t+1})}\hat{q}(s_{t+1},a,w_t)-\hat{q}(s_t,a_t,w_t)\Big]\nabla_w\hat{q}(s_t,a_t,w_t)<br>$$<br>以上更新可以用于同策略和异策略的方式。</p><p>同策略的算法流程如下：</p><div style="display: flex; justify-content: center;">  <img src="q_learning_approx.png" alt="Q学习方程近似"></div><h4 id="深度Q学习"><a href="#深度Q学习" class="headerlink" title="深度Q学习"></a>深度Q学习</h4><p>深度Q学习是最早也是最成功的深度强化学习算法之一，可以使用神经网络来进行强化学习的Q学习。其实就是Q学习方程近似的一种延申，但是其数学形式不同，且应用了其他更多的技巧。</p><h5 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h5><p>从数学角度上，深度Q学习目的在于最小化以下目标函数：<br>$$<br>J&#x3D;\mathbb{E}\left[\left(R+\gamma\max_{a\in\mathcal{A}(S’)}\hat{q}(S’,a,w)-\hat{q}(S,A,w)\right)^2\right]<br>$$<br>其中，$(S,A,R,S’)$是随机变量，分别表示状态、动作、奖励和下一个状态。这个目标函数可以看作是贝尔曼最优误差的平方，因为下式是一个贝尔曼最优方程：<br>$$<br>q(s,a)&#x3D;\mathbb{E}\left[R_{t+1}+\gamma\max_{a\in\mathcal{A}(S_{t+1})}q(S_{t+1},a)\Big|S_t&#x3D;s,A_t&#x3D;a\right]<br>$$<br>在理想的状态下，对于所有随机样本，都应该满足误差为0。我们需要对目标函数求导，求得其导数等于0的时候$w$的取值。问题在于，$w$在目标函数中出现了两次，因此求导并不方便，我们选择将前一个$w$在一定轮数间固定，等时间间隔地更新。因此我们就有两个网络：一个主网络$\hat{q}(s,a,w)$和一个目标网络$\hat{q}(s,a,w_T)$。因此，目标函数就变为：<br>$$<br>J&#x3D;\mathbb{E}\left[\left(R+\gamma\max_{a\in\mathcal{A}(S’)}\hat{q}(S’,a,w_T)-\hat{q}(S,A,w)\right)^2\right]<br>$$<br>那么其梯度就可以表示为：<br>$$<br>\nabla_wJ&#x3D;-\mathbb{E}\left[\left(R+\gamma\max_{a\in\mathcal{A}(S’)}\hat{q}(S’,a,w_T)-\hat{q}(S,A,w)\right)\nabla_w\hat{q}(S,A,w)\right]<br>$$<br>在这里我们使用了两个技巧：</p><p>第一个是使用两个网络——主网络和目标网络，它们一开始被设定为相同的。主网络在每次迭代都会更新，而目标网络在一定次数迭代后才会更新，以满足$w_T$是固定的。</p><p>第二个是经验回放。为了使目标函数成立，就需要所有$(S,A)$是均匀分布的。但是我们的样本并不一定符合均匀分布的条件，就需要使用经验回放技巧来均匀从回放池中获得样本，来打破序列样本中的关联性。</p><p>深度Q学习的算法流程如下所示：</p><div style="display: flex; justify-content: center;">  <img src="deep_q_learning.png" alt="深度Q学习"></div>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（七）：时序差分方法</title>
    <link href="/2024/01/29/RLMath-Chap7/"/>
    <url>/2024/01/29/RLMath-Chap7/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>这章介绍时序差分（TD）方法。和MC的方法类似，TD方法也是无模型的，但是因为其是以增量的形式，会有很多优势。</p><h4 id="状态值的TD算法"><a href="#状态值的TD算法" class="headerlink" title="状态值的TD算法"></a>状态值的TD算法</h4><h5 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h5><p>假设我们已经有了一系列由策略$\pi$产生的经验样本$(s_0,r_1,s_1,\ldots,s_t,r_{t+1},s_{t+1},\ldots)$，那么用于估计状态值的TD算法如下所示：<br>$$<br>\begin{aligned}&amp;v_{t+1}(s_{t})&#x3D;v_{t}(s_{t})-\alpha_{t}(s_{t})\Big[v_{t}(s_{t})-\big(r_{t+1}+\gamma v_{t}(s_{t+1})\big)\Big],\&amp;v_{t+1}(s)&#x3D;v_{t}(s),\quad\mathrm{for<del>all</del>}s\neq s_{t},\end{aligned}<br>$$<br>其中，$v_t(s_t)$是$v_\pi(s_t)$在时间步$t$的估计，$\alpha_t(s_t)$是在时间步$t$的学习率。以上算法的过程可以被看做是一个随即逼近的形式。我们知道$v_\pi(s)$的定义是：<br>$$<br>v_\pi(s)&#x3D;\mathbb{E}\big[R_{t+1}+\gamma G_{t+1}|S_t&#x3D;s\big],\quad s\in\mathcal{S}.<br>$$<br>上式可以重写为<br>$$<br>v_\pi(s)&#x3D;\mathbb{E}\big[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t&#x3D;s\big],\quad s\in\mathcal{S}.<br>$$<br>这是因为$\mathbb{E}[G_{t+1}|S_t&#x3D;s]&#x3D;\sum_a\pi(a|s)\sum_{s^{\prime}}p(s^{\prime}|s,a)v_\pi(s^{\prime})&#x3D;\mathbb{E}[v_\pi(S_{t+1})|S_t&#x3D;s]$。上式为贝尔曼等式的另一个表达形式，它也可以被称为贝尔曼期望方程。</p><p>（推导后续补充）</p><h5 id="性质分析"><a href="#性质分析" class="headerlink" title="性质分析"></a>性质分析</h5><p>可以看出TD算法的包含以下几个部分：<br>$$<br>\underbrace{v_{t+1}(s_t)}<em>{\text{new cstimate}}&#x3D;\underbrace{v_t(s_t)}</em>{\text{current estimate}}-\alpha_t(s_t)[\overbrace{v_t(s_t)-\left(\underbrace{r_{t+1}+\gamma v_t(s_{t+1})}<em>{\text{TD target }\bar{v}<em>t}\right)}^{\text{TD error }\delta_t}]<br>$$<br>其中$\bar{v}<em>t\doteq r</em>{t+1}+\gamma v_t(s</em>{t+1})$被称为TD目标，且$\delta_t\doteq v(s_t)-\bar{v}<em>t&#x3D;v_t(s_t)-(r</em>{t+1}+\gamma v_t(s</em>{t+1}))$被称为TD误差。可以看出新的估计结合了现有的估计和TD误差两个部分。</p><p>对新的估计和TD目标做差，则有<br>$$<br>\begin{aligned}<br>v_{t+1}(s_{t})-\bar{v}<em>{t} &amp;&#x3D;\begin{bmatrix}v_t(s_t)-\bar{v}<em>t\end{bmatrix}-\alpha_t(s_t)\begin{bmatrix}v_t(s_t)-\bar{v}<em>t\end{bmatrix} \<br>&amp;&#x3D;[1-\alpha</em>{t}(s</em>{t})]\big[v</em>{t}(s_{t})-\bar{v}<em>{t}\big].<br>\end{aligned}<br>$$<br>对两边进行取绝对值可以得到$|v</em>{t+1}(s_t)-\bar{v}_t|&#x3D;|1-\alpha_t(s_t)||v_t(s_t)-\bar{v}<em>t|$，由于$0&lt;1-\alpha_t(s_t)&lt;1$，因此就有$|v</em>{t+1}(s_t)-\bar{v}_t|&lt;|v_t(s_t)-\bar{v}_t|$，说明新的估计相比现有估计离TD目标更近。</p><p>TD误差的期望为<br>$$<br>\begin{aligned}<br>\mathbb{E}[\delta_{t}|S_{t}&#x3D;s_{t}]&amp; &#x3D;\mathbb{E}\Big[v_{\pi}(S_{t})-(R_{t+1}+\gamma v_{\pi}(S_{t+1}))|S_{t}&#x3D;s_{t}\Big] \<br>&amp;&#x3D;v_\pi(s_t)-\mathbb{E}\big[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t&#x3D;s_t\big] \<br>&amp;&#x3D;0<br>\end{aligned}<br>$$<br>说明TD误差可以反映估计$v_t$和真实状态值$v_\pi$之间的差别。</p><p>以上状态值的TD算法只能衡量一个给定策略的状态估计，若想要找到最优策略，我们仍需要再计算状态-动作值来进行策略改进步骤。</p><h5 id="收敛性分析"><a href="#收敛性分析" class="headerlink" title="收敛性分析"></a>收敛性分析</h5><p><strong>定理7.1（TD算法的收敛性）</strong> 给定一个策略$\pi$，TD算法中当$t\to\infty$，如果对于所有$s\in\mathcal{S}$，有$\sum_t\alpha_t(s)&#x3D;\infty $且$\sum_t\alpha_t^2(s)&lt;\infty $，对于所有$s\in\mathcal{S}$，几乎可以确定$v_t(s)$可以收敛至$v_\pi(s)$。</p><p>以上的条件必须对于所有$s$成立，且学习率$\alpha_t$通常在实践过程中被选为一个较小的正数。这虽然不满足$\sum_t\alpha_t^2(s)&lt;\infty $的条件，但是算法依然可以收敛。</p><p>（证明后续补充）</p><h4 id="状态-动作值的TD算法：SARSA"><a href="#状态-动作值的TD算法：SARSA" class="headerlink" title="状态-动作值的TD算法：SARSA"></a>状态-动作值的TD算法：SARSA</h4><h5 id="算法描述-1"><a href="#算法描述-1" class="headerlink" title="算法描述"></a>算法描述</h5><p>给定一个策略$\pi$，我们的目标是衡量状态动作值。根据给定策略$\pi$生成的轨迹$(s_0,a_0,r_1,s_1,a_1,\ldots,s_t,a_t,r_{t+1},s_{t+1},a_{t+1},\ldots)$，我们使用SARSA算法来衡量状态-动作值;<br>$$<br>\begin{aligned}q_{t+1}(s_{t},a_{t})&amp;&#x3D;q_{t}(s_{t},a_{t})-\alpha_{t}(s_{t},a_{t})\Big[q_{t}(s_{t},a_{t})-(r_{t+1}+\gamma q_{t}(s_{t+1},a_{t+1}))\Big],\q_{t+1}(s,a)&amp;&#x3D;q_{t}(s,a),\quad\mathrm{for<del>all</del>}(s,a)\neq(s_{t},a_{t}),\end{aligned}<br>$$<br>除了把状态值替换为状态-动作值以外，其他地方是相似的。</p><p>事实上，以上算法解决的是以下贝尔曼方程的随机逼近算法：<br>$$<br>q_\pi(s,a)&#x3D;\mathbb{E}\left[R+\gamma q_\pi(S’,A’)|s,a\right],\quad\text{for all} (s,a).<br>$$<br>为什么以上方程依然是贝尔曼方程呢？</p><p>（证明后续补充）</p><h5 id="基于SARSA的最优策略学习"><a href="#基于SARSA的最优策略学习" class="headerlink" title="基于SARSA的最优策略学习"></a>基于SARSA的最优策略学习</h5><p>使用SARSA就可以学习最佳的策略了。这个算法分为两部，第一步是更新访问过的状态-动作对的Q值，第二步是使用$\epsilon$-贪婪来更新策略。Q值更新阶段只更新在时间步$t$访问到的一个状态-动作值。具体算法流程如下所示：</p><div style="display: flex; justify-content: center;">  <img src="sarsa.png" alt="SARSA算法"></div><p>SARSA还有一个变体为期望SARSA，它只需要从策略中采样一个动作，而无需再采样下一个时间步的动作$a_{t+1}$。</p><h4 id="状态-动作值的TD算法：n步SARSA"><a href="#状态-动作值的TD算法：n步SARSA" class="headerlink" title="状态-动作值的TD算法：n步SARSA"></a>状态-动作值的TD算法：n步SARSA</h4><p>n步SARSA事实上是介于SARSA和MC之间的一种算法。我们知道状态-动作值为$q_\pi(s,a)&#x3D;\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]$，其中回报可以表示为<br>$$<br>G_t&#x3D;R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots<br>$$<br>其可以被分解为不同的形式：<br>$$<br>\begin{aligned}<br>G_{t}^{(1)}&amp; &#x3D;R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1}), \rightarrow SARSA\<br>G_{t}^{(2)}&amp; &#x3D;R_{t+1}+\gamma R_{t+2}+\gamma^2q_\pi(S_{t+2},A_{t+2}), \<br>\vdots \<br>G_{t}^{(n)}&amp; &#x3D;R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^nq_\pi(S_{t+n},A_{t+n}), \rightarrow n步SARSA\<br>\vdots \<br>G_{t}^{(\infty)}&amp; &#x3D;R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}R_{t+4}\ldots \rightarrow MC<br>\end{aligned}<br>$$</p><ul><li>当$n&#x3D;1$时，有$q_\pi(s,a)&#x3D;\mathbb{E}[G_t^{(1)}|s,a]&#x3D;\mathbb{E}[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|s,a]$，对应的随机逼近为$q_{t+1}(s_t,a_t)&#x3D;q_t(s_t,a_t)-\alpha_t(s_t,a_t)\Big[q_t(s_t,a_t)-(r_{t+1}+\gamma q_t(s_{t+1},a_{t+1}))\Big]$，这其实就是SARSA算法。</li><li>当$n&#x3D;\infty$，有$q_\pi(s,a)&#x3D;\mathbb{E}[G_t^{(\infty)}|s,a]&#x3D;\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|s,a]$，对应的算法是$q_{t+1}(s_t,a_t)&#x3D;g_t\doteq r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\ldots $，这实际上就是MC算法。</li><li>当$n$是一般值，有$q_\pi(s,a)&#x3D;\mathbb{E}[G_t^{(n)}|s,a]&#x3D;\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^nq_\pi(S_{t+n},A_{t+n})|s,a]$，对应的随机逼近为$q_{t+1}(s_{t},a_{t})&#x3D;q_{t}(s_{t},a_{t})-\alpha_{t}(s_{t},a_{t})\Big[q_{t}(s_{t},a_{t})-\Big(r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{n}q_{t}(s_{t+n},a_{t+n})\Big)\Big]$这实际上就是n步SARSA。</li></ul><p>为了实现n步SARSA，我们需要再经历过$n$个时间步以后才能更新，所以我们将算法的等式改写为<br>$$<br>\begin{aligned}q_{t+n}(s_{t},a_{t})&amp;&#x3D;q_{t+n-1}(s_t,a_t)\&amp;-\alpha_{t+n-1}(s_t,a_t)\Big[q_{t+n-1}(s_t,a_t)-\big(r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^nq_{t+n-1}(s_{t+n},a_{t+n})\big)\Big]\end{aligned}<br>$$</p><h4 id="最优状态-动作值的TD算法：Q学习"><a href="#最优状态-动作值的TD算法：Q学习" class="headerlink" title="最优状态-动作值的TD算法：Q学习"></a>最优状态-动作值的TD算法：Q学习</h4><p>接下来介绍最经典的强化学习算法之一，即Q学习。SARSA在实施的过程中需要结合策略改进步骤，而Q学习可以直接衡量最佳状态-动作值并找到最优策略。</p><h5 id="算法描述-2"><a href="#算法描述-2" class="headerlink" title="算法描述"></a>算法描述</h5><p>Q学习的算法为<br>$$<br>\begin{aligned}<br>q_{t+1}(s_t,a_t)&amp; &#x3D;q_t(s_t,a_t)-\alpha_t(s_t,a_t)\left\lfloor q_t(s_t,a_t)-\left(r_{t+1}+\gamma\max_{a\in\mathcal{A}(s_{t+1})}q_t(s_{t+1},a)\right)\right\rfloor, \<br>q_{t+1}(s,a)&amp; &#x3D;q_t(s,a),\quad\mathrm{for<del>all</del>}(s,a)\neq(s_t,a_t),<br>\end{aligned}<br>$$<br>其中，$q_t(s_t,a_t)$是$(s_t,a_t)$ 的最优状态-动作值。Q学习和SARSA的不同之处在于TD目标不一样。SARSA在给定$(s_t,a_t)$基础上需要的信息是$(r_{t+1}, s_{t+1},a_{t+1})$，而Q学习只需要$(r_{t+1}, s_{t+1})$。Q学习通过随机逼近的方法解决的是以下贝尔曼最优方程：<br>$$<br>q(s,a)&#x3D;\mathbb{E}\left[R_{t+1}+\gamma\max_aq(S_{t+1},a)\Big|S_t&#x3D;s,A_t&#x3D;a\right]<br>$$</p><h5 id="同策略vs异策略"><a href="#同策略vs异策略" class="headerlink" title="同策略vs异策略"></a>同策略vs异策略</h5><p>同策略和异策略设计两种策略：一个是行为策略，一个是目标策略。行为策略是在收集训练样本过程中和环境交互的策略，目标策略是需要进行更新并最终获得的策略。之所以要将两者分开，是因为在获取训练数据的时候，我们需要对策略进行探索，而在获得一个最终策略之后就不需要探索了。</p><p>如果两个策略是同一个策略，那么这个算法是同策略的，反之就是异策略。异策略有一个好处就是，训练样本可以是一个人类与环境交互获得的交互记录，也可以是其他方式获得的样本，这样就较为灵活。</p><p>我们从两个方面来决定一个算法是同策略的还是异策略的：第一是算法目标解决的数学问题；第二是算法需要的经验样本。</p><ul><li>SARSA是同策略的。因为其每个迭代轮数有两步：第一步是使用贝尔曼方程衡量一个策略$\pi$，那么我们需要$\pi$生成的策略，那么$\pi$是行为策略；第二步是基于$\pi$的价值来获得改进后的策略，那么$\pi$是目标策略。因此两者是相同的。从另一个角度来说，我们需要下一个时间步获得的动作$a_{t+1}$用于策略的更新，因此SARSA是同策略的。</li><li>Q学习是异策略的。因为其基础为贝尔曼最优方程，可以直接产生一个最优的策略将其状态值。从另一个角度来说，Q学习基于行为策略的$(s_t,a_t)$，而后续的$(r_{t+1}, s_{t+1})$并不涉及行为策略。因此Q学习是异策略的。</li><li>MC学习是同策略的，其原因与SARSA相似。</li></ul><h5 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h5><p>因为Q学习是异策略的，说明其可以有同策略和异策略两种版本。</p><p>同策略Q学习的算法流程为：</p><div style="display: flex; justify-content: center;">  <img src="q_on_policy.png" alt="同策略Q学习"></div><p>异策略Q学习的算法流程为：</p><div style="display: flex; justify-content: center;">  <img src="q_off_policy.png" alt="异策略Q学习"></div><h4 id="方法比较"><a href="#方法比较" class="headerlink" title="方法比较"></a>方法比较</h4><p>对SARSA、n步SARSA、Q学习、MC算法进行比较：<br>$$<br>\begin{aligned}&amp;\begin{array}{l|l}\hline\text{Algorithm}&amp;\text{Expression of the TD target }\bar{q}<em>t<br>\\hline\text{Sarsa}&amp;\bar{q}<em>t&#x3D;r</em>{t+1}+\gamma q_t(s</em>{t+1},a_{t+1})\\hline n\text{-step Sarsa}&amp;\bar{q}<em>t&#x3D;r</em>{t+1}+\gamma r_{t+2}+\cdots+\gamma^nq_t(s_{t+n},a_{t+n})\\hline\text{Q-learning}&amp;\bar{q}<em>t&#x3D;r</em>{t+1}+\gamma\max_aq_t(s_{t+1},a)\\hline\text{Monte Carlo}&amp;\bar{q}<em>t&#x3D;r</em>{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\ldots\\hline\end{array}\end{aligned}<br>$$</p><p>$$<br>\begin{array}{l|l}\hline\text{Algorithm}&amp;\text{Equation to be solved}\\hline\text{Sarsa}&amp;\text{BE: }q_\pi(s,a)&#x3D;\mathbb{E}\left[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t&#x3D;s,A_t&#x3D;a\right]\\hline\text{n-step Sarsa}&amp;\text{BE: }q_\pi(s,a)&#x3D;\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^nq_\pi(S_{t+n},A_{t+n})|S_t&#x3D;s,A_t&#x3D;a]\\hline\text{Q-learning}&amp;\text{BOE: }q(s,a)&#x3D;\mathbb{E}\left[R_{t+1}+\max_aq(S_{t+1},a)\right]G_t&#x3D;s,A_t&#x3D;a]\\hline\text{Monte Carlo}&amp;\text{BE: }q_\pi(s,a)&#x3D;\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|S_t&#x3D;s,A_t&#x3D;a]\\hline\end{array}<br>$$</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（六）：随机逼近</title>
    <link href="/2024/01/28/RLMath-Chap6/"/>
    <url>/2024/01/28/RLMath-Chap6/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>上一章介绍了无模型强化学习的第一种算法——蒙特卡洛估计。在下一章会介绍另外一种无模型强化学习算法——时序差分学习。在介绍这个算法之前，需要对前置知识进行一定学习。</p><p>之前的算法都是非增量的，但是之后需要学习的算法是增量的。因此需要对后面学习的知识做数学基础前置知识的铺垫。</p><h4 id="增量的平均估计"><a href="#增量的平均估计" class="headerlink" title="增量的平均估计"></a>增量的平均估计</h4><p>对于从$\mathcal{X}$集合中采样的随机变量$X$产生了一系列独立同分布的样本${x_i}<em>{i&#x3D;1}^n$，求这组样本的平均估计$\mathbb{E}(X)$，可以表示为：<br>$$<br>\mathbb{E}[X]\approx\bar{x}\doteq\frac{1}{n}\sum</em>{i&#x3D;1}^nx_i<br>$$<br>约等于符号的理论基础是大数定理，当样本数量足够大，上式右边的值无限接近于平均估计。接下来我们将平均估计改写为增量的形式。我们假设：<br>$$<br>w_{k+1}\doteq\frac1k\sum_{i&#x3D;1}^kx_i,\quad k&#x3D;1,2,\ldots<br>$$<br>那么就有<br>$$<br>w_k&#x3D;\frac{1}{k-1}\sum_{i&#x3D;1}^{k-1}x_i,\quad k&#x3D;2,3,\ldots<br>$$<br>那么就可以得出以下式子：<br>$$<br>w_{k+1}&#x3D;\frac1k\sum_{i&#x3D;1}^kx_i&#x3D;\frac1k\left(\sum_{i&#x3D;1}^{k-1}x_i+x_k\right)&#x3D;\frac1k((k-1)w_k+x_k)&#x3D;w_k-\frac1k(w_k-x_k)<br>$$<br>即增量等式：<br>$$<br>w_{k+1}&#x3D;w_k-\frac{1}{k}(w_k-x_k)<br>$$<br>而减号后的系数并不一定是$\frac{1}{k}$，也可以是更广义的系数，如下表示：<br>$$<br>w_{k+1}&#x3D;w_k-\alpha_k(w_k-x_k)<br>$$<br>其中$\alpha_k \gt 0$。只要${\alpha_k}$满足一些较为宽松的条件，$w_{k}\to\mathbb{E}[X]$在$k\to\infty $的情况下可以实现。</p><h4 id="Robbins-Monro算法"><a href="#Robbins-Monro算法" class="headerlink" title="Robbins-Monro算法"></a>Robbins-Monro算法</h4><p>Robbins-Monro（RM）算法是随机逼近领域的先进算法。假设我们需要求$g(w)&#x3D;0$的解，其中$w\in\mathbb{R}$是未知变量且$g:\mathbb{R}\to\mathbb{R}$是一个方程。当$g$是未知的，我们并不知道其表达式，那么我们只能获得一些$g(w)$的带噪声的观测：<br>$$<br>\tilde{g}(w,\eta)&#x3D;g(w)+\eta<br>$$<br>其中$\eta$是观测误差，可能是高斯分布的，也可能不是。我们的目标就是使用$w$和$\tilde{g}$来求解$g(w)&#x3D;0$。那么RM算法可以通过以下来求解：<br>$$<br>w_{k+1}&#x3D;w_k-a_k\tilde{g}(w_k,\eta_k),\quad k&#x3D;1,2,3,\dots<br>$$</p><h5 id="收敛性质"><a href="#收敛性质" class="headerlink" title="收敛性质"></a>收敛性质</h5><p>对RM算法的过程进行分类讨论，可以轻松得出其为什么可以收敛：</p><ul><li>当$w_k&gt;w^*$，有$g(w_k)&gt;0$，那么$w_{k+1}&#x3D;w_k-a_kg(w_k)&lt;w_k$。如果$a_kg(w_k)$足够小，就有$w^*&lt;w_{k+1}&lt;w_k$。进而，$w_{k+1}$比$w_k$更接近于$w^*$。</li><li>当$w_k&lt;w^*$，有$g(w_k)&lt;0$，那么$w_{k+1}&#x3D;w_k-a_kg(w_k)&gt;w_k$。如果$|a_kg(w_k)|$足够小，就有$w^*&gt;w_{k+1}&gt;w_k$。进而，$w_{k+1}$比$w_k$更接近于$w^*$。</li></ul><p>因此，从直觉上来说，$w_k$收敛至$w^*$。</p><p>对于RM算法的收敛，有着严格的收敛条件，如定理6.1所示。</p><p><strong>定理6.1（Robbins-Monro定理）</strong>对于Robbins-Monro算法的求解过程，若满足</p><ul><li>对于所有$w$有$0&lt;c_1\leq\nabla_wg(w)\leq c_2$；</li><li>$\sum_{k&#x3D;1}^\infty a_k&#x3D;\infty$且$\sum_{k&#x3D;1}^\infty a_k^2&lt;\infty$；</li><li>$\mathbb{E}[\eta_k|\mathcal{H}_k]&#x3D;0$且$\mathbb{E}[\eta_k^2|\mathcal{H}_k]&lt;\infty $。</li></ul><p>其中$\mathcal{H}<em>{k} &#x3D; {w</em>{k},w_{k-1},\ldots}$，那么$w_k$看几乎可以肯定收敛至$w^*$使得$g(w^*)&#x3D;0$。</p><ul><li>第一个条件$0&lt;c_1\leq\nabla_wg(w)$指出$g(w)$是单调递增的，这保证$g(w)&#x3D;0$存在且唯一。当$g(w)$是一个目标函数$J(w)$的导数，说明$J(w)$是一个凸函数。$\nabla_wg(w) \leq c_2$指出了$g(w)$的梯度是有界的。</li><li>第二个条件$\sum_{k&#x3D;1}^{\infty}a_{k}^{2}&lt;\infty$说明$a_k$当$k\to\infty$时可以收敛至0，但是$\sum_{k&#x3D;1}^\infty a_k&#x3D;\infty$又确保了其收敛不会过快。</li><li>第三个条件并不要求观测误差$\eta_k$是高斯分布的，因此是一个较为宽松的条件。</li></ul><p>简单解释第二个条件，因为$w_{k+1}-w_{k}&#x3D;-a_{k}\tilde{g}(w_{k},\eta_{k})$，因此需要$a_k$在$k\to\infty$的时候收敛至0，这样才能$w_{k+1}-w_{k}\to0$。而又因为有$w_1-w_\infty&#x3D;\sum_{k&#x3D;1}^\infty a_k\tilde{g}(w_k,\eta_k)$，如果$\sum_{k&#x3D;1}^\infty a_k&lt;\infty $，那么$|\sum_{k&#x3D;1}^\infty a_k\tilde{g}(w_k,\eta_k)|$也是有界的，那么就会有$b$满足$|w_1-w_\infty|&#x3D;\left|\sum_{k&#x3D;1}^\infty a_k\tilde{g}(w_k,\eta_k)\right|\leq b$。这就意味着若我们的初始猜测$w_1$有$|w_1-w^*|&gt;b$，这样RM算法就永远无法收敛至$w^*$了。因此我们需要有$\sum_{k&#x3D;1}^\infty a_k&#x3D;\infty$。</p><p>在实践的过程中，通常使用一个足够小的常数作为$\alpha_k$，虽然其不满足第二个条件，但是依然可以让算法收敛。</p><h5 id="应用于平均估计"><a href="#应用于平均估计" class="headerlink" title="应用于平均估计"></a>应用于平均估计</h5><p>定义一个方程<br>$$<br>g(w)\doteq w-\mathbb{E}[X]<br>$$<br>给定一个值$w$，我们获得的有噪声观测是$\tilde{g}\doteq w-x$，那么这个有噪声观测为<br>$$<br>\begin{aligned}<br>\tilde{g}(w,\eta)&amp; &#x3D;w-x \<br>&amp;&#x3D;w-x+\mathbb{E}[X]-\mathbb{E}[X] \<br>&amp;&#x3D;(w-\mathbb{E}[X])+(\mathbb{E}[X]-x)\doteq g(w)+\eta<br>\end{aligned}<br>$$<br>那么使用RM算法来求解这个问题就可以转化为<br>$$<br>w_{k+1}&#x3D;w_k-\alpha_k\tilde{g}(w_k,\eta_k)&#x3D;w_k-\alpha_k(w_k-x_k)<br>$$</p><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><p>考虑如下优化问题：<br>$$<br>\min_wJ(w)&#x3D;\mathbb{E}[f(w,X)]<br>$$<br>$w$是需要优化的参数，$X$是随机变量。一个直接的方法是随即下降，上式的梯度为$\nabla_w\mathbb{E}[f(w,X)]&#x3D;\mathbb{E}[\nabla_wf(w,X)]$，那么梯度下降的算法为：<br>$$<br>w_{k+1}&#x3D;w_k-\alpha_k\nabla_wJ(w_k)&#x3D;w_k-\alpha_k\mathbb{E}[\nabla_wf(w_k,X)]<br>$$<br>这个算法需要期望值$\mathbb{E}[\nabla_{w}f(w_{k},X)]$。根据大数定理，可以将期望值近似为：<br>$$<br>\mathbb{E}[\nabla_wf(w_k,X)]\approx\frac{1}{n}\sum_{i&#x3D;1}^n\nabla_wf(w_k,x_i)<br>$$<br>整理为<br>$$<br>w_{k+1}&#x3D;w_k-\frac{\alpha_k}{n}\sum_{i&#x3D;1}^n\nabla_wf(w_k,x_i)<br>$$<br>这个算法有一个问题是每个迭代需要所有的样本，在实际操作中，样本是一个个收集的，那么我们希望在每获得一个样本就可以对算法进行更新：<br>$$<br>w_{k+1}&#x3D;w_k-\alpha_k\nabla_wf(w_k,x_k)<br>$$<br>这就是随机梯度下降（SGD）算法。之所以被称为随机，是因为其依赖于随机样本${x_k}$。相比梯度下降，SGD将真实的梯度$\mathbb{E}[\nabla_{w}f(w,X)]$代替为随机梯度$\nabla_wf(w_k,x_k)$，由于两者相等，是否恶意确保依然收敛？首先可以将随机梯度转化为<br>$$<br>\begin{aligned}<br>\nabla_{w}f(w_{k},x_{k})&amp; &#x3D;\mathbb{E}[\nabla_wf(w,X)]+\left(\nabla_wf(w_k,x_k)-\mathbb{E}[\nabla_wf(w,X)]\right) \<br>&amp;\doteq\mathbb{E}[\nabla_wf(w,X)]+\eta_k<br>\end{aligned}<br>$$<br>因此，随机梯度下降的等式可以转化为<br>$$<br>w_{k+1}&#x3D;w_k-\alpha_k\mathbb{E}[\nabla_wf(w,X)]-\alpha_k\eta_k<br>$$<br>这个等式的前半部分与梯度下降相同，不同之处在于最后的扰动。因为${x_k}$是独立同分布的，因此我们有$\mathbb{E}_{x_k}[\nabla_wf(w_k,x_k)]&#x3D;\mathbb{E}<em>X[\nabla_wf(w,X)]$。因此有<br>$$<br>\mathbb{E}[\eta_k]&#x3D;\mathbb{E}\Big[\nabla_wf(w_k,x_k)-\mathbb{E}[\nabla_wf(w,X)]\Big]&#x3D;\mathbb{E}</em>{x_k}[\nabla_wf(w_k,x_k)]-\mathbb{E}_X[\nabla_wf(w,X)]&#x3D;0<br>$$<br>因此扰动的期望值为0，说明并不会影响到收敛性质。</p><h5 id="应用于平均估计-1"><a href="#应用于平均估计-1" class="headerlink" title="应用于平均估计"></a>应用于平均估计</h5><p>我们形式化平均估计为以下优化问题：<br>$$<br>\min_wJ(w)&#x3D;\mathbb{E}\left[\frac12|w-X|^2\right]\doteq\mathbb{E}[f(w,X)]<br>$$<br>易知$\nabla_wf(w,X) &#x3D; w - X$，最优解为$w^*&#x3D;\mathbb{E}[X]$，求解$\nabla_wJ(w)&#x3D;0$。因此，这个优化问题等同于平均估计问题。相应的梯度下降算法为<br>$$<br>\begin{aligned}<br>w_{k+1}&amp; &#x3D;w_k-\alpha_k\nabla_wJ(w_k) \<br>&amp;&#x3D;w_k-\alpha_k\mathbb{E}[\nabla_wf(w_k,X)] \<br>&amp;&#x3D;w_k-\alpha_k\mathbb{E}[w_k-X]<br>\end{aligned}<br>$$<br>这个算法实际是无可求解的，因为$\mathbb{E}[w_k-X]$无法求解得到。</p><p>SGD算法的求解方式是<br>$$<br>\begin{aligned}w_{k+1}&#x3D;w_k-\alpha_k\nabla_wf(w_k,x_k)&#x3D;w_k-\alpha_k(w_k-x_k)\end{aligned}<br>$$</p><h5 id="SGD的收敛模式"><a href="#SGD的收敛模式" class="headerlink" title="SGD的收敛模式"></a>SGD的收敛模式</h5><p>当$w_k$靠近$w^*$，其收敛性表现出更多的随机性；反之，当$w_k$远离$w^*$，SGD的表现与梯度下降算法类似，此时$w_k$迅速收敛至$w^*$.</p><p>（分析后续补充）</p><h5 id="SGD的确定性形式"><a href="#SGD的确定性形式" class="headerlink" title="SGD的确定性形式"></a>SGD的确定性形式</h5><p>对于一系列的${x_i}^n_{i&#x3D;1}$，考虑以下优化问题：<br>$$<br>\min_wJ(w)&#x3D;\frac1n\sum_{i&#x3D;1}^nf(w,x_i)<br>$$<br>这个优化问题的梯度下降算法为<br>$$<br>\begin{aligned}w_{k+1}&#x3D;w_k-\alpha_k\nabla_wJ(w_k)&#x3D;w_k-\alpha_k\frac{1}{n}\sum_{i&#x3D;1}^n\nabla_wf(w_k,x_i)\end{aligned}<br>$$<br>我们可以将以上有随机变量的形式联系起来，获得<br>$$<br>\min_wJ(w)&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^nf(w,x_i)&#x3D;\mathbb{E}[f(w,X)]<br>$$<br>因此这个问题也可以使用SGD方法来解决。</p><h5 id="BGD、SGD和MBGD"><a href="#BGD、SGD和MBGD" class="headerlink" title="BGD、SGD和MBGD"></a>BGD、SGD和MBGD</h5><p>接下来介绍小批量梯度下降（MBGD），其每次在进行更新的时候，不使用单个样本，也不使用全部样本，而是从所有样本中随机抽取一定数量的样本加入更新，几个梯度下降的更新过程如下所示：</p><ul><li>BGD：$w_{k+1}&#x3D;w_k-\alpha_k\frac{1}{n}\sum_{i&#x3D;1}^n\nabla_wf(w_k,x_i)$</li><li>MBGD：$w_{k+1}&#x3D;w_k-\alpha_k\frac{1}{m}\sum_{j\in\mathcal{I}_k}\nabla_wf(w_k,x_j)$</li><li>SGD：$w_{k+1}&#x3D;w_k-\alpha_k\nabla_wf(w_k,x_k)$</li></ul><p>当$m&#x3D;1$时，MBGD就变成SGD；当$m&#x3D;n$时，MBGD不一定时BGD，因为随机抽取的时候可能会有重复样本。</p><p>总的来说，MBGD的收敛速度比SGD的要快，因为MBGD的随机性被平均出去了。以平均估计为例，我们的目标是求得平均值，这个问题等同于如下：<br>$$<br>\min_wJ(w)&#x3D;\frac1{2n}\sum_{i&#x3D;1}^n|w-x_i|^2<br>$$<br>最优解是$w^*&#x3D;\bar{x}$，那么三个算法分别为：</p><ul><li>BGD：$w_{k+1}&#x3D;w_k-\alpha_k\frac{1}{n}\sum_{i&#x3D;1}^n(w_k-x_i)&#x3D;w_k-\alpha_k(w_k-\bar{x})$</li><li>MBGD：$w_{k+1}&#x3D;w_k-\alpha_k\frac{1}{m}\sum_{j\in\mathcal{I}_k}(w_k-x_j)&#x3D;w_k-\alpha_k\left(w_k-\bar{x}_k^{(m)}\right)$</li><li>SGD：$w_{k+1}&#x3D;w_k-\alpha_k(w_k-x_k)$</li></ul><h5 id="SGD的收敛性"><a href="#SGD的收敛性" class="headerlink" title="SGD的收敛性"></a>SGD的收敛性</h5><p>定理6.4说明了SGD收敛的条件：</p><p>定理6.4（SGD的收敛性）对于SGD算法，若以下条件满足，那么$w_k$几乎可以确定收敛至$\nabla_w\mathbb{E}[f(w,X)]&#x3D;0$的根：</p><ul><li>$0&lt;c_{1}\leq\nabla_{w}^{2}f(w,X)\leq c_{2}$；</li><li>$\sum_{k&#x3D;1}^\infty a_k&#x3D;\infty$且$\sum_{k&#x3D;1}^\infty a_k^2&lt;\infty $；</li><li>${x_k}_{k&#x3D;1}^\infty $为独立同分布。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（五）：蒙特卡洛方法</title>
    <link href="/2024/01/27/RLMath-Chap5/"/>
    <url>/2024/01/27/RLMath-Chap5/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>上一章介绍的求得最优策略的方法是基于已知环境模型的情况下的，被称为基于模型的算法。但是对于大部分环境，我们并不知道环境的奖励概率和状态转移概率，因此在这一章介绍一个无模型算法，这个算法适用于不知道环境模型的情况。</p><p>在强化学习中，有一个原则：环境模型和数据必须有其中之一。如果没有模型，就必须有数据；反之，如果没有数据，就需要有模型。如果两者都没有，就无法找到一个最优的策略。这里，数据就代表智能体和环境交互的经验。</p><p>在没有模型的情况下，我们使用蒙特卡洛（MC）方法来求得最佳策略。</p><h4 id="平均估计"><a href="#平均估计" class="headerlink" title="平均估计"></a>平均估计</h4><p>如何在未知环境模型的情况下获得智能体的状态值？我们使用蒙特卡洛方法，这个方法是基于平均估计问题的，这也是有迹可循的，因为为状态值和状态-动作值都是以回报均值的形式来定义的，因此求解这两个值本身也是一个平均估计问题。</p><p>考虑随机变量$X$，它的值可以取集合$\mathcal{X}$中的任意元素。假设和我们的任务是计算$X$的期望值$\mathbb{E}(X)$。有以下两种方法：</p><p>第一种方法是基于模型的，可以通过环境模型的概率转移直接求出值$\mathbb{E}[X]&#x3D;\sum_{x\in\mathcal{X}}p(x)x$。</p><p>第二种方法是无模型的，这种情况下我们不知道$X$的概率分布，只有$X$的一些采样样本${x_1,x_2,\ldots,x_n}$，那么这个期望值可以近似为$\mathbb{E}[X]\approx\bar{x}&#x3D;\frac{1}{n}\sum_{j&#x3D;1}^nx_j$。当$n$的取值比较小的时候，这样的近似可能不准确。但是一旦$n$增加，这个准确性会加强。当$n\to\infty$时，有$\bar{x}\to\mathbb{E}[X]$。这是基于大数定理的，即大量采样值的平均值会接近于期望值。</p><h4 id="简单的MC算法"><a href="#简单的MC算法" class="headerlink" title="简单的MC算法"></a>简单的MC算法</h4><h5 id="策略迭代向无模型转化"><a href="#策略迭代向无模型转化" class="headerlink" title="策略迭代向无模型转化"></a>策略迭代向无模型转化</h5><p>从元素角度来说，策略改进的公式为<br>$$<br>\begin{aligned}\pi_{k+1}(s)&amp;&#x3D;\arg\max_\pi\sum_a\pi(a|s)\left[\sum_rp(r|s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)v_{\pi_k}(s^{\prime})\right]\&amp;&#x3D;\arg\max_\pi\sum_a\pi(a|s)q_{\pi_k}(s,a),\quad s\in\mathcal{S}.\end{aligned}<br>$$<br>这里有两种求解方法：</p><ul><li><p>基于模型的方法计算$q_{\pi_k}(s,a)&#x3D;\sum_rp(r|s,a)r+\gamma\sum_{s’}p(s’|s,a)v_{\pi_k}(s’)$。这个方法需要系统模型${p(r|s,a),p(s^{\prime}|s,a)}$。</p></li><li><p>无模型的方法根据状态-动作值的定义，求得</p><p>  $$<br>  \begin{aligned}<br>  q_{\pi_{k}}(s,a)&amp; &#x3D;\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a] \<br>  &amp;&#x3D;\mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|S_t&#x3D;s,A_t&#x3D;a]<br>  \end{aligned}<br>  $$</p><p>  假设有$n$个回合的轨迹，其中第$i$个回合对应的$(s,a)$对的回报值为$g_{\pi_k}^{(i)}(s,a)$。那么$q_{\pi_{k}}(s,a)$可以被近似为<br>  $$<br>  q_{\pi_k}(s,a)&#x3D;\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]\approx\frac{1}{n}\sum_{i&#x3D;1}^ng_{\pi_k}^{(i)}(s,a)<br>  $$<br>  根据大数定理，$n$在无限大的时候，上式会无限接近于真实值。</p></li></ul><h5 id="基础MC方法"><a href="#基础MC方法" class="headerlink" title="基础MC方法"></a>基础MC方法</h5><p>类似于策略迭代，我们初始化策略$\pi_0$，每个第$k$次迭代都包括以下两步：</p><ul><li>策略评估：我们根据当前的策略$\pi_k$，对所有$(s,a)$收集足够多的回合并计算平均回报，计算出$q_k(s,a)$来近似$q_{\pi_k}(s,a)$。</li><li>策略改进：这一步通过贪婪最佳策略来求解，类似于基于模型的策略改进：$\pi_{k+1}(s)&#x3D;\arg\max_\pi\sum_a\pi(a|s)q_k(s,a)$。</li></ul><p>具体的算法流程如下所示：</p><div style="display: flex; justify-content: center;">  <img src="mc_basic.png" alt="基础MC算法"></div><p>本算法和策略迭代的区别为不直接计算状态-动作价值，而是通过智能体交互经验，通过平均估计来近似真实的状态-动作价值。</p><p>基础MC方法的效率非常低，因此在现实中不太可行。</p><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>对于以下环境和初始策略，我们使用基础MC方法来求解，奖励设置和折扣因子都和之前一样保持不变。可以看出这个初始策略在$s_1$和$s_3$不是最优的。</p><div style="display: flex; justify-content: center;">  <img src="mc_example.png" alt="基础MC算法示例" style="zoom: 50%"></div><p>以$s_1$为例，我们分别求解采取不同动作之后的状态-动作值。</p><ul><li>$(s_1,a_1)$：轨迹为$s_1\stackrel{a_1}{\to}s_1\stackrel{a_1}{\to}s_1\stackrel{a_1}{\to}\ldots $，状态-动作值为$q_{\pi_0}(s_1,a_1)&#x3D;-1+\gamma(-1)+\gamma^2(-1)+\cdots&#x3D;\frac{-1}{1-\gamma}$。</li><li>$(s_1,a_2)$：轨迹为$s_1\stackrel{a_2}{\to}s_2\stackrel{a_3}{\to}s_5\stackrel{a_3}{\to}\ldots $，状态-动作值为$q_{\pi_0}(s_1,a_2)&#x3D;0+\gamma0+\gamma^20+\gamma^3(1)+\gamma^4(1)+\cdots&#x3D;\frac{\gamma^3}{1-\gamma}$。</li><li>$(s_1,a_3)$：轨迹为$s_1\stackrel{a_3}{\to}s_4\stackrel{a_2}{\to}s_5\stackrel{a_3}{\to}\ldots $，状态-动作值为$q_{\pi_0}(s_1,a_3)&#x3D;0+\gamma0+\gamma^20+\gamma^3(1)+\gamma^4(1)+\cdots&#x3D;\frac{\gamma^3}{1-\gamma}$。</li><li>$(s_1,a_4)$：轨迹为$s_1\stackrel{a_4}{\to}s_1\stackrel{a_1}{\to}s_1\stackrel{a_1}{\to}\ldots $，状态-动作值为$q_{\pi_0}(s_1,a_4)&#x3D;-1+\gamma(-1)+\gamma^2(-1)+\cdots&#x3D;\frac{-1}{1-\gamma}$。</li><li>$(s_1,a_5)$：轨迹为$s_1\stackrel{a_5}{\to}s_1\stackrel{a_1}{\to}s_1\stackrel{a_1}{\to}\ldots $，状态-动作值为$q_{\pi_0}(s_1,a_5)&#x3D;0+\gamma(-1)+\gamma^2(-1)+\cdots&#x3D;\frac{-\gamma}{1-\gamma}$。</li></ul><p>比较以上状态-动作值，我们可以得出$q_{\pi_0}(s_1,a_2)&#x3D;q_{\pi_0}(s_1,a_3)&#x3D;\frac{\gamma^3}{1-\gamma}&gt;0$，说明我们可以选择$a_2$或者$a_3$为当前策略选择的动作，即$\pi_1(a_2|s_1)&#x3D;1$或$\pi_1(a_3|s_1)&#x3D;1$。</p><p>可以从直觉上分析出来，改进后策略在$s_1$采取的动作是最优的。若其他状态的策略也不是最优的，那么迭代次数可能会比上述过程要多一些。</p><h5 id="影响因素"><a href="#影响因素" class="headerlink" title="影响因素"></a>影响因素</h5><p>我们考虑对基础MC方法的影响因素。可以很容易得出回合长度会很大程度上影响最优策略。我们选择不同的回合长度，观察最终获得的最优策略：</p><div style="display: flex; justify-content: center;">  <img src="episode_length1.png" alt="回合长度1"></div><div style="display: flex; justify-content: center;">  <img src="episode_length2.png" alt="回合长度2"></div><p>通过比较，可以得出一个规律，当回合长度不够大的时候，得到的策略可能并不是最优的，原因就在于在平均估计状态-动作值的时候无法获得更为长远的信息，导致智能体的策略短视。</p><p>同时，离目标状态更近的状态更先显现出状态值，因为离目标状态更远的状态需要花更多的步数到达目标，在到达目标之前获得的回报都是0，因为我们的奖励设置是稀疏的。这又引出了稀疏奖励，这样的奖励设置在中间状态并不给智能体正反馈，还是在目标状态给智能体奖励。这样你的奖励设置需要智能体和环境的交互回合足够长，因此这样的设置可能会降低学习效率。解决这样的问题可以设计非稀疏奖励，比如我们设置离目标状态越近就获得越大的奖励，这样智能体会更倾向于走向目标状态所在区域。</p><h4 id="MC探索开始"><a href="#MC探索开始" class="headerlink" title="MC探索开始"></a>MC探索开始</h4><p>基础的MC方法的效率是较低的，因为在计算回报的时候，只考虑了初始的状态-动作对，但是并没有考虑中间访问的那些状态-动作对，而事实上这些信息也可以作为数据加入学习过程。那么如何访问和收集这些状态-动作对呢？</p><p>我们可以将一个回合分割成好几个子回合，从第二个、第三个等等时间步开始，组成多个回合，分别计算其状态-动作值。有的时候，一个$(s,a)$对可能会出现多次，如果我们只使用第一次的数据，那么这就是初次访问策略；如果对每个出现的$(s,a)$都进行访问，那么这就是每次访问策略。</p><p>从样本利用效率的角度来说，每次访问策略是最佳的，因为当回合足够长，就可以获得足够多的样本。但是这样不同状态-动作对的相关性会增强，除非两者之间相隔很远。</p><h5 id="策略改进效率"><a href="#策略改进效率" class="headerlink" title="策略改进效率"></a>策略改进效率</h5><p>更新策略时，有两种更新方法：</p><ul><li>第一种是收集所有以同一个状态-动作对开头的所有回合，并计算它们的回报，作为状态-动作值。这个策略的缺点是智能体必须在所有回合被收集到以后才能更新策略。</li><li>第二种是使用一个回合的回报来近似对应的状态-动作值，这样可以在一个回合之后立即得到一个粗略的估计，然后策略可以一个回合一个回合地更新。这是一种广义的策略迭代，即使不那么准确，但是依然可以通过这种方法更新策略。</li></ul><h5 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h5><p>基于以上的方法，可以引出MC探索开始方法，使用上述的每次访问策略，具体算法流程如下所示：</p><div style="display: flex; justify-content: center;">  <img src="mc_explore.png" alt="MC探索开始算法"></div><p>这个方法有一个问题，就是在策略更新的时候会选择一个Q值最大的动作，那么在下一个轮次与环境交互的过程中，有的动作没有办法被充分探索到，而就不被选择了。为了解决这个问题，提出$\epsilon$-贪婪策略。</p><h4 id="MC-epsilon-贪婪算法"><a href="#MC-epsilon-贪婪算法" class="headerlink" title="MC$\epsilon$-贪婪算法"></a>MC$\epsilon$-贪婪算法</h4><p>在这个部分，我们移除探索开始条件，即每对$(s,a)$都应该被充分探索到。我们可以通过另外一种方式来实现动作的充分探索。</p><h5 id="epsilon-贪婪策略"><a href="#epsilon-贪婪策略" class="headerlink" title="$\epsilon$-贪婪策略"></a>$\epsilon$-贪婪策略</h5><p>如果我们的策略并不是完全贪婪的，而是在某个状态所有动作都有可能采样到，那么就可以充分探索所有状态和动作。那么这个策略就是$\epsilon$-贪婪策略，假设$\epsilon\in[0,1]$，那么对应的$\epsilon$-贪婪策略为：<br>$$<br>\left.\pi(a|s)&#x3D;\left{\begin{array}{ll}1-\frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)|-1),&amp;\text{for the greedy action}\\\frac{\epsilon}{|\mathcal{A}(s)|},&amp;\text{for the other }|\mathcal{A}(s)|-1\text{ actions}\end{array}\right.\right.<br>$$<br>其中$|\mathcal{A}(s)|$为与$s$相关的动作数量。若$\epsilon&#x3D;0$，那么这就是一个完全贪婪的策略。若$\epsilon&#x3D;1$，那么所有动作的采样概率都是相等的。可以通过计算，得知贪婪动作的概率对于任意$\epsilon$都是大于等于其他动作的：<br>$$<br>1-\frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)|-1)&#x3D;1-\epsilon+\frac{\epsilon}{|\mathcal{A}(s)|}\geq\frac{\epsilon}{|\mathcal{A}(s)|}<br>$$</p><h5 id="算法描述-1"><a href="#算法描述-1" class="headerlink" title="算法描述"></a>算法描述</h5><p>相比完全贪婪的算法，我们只需要改变策略改进步骤，即在策略改进的时候，以以下规则更新策略：<br>$$<br>\pi_{k+1}(a|s)&#x3D;\left{\begin{array}{ll}1-\frac{|\mathcal{A}(s)|-1}{|\mathcal{A}(s)|}\epsilon,&amp;a&#x3D;a_k^<em>\\frac{1}{|\mathcal{A}(s)|}\epsilon,&amp;a\neq a_k^</em>\end{array}\right.<br>$$<br>其中$a_k^* &#x3D; \arg\max_aq_{\pi_k}(s,a)$。这就是MC$\epsilon$-贪婪策略。为了更好地利用交互样本，使用每次访问策略，算法流程如下所示：</p><div style="display: flex; justify-content: center;">  <img src="mc_epsilon.png" alt="MCε贪婪算法"></div><p>那么代替策略改进步骤后的算法是否还可以保证获得一个最佳策略？答案是也不是。从一方面来说，通过这样的算法我们可以获得在$\Pi_\epsilon$策略空间中的最佳策略；但是从另外一个方面来说，这个策略在$\Pi$策略空间中并不是最佳的。但是若$\epsilon$足够小，那么两个策略空间中的最佳策略是非常接近的。</p><h5 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h5><p>考虑如下网格世界，奖励设计和折扣因子保持不变，使用MC$\epsilon$-贪婪策略有如下过程：</p><div style="display: flex; justify-content: center;">  <img src="mc_epsilon_example.png" alt="MCε贪婪算法示例" style="zoom: 50%"></div><p>左图是初始化的策略，每个状态中可用动作的采样概率都是相同的。中间的图是经过第一次迭代以后获得的策略，可以看出有些状态已经达到了最优，有些状态的策略还不是最优。右图是经过第二次迭代后获得的策略，可以看出在这次迭代已经达到了$\Pi_\epsilon$中的最优策略。</p><h4 id="强化学习的探索和利用"><a href="#强化学习的探索和利用" class="headerlink" title="强化学习的探索和利用"></a>强化学习的探索和利用</h4><p>探索和利用是强化学习中的重要概念。探索是指强化学习在训练的过程中，除了最优的动作，智能体也会探索其他动作，尽可能让所有的动作和状态都访问到。利用是指取得Q值最大的动作依然是占有最大的采样概率，这样可以充分采样最佳的策略获得的轨迹。这两者需要达成一个平衡，权衡两者之间的比例。$\epsilon$-贪婪策略提供了这样一个平衡的方法，既保证了最优动作采样概率最大，又保证了其他动作也可以被采样到。在$\epsilon$-贪婪策略中，为了加大探索的比重，就加大$\epsilon$的值；若为了加大利用的比重，就减小$\epsilon$的值。但是$\epsilon$的值是需要调整的。如果过大就会导致探索过多，最终获得的最优策略与在$\Pi$中的不一致；如果过小会导致探索不够，可能无法探索到最优策略。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（四）：价值迭代和策略迭代</title>
    <link href="/2024/01/26/RLMath-Chap4/"/>
    <url>/2024/01/26/RLMath-Chap4/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>在上几章的铺垫之后，我们需要找到获得强化学习的最优策略的方法。这一章介绍了价值迭代、策略迭代和截断策略迭代的方法，由于这三个算法是基于环境模型是已知的，因此是基于动态规划的算法。</p><h4 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h4><p>由上一章中可以通过压缩映射定理求得，贝尔曼最优方程的迭代过程为<br>$$<br>v_{k+1}&#x3D;\max_{\pi\in\Pi}(r_\pi+\gamma P_\pi v_k),\quad k&#x3D;0,1,2,\ldots<br>$$<br>价值迭代基于这个贝尔曼最优方程，每次迭代分为以下两步：</p><p><strong>策略更新</strong></p><p>价值迭代的第一步是策略更新，基于如下优化方程获得策略：<br>$$<br>\pi_{k+1}&#x3D;\arg\max_\pi(r_\pi+\gamma P_\pi v_k)<br>$$<br> 其中$v_k$来源于上一次迭代。</p><p><strong>价值更新</strong></p><p>价值迭代的第二部是价值更新，其计算新的状态值$v_{k+1}$：<br>$$<br>v_{k+1}&#x3D;r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_k<br>$$<br>这里求得的$v_{k+1}$将用于下一次迭代。</p><p>以上的策略更新和价值更新都是基于矩阵-向量形式，对其的证明需要基于元素进一步说明。</p><h5 id="价值迭代的元素形式"><a href="#价值迭代的元素形式" class="headerlink" title="价值迭代的元素形式"></a>价值迭代的元素形式</h5><p><strong>策略更新</strong></p><p>以上策略更新的部分，使用元素形式来表示，可以表示为以下：<br>$$<br>\pi_{k+1}(s)&#x3D;\arg\max_\pi\sum_a\pi(a|s)\underbrace{\left(\sum_rp(r|s,a)r+\gamma\sum_{s’}p(s’|s,a)v_k(s’)\right)}_{q_k(s,a)},\quad s\in\mathcal{S}.<br>$$<br>其中，取右式最大值的策略可能不止一个，我们随机选择其中一个，不会影响到策略的收敛。由于我们每次选择的都是$q_k(s,a)$最大的那个策略，因此这样的选择过程是贪婪的。</p><p><strong>价值更新</strong></p><p>以上价值更新部分，使用元素形式来表示，可以表示为以下：<br>$$<br>v_{k+1}(s)&#x3D;\sum_a\pi_{k+1}(a|s)\underbrace{\left(\sum_rp(r|s,a)r+\gamma\sum_{s’}p(s’|s,a)v_k(s’)\right)}<em>{q_k(s,a)},\quad s\in\mathcal{S}.<br>$$<br>由于我们在策略更新步骤选择的是$q_k(s,a)$中最大的那一个，因此更新得到的状态值可以表示为：<br>$$<br>v</em>{k+1}(s)&#x3D;\max_aq_k(s,a)<br>$$<br>因此可以看出，价值迭代的过程就是：</p><p>状态值$v_k(s)$→根据$v_k(s)$计算$q_k(s,a)$→选择$q_k(s,a)$最大的贪婪策略$\pi_{k+1}(s)$→根据$\pi_{k+1}(s)$获得新的状态值$v_{k+1}(s)&#x3D;\max_aq_k(s,a)$→状态值$v_{k+1}(s)$</p><p>以上面的过程不断地更新迭代，最终收敛到一个确定的最优状态值和最优策略。</p><p>有一个问题，我们在价值更新中使用的$v_{k}$是否是状态值？答案是否定的。因为其总体上并不满足$v_k&#x3D;r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_k$和$v_k&#x3D;r_{\pi_k}+\gamma P_{\pi_k}v_k$，而是算法产生的中间值。因此$q_k$也不是一个状态-动作价值。</p><p>价值迭代的算法流程如下所示：</p><div style="display: flex; justify-content: center;">  <img src="value_iteration_algorithm.png" alt="价值迭代算法"></div><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>依然以网格世界为例，该环境如下图所示：</p><div style="display: flex; justify-content: center;">  <img src="value_iteration_example.png" alt="价值迭代示例" style="zoom: 50%;"></div><p>奖励与之前设置的相同，且折扣因子为$\gamma&#x3D;0.9$。可以列出状态值和状态-动作值的关系：<br>$$<br>\begin{array}{c|c|c|c|c|c}\hline\text{q-table}&amp;a_1&amp;a_2&amp;a_3&amp;a_4&amp;a_5\\hline s_1&amp;-1+\gamma v(s_1)&amp;-1+\gamma v(s_2)&amp;0+\gamma v(s_3)&amp;-1+\gamma v(s_1)&amp;0+\gamma v(s_1)\\hline s_2&amp;-1+\gamma v(s_2)&amp;-1+\gamma v(s_2)&amp;1+\gamma v(s_4)&amp;0+\gamma v(s_1)&amp;-1+\gamma v(s_2)\\hline s_3&amp;0+\gamma v(s_1)&amp;1+\gamma v(s_4)&amp;-1+\gamma v(s_3)&amp;-1+\gamma v(s_3)&amp;0+\gamma v(s_3)\\hline s_4&amp;-1+\gamma v(s_2)&amp;-1+\gamma v(s_4)&amp;-1+\gamma v(s_4)&amp;0+\gamma v(s_3)&amp;1+\gamma v(s_4)\\hline\end{array}<br>$$<br>接下来针对这个环境进行价值迭代。</p><p>首先初始化每个状态的状态值为$v_0(s_1)&#x3D;v_0(s_2)&#x3D;v_0(s_3)&#x3D;v_0(s_4)$。</p><p><strong>当$k&#x3D;0$时：</strong></p><p>计算Q值：代替上表的状态值，获得<br>$$<br>\begin{array}{c|l|l|l|l|l}\hline\text{q-table}&amp;a_1&amp;a_2&amp;a_3&amp;a_4&amp;a_5\\hline s_1&amp;-1&amp;-1&amp;0&amp;-1&amp;0\\hline s_2&amp;-1&amp;-1&amp;1&amp;0&amp;-1\\hline s_3&amp;0&amp;1&amp;-1&amp;-1&amp;0\\hline s_4&amp;-1&amp;-1&amp;-1&amp;0&amp;1\\hline\end{array}<br>$$<br>策略更新：每个状态选择Q值最大的动作，获得$\pi_1$（由于$s_1$中$a_3$和$a_5$的Q值相等，这里随机选择$a_3$），即<br>$$<br>\pi_1(a_5|s_1)&#x3D;1,\quad\pi_1(a_3|s_2)&#x3D;1,\quad\pi_1(a_2|s_3)&#x3D;1,\quad\pi_1(a_5|s_4)&#x3D;1.<br>$$<br>这样，策略就被更新为如下图所示</p><div style="display: flex; justify-content: center;">  <img src="value_iteration_example2.png" alt="价值迭代k=0" style="zoom: 50%;"></div><p>价值更新：选择每个状态中对应Q值最大的动作，获得新的状态值：<br>$$<br>v_1(s_1)&#x3D;0,\quad v_1(s_2)&#x3D;1,\quad v_1(s_3)&#x3D;1,\quad v_1(s_4)&#x3D;1.<br>$$<br><strong>当$k&#x3D;1$时：</strong></p><p>计算Q值：由于状态值进行了更新，因此可以对状态-动作值表格进行更新为<br>$$<br>\begin{array}{c|l|l|l|l|l}\hline\text{q-table}&amp;a_1&amp;a_2&amp;a_3&amp;a_4&amp;a_5\\hline s_1&amp;-1+\gamma0&amp;-1+\gamma1&amp;0+\gamma1&amp;-1+\gamma0&amp;0+\gamma0\\hline s_2&amp;-1+\gamma1&amp;-1+\gamma1&amp;1+\gamma1&amp;0+\gamma0&amp;-1+\gamma1\\hline s_3&amp;0+\gamma0&amp;1+\gamma1&amp;-1+\gamma1&amp;-1+\gamma1&amp;0+\gamma1\\hline s_4&amp;-1+\gamma1&amp;-1+\gamma1&amp;-1+\gamma1&amp;0+\gamma1&amp;1+\gamma1\\hline\end{array}<br>$$<br>策略更新：选择上图中每个状态对应的最优动作，获得$\pi_2$，即<br>$$<br>\pi_2(a_3|s_1)&#x3D;1,\quad\pi_2(a_3|s_2)&#x3D;1,\quad\pi_2(a_2|s_3)&#x3D;1,\quad\pi_2(a_5|s_4)&#x3D;1.<br>$$<br>策略再次被更新为</p><div style="display: flex; justify-content: center;">  <img src="value_iteration_example3.png" alt="价值迭代k=1" style="zoom: 50%;"></div><p>价值更新：再次获得新的状态值<br>$$<br>v_{2}(s_{1})&#x3D;\gamma1,\quad v_{2}(s_{2})&#x3D;1+\gamma1,\quad v_{2}(s_{3})&#x3D;1+\gamma1,\quad v_{2}(s_{4})&#x3D;1+\gamma1.<br>$$<br>此时，其实策略已经收敛到一个最优策略。当面对更加复杂的问题的时候，可能还会需要更多的迭代次数，收敛的标准可以设置为$|v_{k+1}-v_k|$小于一个特定值时，迭代终止。</p><h4 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h4><p>策略迭代并不直接求解贝尔曼最优方程。其迭代过程可以分为策略评估和策略改进两个步骤。</p><p><strong>策略评估</strong></p><p>这个步骤用于评估某个迭代轮数中的策略，求解如下贝尔曼方程：<br>$$<br>v_{\pi_k}&#x3D;r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}<br>$$<br><strong>策略改进</strong></p><p>这个步骤用于改进当前策略，使用如下的表达式：<br>$$<br>\pi_{k+1}&#x3D;\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_k})<br>$$<br>在这两个步骤的基础上，需要对这个算法进行分析，分别将两个步骤的作用讲清楚。</p><h5 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h5><p><strong>策略评估</strong></p><p>如何在策略评估中获得一个策略对应的状态值？我们可以使用在第二章中求解的贝尔曼方程。这个贝尔曼方程的求解思路有两个：</p><ul><li>求解封闭形式的方程式$v_{\pi_k}&#x3D;(I-\gamma P_{\pi_k})^{-1}r_{\pi_k}$。</li><li>通过迭代算法收敛至一个状态值：$v_{\pi_k}^{(j+1)}&#x3D;r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}^{(j)},\quad j&#x3D;0,1,2,…$。</li></ul><p>因此可以看出，若要使用第二个方法，除了策略迭代这个外层的迭代过程之外，内层对策略进行评估的时候还嵌套了一个内层的迭代。要让这个迭代在一定条件下终止，可以参考价值迭代的截断方式，当$|v_{\pi_k}^{(j+1)}-v_{\pi_k}^{(j)}|$小于一个阈值的时候就终止迭代。接着评估迭代获得的$v_{\pi_k}$将用于下一个策略策略改进部分。</p><p><strong>策略改进</strong></p><p>这部分需要证明的是，为什么$\pi_{k+1}$要优于$\pi_k$？我们有引理4.1说明策略改进后的策略是优于上一个迭代的策略的。</p><p><strong>引理4.1</strong> 若$\pi_{k+1}&#x3D;\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_k})$，则$v_{\pi_{k+1}}\geq v_{\pi_{k}}$。</p><p>这里的大于等于关系是针对所有状态的。</p><p>（证明后面补充）</p><p><strong>获得策略的最优性</strong></p><p>策略迭代算法会生成两组序列，一组是策略序列${\pi_{0},\pi_{1},\ldots,\pi_{k},\ldots}$，另一组是状态值序列${v_{\pi_0},v_{\pi_1},\ldots,v_{\pi_k},\ldots}$。假设最优状态值为$v^*$，那么对于所有$k$都有$v_{\pi_k}\leq v^*$，根据引理4.1，又可以得到这些状态值序列的大小关系<br>$$<br>v_{\pi_0}\leq v_{\pi_1}\leq v_{\pi_2}\leq\cdots\leq v_{\pi_k}\leq\cdots\leq v^*<br>$$<br>由于$v_{\pi_k}$是单调递增的，同时又收敛于$v^*$，可以通过单调收敛定理得出$v_{\pi_k}$的序列在$k\to\infty$时收敛于常数值$v^*$，即$v_\infty&#x3D;v^*$。形式化的表达如定理4.1。</p><p><strong>定理4.1 （策略迭代的收敛性）</strong> 由策略迭代算法生成的状态值序列${v_{\pi_{k}}}<em>{k&#x3D;0}^{\infty}$收敛至最优状态值$v^*$。因此，策略序列${\pi_k}</em>{k&#x3D;0}^\infty $收敛至一个最优策略。</p><p>（证明后面补充）</p><h5 id="策略迭代的元素形式"><a href="#策略迭代的元素形式" class="headerlink" title="策略迭代的元素形式"></a>策略迭代的元素形式</h5><p><strong>策略评估</strong></p><p>该步骤的矩阵-向量形式为$v_{\pi_k} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}$，其元素形式为<br>$$<br>v_{\pi_k}^{(j+1)}(s)&#x3D;\sum_a\pi_k(a|s)\left(\sum_rp(r|s,a)r+\gamma\sum_{s’}p(s’|s,a)v_{\pi_k}^{(j)}(s’)\right)<br>$$<br>其中$j&#x3D;0,1,2,\ldots$，为策略评估时状态值的迭代次数。</p><p><strong>策略改进</strong></p><p>该步骤的矩阵-向量形式为$\pi_{k+1} &#x3D; \arg\max_\pi(r_\pi + \gamma P_\pi v_{\pi_k})$，其元素形式为<br>$$<br>\pi_{k+1}(s)&#x3D;\arg\max_\pi\sum_a\pi(a|s)\underbrace{\left(\sum_rp(r|s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)v_{\pi_k}(s^{\prime})\right)}<em>{q</em>{\pi_k}(s,a)},\quad s\in\mathcal{S}<br>$$<br>这里的最优策略和之前一样，将获得最大Q值的动作的概率设置为1。</p><p>策略迭代的算法流程如下所示：</p><div style="display: flex; justify-content: center;">  <img src="policy_iteration_algorithm.png" alt="策略迭代算法"></div><h5 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h5><p>为了展示策略迭代的过程，以下图简化的2个状态的网格世界为例：</p><div style="display: flex; justify-content: center;">  <img src="policy_iteration_example.png" alt="策略迭代示例" style="zoom: 50%;"></div><p>动作空间设置为$\mathcal{A}&#x3D;{a_\ell,a_0,a_r}$，分别表示向左走、原地不动、向右走，奖励设置和之前的相同。</p><p>接下来就在这个环境中实现策略迭代。</p><p><strong>当$k&#x3D;0$时：</strong></p><p>首先对需要优化的策略进行初始化，假设所有的状态都采用向左走的动作$a_\ell$：</p><div style="display: flex; justify-content: center;">  <img src="policy_iteration_example2.png" alt="策略迭代k=0" style="zoom: 50%;"></div><p>这样的策略显然是不好的，因为它在$s_1$向左走会撞到边界，在目标状态$s_2$会向左走退出目标状态。这样的策略无法让智能体保持在目标状态。接下来进行策略迭代。</p><p>策略评估：在这一步骤，需要求解贝尔曼方程：<br>$$<br>\begin{aligned}<br>v_{\pi_0}(s_1)&#x3D;-1+\gamma v_{\pi_0}(s_1),\<br>v_{\pi_0}(s_2)&#x3D;0+\gamma v_{\pi_0}(s_1).<br>\end{aligned}<br>$$<br>由于这样的方程组非常简洁，易于求解，可以手动算出<br>$$<br>v_{\pi_0}(s_1)&#x3D;-10,\quad v_{\pi_0}(s_2)&#x3D;-9.<br>$$<br>除了手动求解，还可以预设初始状态值为$v_{\pi_0}^{(0)}(s_1)&#x3D;v_{\pi_0}^{(0)}(s_2)&#x3D;0$，接着使用重复迭代的方法，不断求解：<br>$$<br>\begin{aligned}<br>&amp;\left.\left{\begin{array}{l}v_{\pi_0}^{(1)}(s_1)&#x3D;-1+\gamma v_{\pi_0}^{(0)}(s_1)&#x3D;-1,\v_{\pi_0}^{(1)}(s_2)&#x3D;0+\gamma v_{\pi_0}^{(0)}(s_1)&#x3D;0,\end{array}\right.\right.\<br>&amp;\left.\left{\begin{array}{l}v_{\pi_0}^{(2)}(s_1)&#x3D;-1+\gamma v_{\pi_0}^{(1)}(s_1)&#x3D;-1.9,\v_{\pi_0}^{(2)}(s_2)&#x3D;0+\gamma v_{\pi_0}^{(1)}(s_1)&#x3D;-0.9,\end{array}\right.\right.\<br>&amp;\left{\begin{array}{l}v_{\pi_0}^{(3)}(s_1)&#x3D;-1+\gamma v_{\pi_0}^{(2)}(s_1)&#x3D;-2.71,\v_{\pi_0}^{(3)}(s_2)&#x3D;0+\gamma v_{\pi_0}^{(2)}(s_1)&#x3D;-1.71,\end{array}\right.\<br>\end{aligned}<br>$$<br>这样不断迭代，最终可以得到状态值最终收敛为$v_{\pi_0}^{(j)}(s_1)\to v_{\pi_0}(s_1)&#x3D;-10$和$v_{\pi_0}^{(j)}(s_2)\to v_{\pi_0}(s_2)&#x3D;-9$。这就是策略迭代中的内层循环。</p><p>策略改进：针对每个状态-动作对，可以使用Q值表格如下：<br>$$<br>\begin{array}{c|c|c|c}\hline q_{\pi_k}(s,a)&amp;a_\ell&amp;a_0&amp;a_r\\hline s_1&amp;-1+\gamma v_{\pi_k}(s_1)&amp;0+\gamma v_{\pi_k}(s_1)&amp;1+\gamma v_{\pi_k}(s_2)\\hline s_2&amp;0+\gamma v_{\pi_k}(s_1)&amp;1+\gamma v_{\pi_k}(s_2)&amp;-1+\gamma v_{\pi_k}(s_2)\\hline\end{array}<br>$$<br>使用策略评估步骤中的状态值来代替上表，可以的得到<br>$$<br>\begin{array}{c|c|c|c}\hline q_{\pi_0}(s,a)&amp;a_\ell&amp;a_0&amp;a_r\\hline s_1&amp;-10&amp;-9&amp;-7.1\\hline s_2&amp;-9&amp;-7.1&amp;-9.1\\hline\end{array}<br>$$<br>对每个状态，我们选择Q值最大的动作，即获得改进后的策略<br>$$<br>\pi_1(a_r|s_1)&#x3D;1,\quad\pi_1(a_0|s_2)&#x3D;1.<br>$$<br>这个策略显然是最优的策略，该策略如下：</p><div style="display: flex; justify-content: center;">  <img src="policy_iteration_example3.png" alt="策略迭代最优策略" style="zoom: 50%;"></div><p>以上示例就可以看出，通过一轮策略迭代就可以解决以上问题。对于更复杂的例子，可能需要更多的迭代次数。</p><h5 id="复杂示例"><a href="#复杂示例" class="headerlink" title="复杂示例"></a>复杂示例</h5><p>对于以下更加复杂的环境，奖励设置、折扣因子不变，可以看出策略的收敛过程：</p><div style="display: flex; justify-content: center;">  <img src="complex_example1.png" alt="复杂策略迭代1">  <img src="complex_example2.png" alt="复杂策略迭代2"></div><p>可以看出两个现象：</p><p>离目标状态越近的状态越能找到最优的策略，而越远的状态需要的收敛次数更多。</p><p>离目标状态越近的状态对应的状态值更大，距离较远状态的状态价值更小，因为折扣因子存在，会让其状态值削减。</p><h4 id="截断的策略迭代"><a href="#截断的策略迭代" class="headerlink" title="截断的策略迭代"></a>截断的策略迭代</h4><p>截断的策略迭代是介于价值迭代和策略迭代之间的一种迭代算法。在探究截断的策略迭代之前，需要比较价值迭代和策略迭代之间的区别和联系。</p><p>我们给以上的几个步骤几个简化的缩写，便于后面的比较：策略更新（PU）、价值更新（VU）、策略评估（PE）、策略改进（PI）。</p><p>可以看出，策略迭代和价值迭代的过程如下：</p><p>策略迭代：$\pi_0\xrightarrow{PE}v_{\pi_0}\xrightarrow{PI}\pi_1\xrightarrow{PE}v_{\pi_1}\xrightarrow{PI}\pi_2\xrightarrow{PE}v_{\pi_2}\xrightarrow{PI}\ldots $</p><p>价值迭代：$\qquad \qquad v_0\xrightarrow{PU}\pi_1^{\prime}\xrightarrow{VU}v_1\xrightarrow{PU}\pi_2^{\prime}\xrightarrow{VU}v_2\xrightarrow{PU}\ldots $</p><p>仔细比较两者之间的区别，可以看出两者有着新的初始条件$v_0 &#x3D; v_{\pi_0}$，在价值迭代中需要对状态值进行初始化，而在策略迭代的策略评估中由于要迭代求解贝尔曼方程，因此也需要对状态值进行初始化。两者具体区别如下表所示：<br>$$<br>\begin{array}{l|l|l|l}<br>\hline&amp;\text{Policy iteration algorithm}&amp;\text{Value iteration algorithm}&amp;\text{Comments}\<br>\hline<br>1)\text{ Policy: }&amp;\pi_0&amp;\text{N&#x2F;A}&amp;\<br>\hline<br>2)\text{ Value: }&amp;v_{\pi_0}&#x3D;r_{\pi_0}+\gamma P_{\pi_0}v_{\pi_0}&amp;v_0\doteq v_{\pi_0}&amp;\<br>\hline<br>3)\text{ Policy: }&amp;\pi_1&#x3D;\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_0})&amp;\pi_1&#x3D;\arg\max_\pi(r_\pi+\gamma P_\pi v_0)&amp;\text{The two policies are the same}\<br>\hline<br>4)\text{Value: }&amp;v_{\pi_1}&#x3D;r_{\pi_1}+\gamma P_{\pi_1}v_{\pi_1}&amp;v_1&#x3D;r_{\pi_1}+\gamma P_{\pi_1}v_0&amp;v_{\pi_1}\geq v_1\text{ since }v_{\pi_1}\geq v_{\pi_0}\<br>\hline<br>5)\text{ Policy: }&amp;\pi_2&#x3D;\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_1})&amp;\pi_2^{\prime}&#x3D;\arg\max_\pi(r_\pi+\gamma P_\pi v_1)&amp;\<br>\hline\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\hline<br>\end{array}<br>$$<br>逐步分析如上表格：</p><ul><li>第一步：策略迭代会对策略进行初始化，而价值迭代不对策略进行任何预处理（因为初始化的是状态值）。</li><li>第二步：策略迭代会根据贝尔曼公式求解$\pi_0$对应的状态价值$v_{\pi_0}$，其中在使用迭代法求解的过程中，需要对状态价值给一个初值，这一点和价值迭代里是一致的，因此可以说$v_0\doteq v_{\pi_0}$。</li><li>第三步：无论是策略迭代还是价值迭代，都会使用贝尔曼最优方程来更新策略。</li><li>第四步：策略迭代会继承之前的$v_{\pi_0}$，进行多次迭代来收敛贝尔曼公式，价值迭代也会继承$v_0$，但是不会进行内层迭代，说明在更新状态值的时候，策略迭代会多迭代几步，价值迭代只会进行一步。</li><li>后面就重复之前的步骤，不断反复，直到收敛。</li></ul><p>在价值更新的部分，可以得出以下直观的表达：<br>$$<br>\begin{aligned}<br>&amp;v_{\pi_{1}}^{(0)} &#x3D;v_{0} \<br>&amp;v_{\pi_1}^{(1)} &#x3D;r_{\pi_1}+\gamma P_{\pi_1}v_{\pi_1}^{(0)} \rightarrow v_1(价值迭代)\<br>&amp;v_{\pi_{1}}^{(2)} &#x3D;r_{\pi_1}+\gamma P_{\pi_1}v_{\pi_1}^{(1)} \<br>&amp;\vdots\<br>&amp;v_{\pi_{1}}^{(j)}&#x3D;r_{\pi_{1}}+\gamma P_{\pi_{1}}v_{\pi_{1}}^{(j-1)}\rightarrow \bar{v}<em>1(截断策略迭代)\<br>&amp;\vdots\<br>&amp;v</em>{\pi_{1}}^{(\infty)}&#x3D;r_{\pi_{1}}+\gamma P_{\pi_{1}}v_{\pi_{1}}^{(\infty)}\rightarrow v_{\pi_1}(策略迭代)<br>\end{aligned}<br>$$<br>可以看出，截断策略迭代介于价值迭代和策略迭代之间，就是在策略迭代求解贝尔曼方程的时候不会完全收敛，而只是收敛几步，就进行到更新策略的阶段。价值迭代和策略迭代也可以看作是特殊的截断策略迭代，即价值迭代内层迭代次数为$j_\text{truncate}&#x3D;1$，策略迭代内层迭代次数为$j_\text{truncate}&#x3D; \infty$。当然，这一切都建立在$v_{\pi_1}^{(0)}&#x3D;v_0&#x3D;v_{\pi_0}$的基础上，否则这三者没有办法直接比较。</p><h5 id="截断的策略迭代-1"><a href="#截断的策略迭代-1" class="headerlink" title="截断的策略迭代"></a>截断的策略迭代</h5><p>既然截断的策略迭代得到的$v_k$和策略迭代获得的$v_{\pi_k}$不相同，是否还可以通过迭代得到最优策略？答案是肯定的。从直觉上来说，截断的策略迭代介于策略迭代和价值迭代之间，而策略和价值迭代都可以收敛至一个最优策略。同时，它的迭代速度介于那两者之间，慢于策略迭代，快于价值迭代。这在实践中也可以得到：</p><div style="display: flex; justify-content: center;">  <img src="iteration_illustration.png" alt="三个迭代算法的收敛速度"></div><p>我们可以通过命题4.1获得内层迭代中的状态序列的大小关系。</p><p><strong>命题4.1（价值改进）</strong>考虑策略评估中的迭代算法：<br>$$<br>v_{\pi_k}^{(j+1)}&#x3D;r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}^{(j)},\quad j&#x3D;0,1,2,…<br>$$<br>如果初始猜测为$v_{\pi_k}^{(0)}&#x3D;v_{\pi_{k-1}}$，以下成立：<br>$$<br>v_{\pi_k}^{(j+1)}\geq v_{\pi_k}^{(j)}<br>$$<br>（证明后面补充）</p><p>命题4.1建立在$v_{\pi_k}^{(0)}&#x3D;v_{\pi_{k-1}}$的基础上，但是实际上要严格求得$v_{\pi_{k-1}}$是不容易实现的，但是$v_{k-1}$是容易求得的。因此可以体现截断的策略迭代的优势，即求得$v_{k-1}$，同时加速迭代的速度。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（三）：最优状态值和贝尔曼最优方程</title>
    <link href="/2024/01/25/RLMath-Chap3/"/>
    <url>/2024/01/25/RLMath-Chap3/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>自上一部分我们探究了如何求得状态值和状态-动作价值。但是我们的终极任务是为了找到让状态值最大的策略，因此我们需要一个能找到最优策略的方法。</p><h4 id="简单的策略改进"><a href="#简单的策略改进" class="headerlink" title="简单的策略改进"></a>简单的策略改进</h4><p>在此介绍一个例子，如下图的策略，可以看出这个策略不是最好的策略，因为显然在$s_1$执行动作$a_2$会导致智能体进入禁止区域。</p><div style="display: flex; justify-content: center;">  <img src="suboptimal_policy.png" alt="非最优策略" style="zoom: 50%;"></div><p>那么如何改进这个不是最优的策略？我们从直觉上和计算上两个方面进行分析：</p><ul><li><p>直觉上：由于上述环境非常简单，我们可以非常容易地得出，若在$s_1$处向下走，即执行$a_3$动作，就可以避开禁止格子，并快速到达目标位置。</p></li><li><p>计算上：我们通过状态值和状态-动作价值来计算，对应的贝尔曼方程为：</p><p>$$<br>\begin{gathered}<br>v_\pi(s_{1}) &#x3D;-1+\gamma v_{\pi}(s_{2}), \\<br>v_\pi(s_2) &#x3D;+1+\gamma v_\pi(s_4), \\<br>v_\pi(s_3) &#x3D;+1+\gamma v_\pi(s_4), \\<br>v_\pi(s_4) &#x3D;+1+\gamma v_\pi(s_4).<br>\end{gathered}<br>$$<br>我们令折扣因子$\gamma&#x3D;0.9$，可以求得<br>$$<br>\begin{aligned}&amp;v_{\pi}(s_{4})&#x3D;v_\pi(s_3)&#x3D;v_\pi(s_2)&#x3D;10,\\<br>&amp;v_{\pi}(s_{1})&#x3D;8.\end{aligned}<br>$$<br>接着可以求得在状态$s_1$对应的状态-动作价值，如下：<br>$$<br>\begin{aligned}<br>q_\pi(s_1,a_1) &amp;&#x3D;-1+\gamma v_\pi(s_1)&#x3D;6.2, \\<br>q_\pi(s_1,a_2) &amp;&#x3D;-1+\gamma v_\pi(s_2)&#x3D;8, \\<br>q_\pi(s_1,a_3) &amp;&#x3D;0+\gamma v_\pi(s_3)&#x3D;9, \\<br>q_\pi(s_1,a_4) &amp;&#x3D;-1+\gamma v_\pi(s_1)&#x3D;6.2, \\<br>q_{\pi}(s_{1},a_{5}) &amp;&#x3D;0+\gamma v_\pi(s_1)&#x3D;7.2.<br>\end{aligned}<br>$$<br>显然，采用$a_3$获得的状态-动作价值最大：<br>$$<br>q_\pi(s_1,a_3)\geq q_\pi(s_1,a_i),\quad\text{for all }i\neq3.<br>$$<br>因此可以更新策略中的$s_1$对应的动作为$a_3$。</p></li></ul><p>因此可以得到，想要把策略更新为最好的策略，就需要选择每个状态中的最优动作（状态-动作价值最大化）。但是以上策略中，只有$s_1$的动作不是最优的，而要计算最优的策略是需要依赖于其他状态值。若其他状态值也不是最优的，就需要方法来解决。</p><h4 id="最优状态值和最优策略"><a href="#最优状态值和最优策略" class="headerlink" title="最优状态值和最优策略"></a>最优状态值和最优策略</h4><p>首先我们定义最优状态值。若一个策略$\pi_1$优于另一个策略$\pi_2$，则表述为：<br>$$<br>v_{\pi_1}(s)\geq v_{\pi_2}(s),\quad\text{for all }s\in\mathcal{S}<br>$$</p><p>最优状态值的对应策略要优于所有其他策略，即定义为：</p><p><em><em>定义3.1  若对于任意其他策略$\pi$，策略$\pi^</em>$对于任意$s\in\mathcal{S}$，都有$v_{\pi^</em>}(s)\geq v_\pi(s)$，则说明$\pi^*$是最优的，$\pi^*$的状态值$v_{\pi^*}(s)$是最优状态值。**</p><p>沿着这样的定义，我们需要探究最优策略的存在性、唯一性、随机性和求解算法。</p><h4 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h4><p>贝尔曼最优方程的元素表达为：<br>$$<br>\begin{aligned}<br>v(s)&amp; &#x3D;\max_{\pi(s)\in\Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)\left(\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s’\in\mathcal{S}}p(s’|s,a)v(s’)\right) \\<br>&amp;&#x3D;\max_{\pi(s)\in\Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)q(s,a)<br>\end{aligned}<br>$$<br>其中$v(s)$和$v(s’)$为未知量，$\pi(s)$为$s$的对应策略，$\Pi(s)$为$s$对应策略集合，且<br>$$<br>q(s,a)\doteq\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s’\in\mathcal{S}}p(s’|s,a)v(s’)<br>$$<br>观察贝尔曼最优方程，会有一个困惑，即$v(s)$和$\pi(a|s)$都是未知的，如何从一个方程同时求得两个未知量是需要解决的问题。</p><h5 id="贝尔曼方程右式最大化"><a href="#贝尔曼方程右式最大化" class="headerlink" title="贝尔曼方程右式最大化"></a>贝尔曼方程右式最大化</h5><p>为了求解以下贝尔曼方程右式的最大化：<br>$$<br>v(s)&#x3D;\max_{\pi(s)\in\Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)q(s,a),\quad s\in\mathcal{S}.<br>$$<br>右式的含义为有不同个$q(s,a)$，只不过前面都乘了一个小于等于1且大于等于0的系数，且这些系数之和为1。那么显然，只要把这些系数的份额全部都分给最大的$q(s,a)$，那么获得的$v(s)$是最大的。从数学表述上来说，因为$\sum_a\pi(a|s)&#x3D;1$，可以得到<br>$$<br>\sum_{a\in\mathcal{A}}\pi(a|s)q(s,a)\leq\sum_{a\in\mathcal{A}}\pi(a|s)\max_{a\in\mathcal{A}}q(s,a)&#x3D;\max_{a\in\mathcal{A}}q(s,a)<br>$$<br>等号成立条件为<br>$$<br>\pi(a|s)&#x3D;\left<br>\{\begin{array}{ll}1,&amp;a&#x3D;a^*,\\0,&amp;a\neq a^*.<br>\end{array}\right.<br>$$<br>其中，$a^*&#x3D;\arg\max_aq(s,a)$。因此，最佳的策略在一个状态$s$下会选择状态-动作价值$q(s,a)$最大的动作。</p><h5 id="贝尔曼最优公式的矩阵-向量形式"><a href="#贝尔曼最优公式的矩阵-向量形式" class="headerlink" title="贝尔曼最优公式的矩阵-向量形式"></a>贝尔曼最优公式的矩阵-向量形式</h5><p>贝尔曼最优公式的矩阵-向量形式为：<br>$$<br>v&#x3D;\max_{\pi\in\Pi}(r_\pi+\gamma P_\pi v)<br>$$<br>各个部分为：</p><ul><li><p>$v\in\mathbb{R}^{|\mathcal{S}|}$</p></li><li><p>$[r_\pi]_s\doteq\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r$</p><p>即<br>$$<br>r_{\pi}&#x3D;\begin{pmatrix}r_{\pi}(s_{1})\\r_{\pi}(s_{2})\\\vdots\\r_{\pi}(s_{n})\end{pmatrix}&#x3D;\begin{pmatrix}\sum_{a\in\mathcal{A}}\pi(a|s_{1})\sum_{r\in R}p(r|s_{1},a)r\\\sum_{a\in\mathcal{A}}\pi(a|s_{2})\sum_{r\in R}p(r|s_{2},a)r\\\vdots\\\sum_{a\in\mathcal{A}}\pi(a|s_{n})\sum_{r\in R}p(r|s_{n},a)r\end{pmatrix}<br>$$</p></li><li><p>$[P_\pi]_{s,s’}&#x3D;p(s’|s)\doteq\sum_{a\in\mathcal{A}}\pi(a|s)p(s’|s,a)$</p><p>即<br>$$<br>\begin{aligned}<br>P_{\pi}&amp;&#x3D;\begin{pmatrix}p(s_{1}|s_{1})&amp;p(s_{2}|s_{1})&amp;\cdots&amp;p(s_{n}|s_{1})&amp;\\p(s_{1}|s_{2})&amp;p(s_{2}|s_{2})&amp;\cdots&amp;p(s_{n}|s_{2})\\\vdots&amp;\vdots&amp;&amp;\vdots\\p(s_{1}|s_{2})&amp;p(s_{2}|s_{n})&amp;\cdots&amp; p(s_{n}|s_{n})\end{pmatrix}\\<br>&amp;&#x3D;\begin{pmatrix}\sum_{a\in\mathcal{A}}\pi(a|s_1)p(s_1|s_1,a)&amp;\sum_{a\in\mathcal{A}}\pi(a|s_1)p(s_2|s_1,a)&amp;\cdots&amp;\sum_{a\in\mathcal{A}}\pi(a|s_1)p(s_n|s_1,a)&amp;\\\sum_{a\in\mathcal{A}}\pi(a|s_2)p(s_1|s_2,a)&amp;\sum_{a\in\mathcal{A}}\pi(a|s_2)p(s_2|s_2,a)&amp;\cdots&amp;\sum_{a\in\mathcal{A}}\pi(a|s_2)p(s_n|s_2,a)\\\vdots&amp;\vdots&amp;&amp;\vdots\\\sum_{a\in\mathcal{A}}\pi(a|s_n)p(s_1|s_n,a)&amp;\sum_{a\in\mathcal{A}}\pi(a|s_n)p(s_2|s_n,a)&amp;\cdots&amp; \sum_{a\in\mathcal{A}}\pi(a|s_n)p(s_n|s_n,a)\end{pmatrix}<br>\end{aligned}<br>$$</p></li></ul><p>我们令贝尔曼最优方程的右边为$f(v)\doteq\max_{\pi\in\Pi}(r_\pi+\gamma P_\pi v)$，那么贝尔曼方程可以表示为$v&#x3D;f(v)$。接下来探究如何求解这个方程。</p><h5 id="压缩映射定理"><a href="#压缩映射定理" class="headerlink" title="压缩映射定理"></a>压缩映射定理</h5><p>我们使用压缩映射定理来解释贝尔曼最优方程，压缩映射定理又称为不动点定理。压缩映射定理如下：</p><p><strong>定理3.1（压缩映射定理） 对于任何以$x&#x3D;f(x)$为形式且$x$和$f(x)$为是向量的等式，若$f$为压缩映射，那么满足以下性质：</strong></p><ul><li>存在性：存在一个满足$f(x^*)&#x3D;x^*$的不动点$x^*$。</li><li>唯一性：不动点$x^*$是唯一的。</li><li>算法：对迭代过程$x_{k+1}&#x3D;f(x_k)$，$k&#x3D;0,1,2,\dots$，最初猜测$x_0$开始，当$k\to\infty$，$x_k\to x^*$。而且这样的收敛速率是呈指数的。</li></ul><h5 id="贝尔曼最优方程的压缩映射性质"><a href="#贝尔曼最优方程的压缩映射性质" class="headerlink" title="贝尔曼最优方程的压缩映射性质"></a>贝尔曼最优方程的压缩映射性质</h5><p><strong>定理3.2（$f(v)$的压缩性质） 贝尔曼最优方程的右式$f(v)$是一个压缩映射，即对任意$v_1,v_2\in\mathbb{R}^{|\mathcal{S}|}$，有$|f(v_1)-f(v_2)|_\infty\leq\gamma|v_1-v_2|_\infty $。</strong></p><p>证明如下：</p><p>考虑有两个向量$v_1,v_2\in\mathbb{R}^{|\mathcal{S}|}$，并有$\pi_1^*\doteq\arg\max_\pi(r_\pi+\gamma P_\pi v_1)$和$\pi_2^*\doteq\arg\max_\pi(r_\pi+\gamma P_\pi v_2)$，那么就有<br>$$<br>\begin{aligned}<br>f(v_1)&#x3D;\max_\pi(r_\pi+\gamma P_\pi v_1)&#x3D;r_{\pi_1^*}+\gamma P_{\pi_1^*}v_1\geq r_{\pi_2^*}+\gamma P_{\pi_2^*}v_1,\\f(v_2)&#x3D;\max_\pi(r_\pi+\gamma P_\pi v_2)&#x3D;r_{\pi_2^*}+\gamma P_{\pi_2^*}v_2\geq r_{\pi_1^*}+\gamma P_{\pi_1^*}v_2.<br>\end{aligned}<br>$$</p><p>其中的大于等于是以元素为单位的。那么就有<br>$$<br>\begin{aligned}<br>f(v_1)-f(v_2)&amp; \begin{aligned}&#x3D;r_{\pi_1^*}+\gamma P_{\pi_1^*}v_1-(r_{\pi_2^*}+\gamma P_{\pi_2^*}v_2)\end{aligned} \\<br>&amp;\leq r_{\pi_1^*}+\gamma P_{\pi_1^*}v_1-(r_{\pi_1^*}+\gamma P_{\pi_1^*}v_2) \\<br>&amp;&#x3D;\gamma P_{\pi_1^*}(v_1-v_2).<br>\end{aligned}<br>$$<br>同理可以得到$f(v_2)-f(v_1)\leq\gamma P_{\pi_2^*}(v_2-v_1)$，因此有<br>$$<br>\gamma P_{\pi_2^*}(v_1-v_2)\leq f(v_1)-f(v_2)\leq\gamma P_{\pi_1^*}(v_1-v_2)<br>$$<br>我们定义<br>$$<br>z\doteq\max{|\gamma P_{\pi_2^*}(v_1-v_2)|,|\gamma P_{\pi_1^*}(v_1-v_2)|}\in\mathbb{R}^{|S|}<br>$$<br>由于$z\geq0$，显然<br>$$<br>-z\leq\gamma P_{\pi_2^*}(v_1-v_2)\leq f(v_1)-f(v_2)\leq\gamma P_{\pi_1^*}(v_1-v_2)\leq z<br>$$<br>因此<br>$$<br>|f(v_1)-f(v_2)|\leq z<br>$$<br>则<br>$$<br>|f(v_1)-f(v_2)|_\infty\leq|z|_\infty<br>$$<br>从元素的角度来说，有<br>$$<br>z_i&#x3D;\max{\gamma|p_i^T(v_1-v_2)|,\gamma|q_i^T(v_1-v_2)|}<br>$$<br>由于$p_i$的元素都是非负的并且和等于1，则<br>$$<br>|p_i^T(v_1-v_2)|\leq p_i^T|v_1-v_2|\leq|v_1-v_2|_\infty<br>$$<br>同理，有$|q_i^T(v_1-v_2)|\leq|v_1-v_2|_\infty$，因此有$z_i\leq\gamma|v_1-v_2|_\infty $，且<br>$$<br>|z|_\infty&#x3D;\max_i|z_i|\leq\gamma|v_1-v_2|_\infty<br>$$<br>将式子替换，可以得到<br>$$<br>|f(v_1)-f(v_2)|_\infty\leq\gamma|v_1-v_2|_\infty<br>$$<br>因此可以证得$f(v)$的压缩性质。</p><h4 id="通过贝尔曼最优方程求解最优策略"><a href="#通过贝尔曼最优方程求解最优策略" class="headerlink" title="通过贝尔曼最优方程求解最优策略"></a>通过贝尔曼最优方程求解最优策略</h4><p>求解以下贝尔曼最优方程：<br>$$<br>v^*&#x3D;\max_{\pi\in\Pi}(r_\pi+\gamma P_\pi v^*)<br>$$<br>显然可以看出，$v^*$是一个不动点，因为满足$v^* &#x3D; f(v^*)$。因此，可以得出该贝尔曼最优方程满足以下定理：</p><p><em><em>定理3.3 （存在性、唯一性和求解算法）对于贝尔曼最优方程$v &#x3D; f(v) &#x3D;\max_{\pi\in\Pi}(r_\pi+\gamma P_\pi v)$，总是存在一个唯一的解$v^</em>$，可以通过以下迭代过程求解：</em>*<br>$$<br>v_{k+1}&#x3D;f(v_k)&#x3D;\max_{\pi\in\Pi}(r_\pi+\gamma P_\pi v_k),\quad k&#x3D;0,1,2,\ldots<br>$$<br><em><em>$v_k$的值基于初始猜测$v_0$，以指数速度在$k\to\infty $时收敛至$v^</em>$。</em>*</p><p>并且$v^*$满足：</p><p>$v^*$的存在性：贝尔曼最优方程的解总是存在。</p><p>$v^*$的唯一性：贝尔曼最优方程的解总是唯一。</p><p>$v^*$的求解：$v^*$的值可以通过定理3.3的迭代过程求解。（价值迭代过程）</p><p>当$v^*$被求解之后，对应的最优策略$\pi^*$可以通过求解$\pi^*&#x3D;\arg\max_{\pi\in\Pi}(r_\pi+\gamma P_\pi v^*)$，替换进贝尔曼最优方程可以获得<br>$$<br>v^*&#x3D;r_{\pi^*}+\gamma P_{\pi^*}v^*<br>$$<br>因此，$v^*&#x3D;v_{\pi^*}$是最优策略$\pi^*$对应的状态值。因此可以通过贝尔曼最优方程求解$v^*$和$\pi^*$。接下来的定理解释这两者的最优性。</p><p><em><em>定理3.4（$v^</em>$和$\pi^</em>$的最优性）对于任意策略$\pi$，都有$v^*&#x3D;v_{\pi^*}\geq v_\pi$。**</p><p>证明如下：</p><p>因为<br>$$<br>v^*&#x3D;\max_\pi(r_\pi+\gamma P_\pi v^*)&#x3D;r_{\pi^*}+\gamma P_{\pi^*}v^*\geq r_\pi+\gamma P_\pi v^*<br>$$<br>我们可以得到<br>$$<br>v^*-v_\pi\geq(r_\pi+\gamma P_\pi v^*)-(r_\pi+\gamma P_\pi v_\pi)&#x3D;\gamma P_\pi(v^*-v_\pi)<br>$$<br>重复以上过程，可以获得$v^{*}-v_{\pi}\geq\gamma P_{\pi}(v^{*}-v_{\pi})\geq\gamma^{2}P_{\pi}^{2}(v^{*}-v_\pi)\geq\cdots\geq\gamma^nP_\pi^n(v^*-v_\pi)$。因此有<br>$$<br>v^*-v_\pi\geq\lim_{n\to\infty}\gamma^nP_\pi^n(v^*-v_\pi)&#x3D;0<br>$$<br>因此可以证得定理3.4。</p><p>进一步，我们可以得到一个贪婪最优策略的定理：</p><p><strong>定理3.5（贪婪最优策略） 对于任意$s\in\mathcal{S}$，确定的贪婪策略</strong><br>$$<br>\left.\pi^*(a|s)&#x3D;\left\{\begin{array}{ll}1,&amp;a&#x3D;a^*(s),\0,&amp;a\neq a^*(s).\end{array}\right.\right.<br>$$<br><em><em>是贝尔曼最优方程的一个最优策略，其中$a^*(s)&#x3D;\arg\max_aq^</em>(a,s)$，$q^*(s,a)\doteq\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s’\in\mathcal{S}}p(s’|s,a)v^*(s’)$。</em>*</p><p>证明如下：</p><p>从元素角度上来说，可以得到<br>$$<br>\pi^*(s)&#x3D;\arg\max_{\pi\in\Pi}\sum_{a\in\mathcal{A}}\pi(a|s)\underbrace{\left(\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v^*(s^{\prime})\right)}_{q^*(s,a)}<br>$$<br>显然$\sum_{a\in\mathcal{A}}\pi(a|s)q^*(s,a)$在$\pi(s)$选择最大$q^*(s,a)$时可以取得最大值。</p><p>$\pi^*$有两个重要的性质：一是不唯一性，因为两个不同的策略可能会获得相同的$v(s)$，也有可能会取得最大的状态值，说明这个最优策略不是唯一的；而是随机性，则这个最优策略可能是随机或者是确定的，但是根据定理3.5，一定总是存在一个确定的最优策略。</p><h4 id="影响最优策略的因素"><a href="#影响最优策略的因素" class="headerlink" title="影响最优策略的因素"></a>影响最优策略的因素</h4><p>根据贝尔曼最优方程：<br>$$<br>v(s)&#x3D;\max_{\pi(s)\in\Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)\bigg(\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s’\in\mathcal{S}}p(s’|s,a)v(s’)\bigg),\quad s\in\mathcal{S}.<br>$$<br>我们可以得到影响最优状态值和最优策略的决定因素：</p><ul><li>即时奖励$r$</li><li>折扣因子$\gamma$</li><li>环境模型的概率转移$p(s’|s,a)$和$p(r|s,a)$</li></ul><p>（后期将补充压缩映射定理的证明和影响因素的详细分析）</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（二）：状态价值和贝尔曼方程</title>
    <link href="/2024/01/24/RLMath-Chap2/"/>
    <url>/2024/01/24/RLMath-Chap2/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>在上一章介绍了回报的概念，但是回报是以一个回合为单位的，只能衡量一个轨迹、一个回合的回报，而有些策略不是固定的，每个回合和轨迹不一定是一样的，因此我们需要一个值来衡量策略，这就是在这一章介绍的状态价值和状态-动作价值。求解这些价值就是求解贝尔曼方程的过程。</p><h4 id="回报的重要性"><a href="#回报的重要性" class="headerlink" title="回报的重要性"></a>回报的重要性</h4><p>依然参照网格世界环境，接下来列举三种策略：</p><div style="display: flex; justify-content: center;">  <img src="3_policies.png" alt="三种不同策略示例" style="zoom: 50%;"></div><p>从直觉上来说，第一个策略最好，因为它绝对不会进入禁止格子；第二个策略最好，因为它每次都会进入禁止格子。</p><p>从数学计算方面来说，我们可以计算它们的折扣回报。在这里，我们的折扣因子都为$\gamma \in (0,1)$：</p><ul><li><p>第一个策略的轨迹是$s_1\to s_3\to s_4\to s_4\cdots $，对应的折扣回报为<br>$$<br>\begin{aligned}<br>\mathrm{return}_1&amp; &#x3D;0+\gamma1+\gamma^21+\ldots  \\<br>&amp;&#x3D;\gamma(1+\gamma+\gamma^2+\ldots) \\<br>&amp;&#x3D;\frac\gamma{1-\gamma}<br>\end{aligned}<br>$$</p></li><li><p>第二个策略的轨迹是$s_1\to s_2\to s_4\to s_4\cdots $，对应的折扣回报为<br>$$<br>\begin{aligned}<br>\mathrm{return}_{2}&amp; &#x3D;-1+\gamma1+\gamma^21+\ldots  \\<br>&amp;&#x3D;-1+\gamma(1+\gamma+\gamma^2+\ldots) \\<br>&amp;&#x3D;-1+\frac{\gamma}{1-\gamma}<br>\end{aligned}<br>$$</p></li><li><p>第三个策略的轨迹有两种可能（上面两个策略的轨迹），每个轨迹出现的概率都是0.5，因此平均的折扣回报为</p><p>$$<br>\begin{aligned}<br>\mathrm{return}_{3}&amp; &#x3D;0.5\left(-1+\frac{\gamma}{1-\gamma}\right)+0.5\left(\frac{\gamma}{1-\gamma}\right) \\<br>&amp;&#x3D;-0.5+\frac\gamma{1-\gamma}<br>\end{aligned}<br>$$</p></li></ul><p>根据以上计算，我们得到$\mathrm{return}_1&gt;\mathrm{return}_3&gt;\mathrm{return}_2$，和我们直觉上的结论一致。说明回报可以作为比较策略好坏的衡量标准。</p><h4 id="回报的计算"><a href="#回报的计算" class="headerlink" title="回报的计算"></a>回报的计算</h4><p>以下图的策略为例，我们介绍计算回报的方法。</p><div style="display: flex; justify-content: center;">  <img src="return_method.png" alt="策略示例" style="zoom: 50%;"></div><p>计算回报有两种方法：</p><ul><li><p>第一种是依照定义。使用$v_i$来表示每个状态的折扣回报，每个回报可以表示为：</p><p>$$<br>\begin{aligned}<br>v_{1} &#x3D;r_1+\gamma r_2+\gamma^2r_3+\ldots, \\<br>v_{2} &#x3D;r_2+\gamma r_3+\gamma^2r_4+\ldots, \\<br>v_{3} &#x3D;r_3+\gamma r_4+\gamma^2r_1+\ldots, \\<br>v_{4}&#x3D;r_4+\gamma r_1+\gamma^2r_2+\ldots.<br>\end{aligned}<br>$$</p></li><li><p>第二种是通过自举。每个状态的折扣回报可以用下一个状态的折扣回报表示：</p><p>$$<br>\begin{aligned}<br>v_{1}&#x3D;r_1+\gamma(r_2+\gamma r_3+\ldots)&#x3D;r_1+\gamma v_2,\\v_{2}&#x3D;r_2+\gamma(r_3+\gamma r_4+\ldots)&#x3D;r_2+\gamma v_3,\\v_{3}&#x3D;r_3+\gamma(r_4+\gamma r_1+\ldots)&#x3D;r_3+\gamma v_4,\\v_{4}&#x3D;r_4+\gamma(r_1+\gamma r_2+\ldots)&#x3D;r_4+\gamma v_1.<br>\end{aligned}<br>$$</p></li></ul><p>可以看出来，某个状态的折扣回报是依赖于另一个状态的折扣回报的。这体现了自举的思想，即通过自身获得一些值。这样的自举可以用以下矩阵-向量等式表达：<br>$$<br>\underbrace{\left[\begin{array}{c}v_1\\v_2\\v_3\\v_4\end{array}\right]}_v&#x3D;\left[\begin{array}{c}r_1\\r_2\\r_3\\r_4\end{array}\right]+\left[\begin{array}{c}\gamma v_2\\\gamma v_3\\\gamma v_4\\\gamma v_1\end{array}\right]&#x3D;\underbrace{\left[\begin{array}{c}r_1\\r_2\\r_3\\r_4\end{array}\right]}_r+\gamma\underbrace{\left[\begin{array}{ccc}0&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;0\\0&amp;0&amp;0&amp;1\\1&amp;0&amp;0&amp;0\end{array}\right]}_P\underbrace{\left[\begin{array}{c}v_1\\v_2\\v_3\\v_4\end{array}\right]}_v<br>$$<br>使用紧凑一些的表达，该等式可以表达为$v&#x3D;r+\gamma Pv$。因此通过矩阵计算，得到$v$的值可以计算为$v &#x3D; (I - \gamma P)^{-1}r$。在后面的部分会证明$(I - \gamma P)$是可逆的。</p><h4 id="状态价值"><a href="#状态价值" class="headerlink" title="状态价值"></a>状态价值</h4><p>回报虽然可以计算某一个回合的累计奖励，但是对于有随机性的策略，一个回合的累计奖励并不能很好地衡量某个策略的好坏，因此引入状态价值的概念。</p><p>首先，定义状态转移的轨迹为<br>$$<br>S_t\xrightarrow{A_t}S_{t+1},R_{t+1}\xrightarrow{A_{t+1}}S_{t+2},R_{t+2}\xrightarrow{A_{t+2}}S_{t+3},R_{t+3}\ldots.<br>$$<br>在这里，所有的大写字母都是随机变量，状态随机变量属于集合$\mathcal{S}$，动作随机变量属于集合$\mathcal{A}(S_{t})$，奖励随机变量属于集合$\mathcal{R}(S_t,A_t)$。根据定义，这个轨迹的折扣回报为<br>$$<br>G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots<br>$$<br>由于$R_{t+1}$等都是随机变量，$G_t$也是随机变量。我们可以计算$G_t$的期望：<br>$$<br>v_\pi(s)\doteq\mathbb{E}[G_t|S_t&#x3D;s]<br>$$<br>这里$v_\pi(s)$可以被称为状态价值函数或者是$s$的状态值。我们可以得到其有以下的特点：</p><ul><li>$v_\pi(s)$依赖于$s$，标志着从$s$出发的轨迹的平均回报。</li><li>$v_\pi(s)$依赖于$\pi$，因为$s$之后的轨迹需要依赖于该策略$\pi$。</li><li>$v_\pi(s)$不依赖于时间步$t$，无论在哪个时间步到达状态$s$，其状态值都是一样的。</li></ul><p>若策略和环境模型都是确定的，那么一个回合的折扣回报和其期望是一样的；若策略和环境模型是随机的，那么两者很有可能不同。计算折扣回报的期望更有利于计算当前策略下的状态值。</p><h4 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h4><p>贝尔曼方程是分析状态值的工具。</p><p>我们首先从以上对折扣回报的表示出发，该折扣回报可以表示为<br>$$<br>\begin{aligned}<br>G_{t}&amp; &#x3D;R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots  \\<br>&amp;&#x3D;R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots) \\<br>&amp;&#x3D;R_{t+1}+\gamma G_{t+1}<br>\end{aligned}<br>$$<br>这个表达式建立了时间$t$和$t+1$的回报两者之间的关系。由状态值的定义可知，状态值就是折扣回报的期望值，如下表示：<br>$$<br>\begin{aligned}<br>v_{\pi}(s)&amp; &#x3D;\mathbb{E}[G_t|S_t&#x3D;s] \\<br>&amp;&#x3D;\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t&#x3D;s] \\<br>&amp;&#x3D;\mathbb{E}[R_{t+1}|S_t&#x3D;s]+\gamma\mathbb{E}[G_{t+1}|S_t&#x3D;s]<br>\end{aligned}<br>$$<br>这样状态就被分为两个部分：</p><ul><li><p>前半部分为即时奖励的期望，可以被计算为</p><p>$$<br>\begin{aligned}<br>\mathbb{E}[R_{t+1}|S_{t}&#x3D;s]&amp; &#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\mathbb{E}[R_{t+1}|S_t&#x3D;s,A_t&#x3D;a] \\<br>&amp;&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r<br>\end{aligned}<br>$$</p></li><li><p>后半部分为下一个时间步的未来期望，可以被计算为</p><p>$$<br>\begin{aligned}<br>\mathbb{E}[G_{t+1}|S_{t}&#x3D;s]&amp; &#x3D;\sum_{s’\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_t&#x3D;s,S_{t+1}&#x3D;s’]p(s’|s) \\<br>&amp;&#x3D;\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_{t+1}&#x3D;s^{\prime}]p(s^{\prime}|s) \\<br>&amp;&#x3D;\sum_{s^{\prime}\in\mathcal{S}}v_\pi(s^{\prime})p(s^{\prime}|s) \\<br>&amp;&#x3D;\sum_{s^{\prime}\in\mathcal{S}}v_{\pi}(s^{\prime})\sum_{a\in\mathcal{A}}p(s^{\prime}|s,a)\pi(a|s)<br>\end{aligned}<br>$$</p></li></ul><p>以上第二步是由MDP的性质得到的。</p><p>将上面两个等式带入价值函数的原式，可以获得<br>$$<br>\begin{aligned}<br>v_{\pi}(s)&amp; &#x3D;\mathbb{E}[R_{t+1}|S_t&#x3D;s]+\gamma\mathbb{E}[G_{t+1}|S_t&#x3D;s] \\<br>&amp;&#x3D;\underbrace{\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r}_{\text{mean of immediate rewards}}+\underbrace{\gamma\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})}_{\text{mean of future rewards}} \\<br>&amp;&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\left[\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})\right],\quad\text{for all }s\in\mathcal{S}<br>\end{aligned}<br>$$<br>以上就是贝尔曼方程，是用于设计强化学习的最基础的工具。</p><p>根据环境模型，我们可以很容易得到<br>$$<br>\begin{aligned}&amp;p(s’|s,a)&#x3D;\sum_{r\in\mathcal{R}}p(s’,r|s,a)\\&amp;p(r|s,a)&#x3D;\sum_{s’\in\mathcal{S}}p(s’,r|s,a)\end{aligned}<br>$$<br>将上面两式代入原来的贝尔曼方程中，可以将其表示为<br>$$<br>v_\pi(s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s^{\prime},r|s,a)\left[r+\gamma v_\pi(s^{\prime})\right]<br>$$<br>若奖励$r$完全取决于下一个状态$s’$，那么奖励就是$r(s’)$，则$p(r(s’)|s,a)&#x3D;p(s’|s,a)$，那么代入贝尔曼方程，可以得到<br>$$<br>v_\pi(s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)\left[r(s^{\prime})+\gamma v_\pi(s^{\prime})\right]<br>$$</p><h4 id="贝尔曼方程举例"><a href="#贝尔曼方程举例" class="headerlink" title="贝尔曼方程举例"></a>贝尔曼方程举例</h4><p>以如下策略为例：</p><div style="display: flex; justify-content: center;">  <img src="bellman_example.png" alt="贝尔曼方程举例" style="zoom: 50%;"></div><p>代入以上贝尔曼方程，可以获得以下四个等式：<br>$$<br>\begin{aligned}<br>v_\pi(s_1)&#x3D;0+\gamma v_\pi(s_3),\\<br>v_\pi(s_2)&#x3D;1+\gamma v_\pi(s_4),\\<br>v_\pi(s_3)&#x3D;1+\gamma v_\pi(s_4),\\<br>v_\pi(s_4)&#x3D;1+\gamma v_\pi(s_4).<br>\end{aligned}<br>$$<br>求解每个状态的状态值，其实就是求解一个四元一次方程，解得：<br>$$<br>\begin{aligned}<br>v_\pi(s_4) &#x3D;\frac1{1-\gamma}, \\<br>v_\pi(s_3) &#x3D;\frac{1}{1-\gamma}, \\<br>v_\pi(s_2) &#x3D;\frac1{1-\gamma}, \\<br>v_\pi(s_1) &#x3D;\frac{\gamma}{1-\gamma}.<br>\end{aligned}<br>$$<br>如果我们设置折扣因子$\gamma&#x3D;0.9$，代入以上可以得到：<br>$$<br>\begin{aligned}<br>v_\pi(s_4) &#x3D;\frac1{1-0.9}&#x3D;10, \\<br>v_\pi(s_3) &#x3D;\frac1{1-0.9}&#x3D;10, \\<br>v_\pi(s_2) &#x3D;\frac1{1-0.9}&#x3D;10, \\<br>v_{\pi}(s _1)&#x3D;\frac{0.9}{1-0.9}&#x3D;9.<br>\end{aligned}<br>$$<br>以另一个策略为例：</p><div style="display: flex; justify-content: center;">  <img src="bellman_example_2.png" alt="贝尔曼方程举例2" style="zoom: 50%;"></div><p>我们再代入贝尔曼方程，可得：<br>$$<br>\begin{aligned}<br>v_\pi(s_1)&amp;&#x3D;0.5[0+\gamma v_\pi(s_3)]+0.5[-1+\gamma v_\pi(s_2)],\\<br>v_\pi(s_2)&amp;&#x3D;1+\gamma v_\pi(s_4),\\<br>v_\pi(s_3)&amp;&#x3D;1+\gamma v_\pi(s_4),\\<br>v_\pi(s_4)&amp;&#x3D;1+\gamma v_\pi(s_4).<br>\end{aligned}<br>$$<br>这同样是一组四元一次方程，可以解得：<br>$$<br>\begin{aligned}<br>&amp;v_\pi(s_4) &#x3D;\frac1{1-\gamma}, \\<br>&amp;v_\pi(s_3) &#x3D;\frac1{1-\gamma}, \\<br>&amp;v_\pi(s_2) &#x3D;\frac{1}{1-\gamma}, \\<br>&amp;v_\pi(s_1) &#x3D;0.5[0+\gamma v_\pi(s_3)]+0.5[-1+\gamma v_\pi(s_2)]&#x3D;-0.5+\frac\gamma{1-\gamma}.<br>\end{aligned}<br>$$<br>设置折扣因子$\gamma&#x3D;0.9$，代入以上可以得到：<br>$$<br>\begin{aligned}<br>&amp;v_\pi(s_4) &#x3D;10, \\<br>&amp;v_\pi(s_3) &#x3D;10, \\<br>&amp;v_\pi(s_2) &#x3D;10, \\<br>&amp;v_{\pi}(s_{1}) &#x3D;-0.5+9&#x3D;8.5.<br>\end{aligned}<br>$$<br>比较刚刚两个策略，容易得到第一个策略要优于第二个策略：<br>$$<br>v_{\pi_1}(s_i)\geq v_{\pi_2}(s_i),\quad i&#x3D;1,2,3,4.<br>$$<br>从数学计算上来说，结论也是符合我们的直觉的。</p><h4 id="贝尔曼方程的矩阵-向量形式"><a href="#贝尔曼方程的矩阵-向量形式" class="headerlink" title="贝尔曼方程的矩阵-向量形式"></a>贝尔曼方程的矩阵-向量形式</h4><p>以上的贝尔曼方程都是从元素层面的来呈现的。由于这样的方程适用于所有状态，也可以使用矩阵-向量的形式。首先我们将贝尔曼方程重写为：<br>$$<br>v_\pi(s)&#x3D;r_\pi(s)+\gamma\sum_{s’\in\mathcal{S}}p_\pi(s’|s)v_\pi(s’)<br>$$<br>其中：<br>$$<br>\begin{aligned}<br>r_{\pi}(s)&amp; \doteq\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r, \\<br>p_{\pi}(s^{\prime}|s)&amp; \dot{&#x3D;}\sum_{a\in\mathcal{A}}\pi(a|s)p(s’|s,a).<br>\end{aligned}<br>$$<br>对于状态$s_i$，其中$i&#x3D;1,\cdots,n$，$n&#x3D;|\mathcal{S}|$，可以写作<br>$$<br>v_\pi(s_i)&#x3D;r_\pi(s_i)+\gamma\sum_{s_j\in\mathcal{S}}p_\pi(s_j|s_i)v_\pi(s_j)<br>$$<br>令$v_\pi&#x3D;[v_\pi(s_1),\ldots,v_\pi(s_n)]^T\in\mathbb{R}^n$，$r_\pi&#x3D;[r_\pi(s_1),\ldots,r_\pi(s_n)]^T\in\mathbb{R}^n$，且$P_\pi\in\mathbb{R}^{n\times n}$，$[P_\pi]_{ij}&#x3D;p_\pi(s_j|s_i)$，那么上式可以被写作：<br>$$<br>v_\pi&#x3D;r_\pi+\gamma P_\pi v_\pi<br>$$<br>$P_\pi$有一些性质：</p><ul><li>它是一个非负的矩阵，说明它的每一个元素都大于等于零，即$P_\pi\geq0$。</li><li>它是一个随机的矩阵，说明它每行元素之和都等于1，即$P_{\pi}\mathbf{1}&#x3D;\mathbf{1}$，其中$\mathbf{1}&#x3D;[1,\ldots,1]^T$。</li></ul><p>对于刚刚例子中的第二个策略，我们可以得到贝尔曼方程的矩阵-向量形式：<br>$$<br>\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_v&#x3D;\underbrace{\left[\begin{array}{c}r_\pi(s_1)\\r_\pi(s_2)\\r_\pi(s_3)\\r_\pi(s_4)\end{array}\right]}_r+\gamma\underbrace{\left[\begin{array}{ccc}p_\pi(s_1|s_1)&amp;p_\pi(s_2|s_1)&amp;p_\pi(s_3|s_1)&amp;p_\pi(s_4|s_1)\\p_\pi(s_1|s_2)&amp;p_\pi(s_2|s_2)&amp;p_\pi(s_3|s_2)&amp;p_\pi(s_4|s_2)\\p_\pi(s_1|s_3)&amp;p_\pi(s_2|s_3)&amp;p_\pi(s_3|s_3)&amp;p_\pi(s_4|s_3)\\p_\pi(s_1|s_4)&amp;p_\pi(s_2|s_4)&amp;p_\pi(s_3|s_4)&amp;p_\pi(s_4|s_4)\end{array}\right]}_P\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_v<br>$$</p><p>用具体的值来代替可得到<br>$$<br>\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_v&#x3D;\underbrace{\left[\begin{array}{c}0.5(0)+0.5(-1)\\1\\1\\1\end{array}\right]}_r+\gamma\underbrace{\left[\begin{array}{ccc}0&amp;0.5&amp;0.5&amp;0\\0&amp;0&amp;0&amp;1\\0&amp;0&amp;0&amp;1\\0&amp;0&amp;0&amp;1\end{array}\right]}_P\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_v<br>$$</p><p>可以看出$P_\pi$满足以上的两个性质。</p><h4 id="贝尔曼方程解状态值"><a href="#贝尔曼方程解状态值" class="headerlink" title="贝尔曼方程解状态值"></a>贝尔曼方程解状态值</h4><p>有了贝尔曼方程，我们需要求解这个贝尔曼方程，这个求解的过程就是策略评估。通过以上的矩阵-向量形式，我们可以求得封闭形式的解。</p><h5 id="封闭形式求解"><a href="#封闭形式求解" class="headerlink" title="封闭形式求解"></a>封闭形式求解</h5><p>根据贝尔曼方程，我们可以获得：<br>$$<br>v_\pi&#x3D;(I-\gamma P_\pi)^{-1}r_\pi<br>$$<br>其中 $(I-\gamma P_\pi)^{-1}$满足以下性质：</p><ul><li>$I-\gamma P_\pi$是可逆的。证明如下：</li></ul><p>$$<br>根据盖尔圆定理，第i个盖尔圆的圆心O和半径r满足<br>$$</p><p>$$<br>O&#x3D;[I-\gamma P_\pi]_{ii}&#x3D;1-\gamma p_\pi(s_i|s_i)<br>$$</p><p>$$<br>r&#x3D;\sum_{j\neq i}[I-\gamma P_{\pi}]_{ij}&#x3D;\sum_{j\neq i}\gamma p_{\pi}(s_{j}|s_{i})<br>$$</p><p>$$<br>由于\gamma&lt;1，因此有\sum_{j\neq i}\gamma p_\pi(s_j|s_i)&lt;1-\gamma p_\pi(s_i|s_i)<br>$$</p><p>$$<br>说明这些盖尔圆都不包括原点，证得I-\gamma P_\pi为可逆的<br>$$</p><ul><li>$(I-\gamma P_\pi)^{-1} \geq I$，即大于单位矩阵。根据矩阵级数展开，有$(I-\gamma P_{\pi})^{-1}&#x3D;I+\gamma P_{\pi}+\gamma^{2}P_{\pi}^{2}+\cdots\geq I\geq0$</li><li>对于任意$r\geq0$，满足$(I-\gamma P_{\pi})^{-1}r\geq r\geq0$。由上一个性质可得$[I-\gamma P_{\pi})^{-1}-I]r\geq0$，进而可以进一步求得对于任意$r_{1}\geq r_{2}$，有$(I-\gamma P_{\pi})^{-1}r_{1}\geq(I-\gamma P_{\pi})^{-1}r_{2}$。</li></ul><h5 id="迭代求解"><a href="#迭代求解" class="headerlink" title="迭代求解"></a>迭代求解</h5><p>封闭形式求解虽然提供了很强的理论支持，但是由于涉及到矩阵求逆，计算的代价非常高。因此提出另外一个更为简单的方法，即迭代求解。可以直接通过迭代以下贝尔曼方程：<br>$$<br>v_{k+1}&#x3D;r_\pi+\gamma P_\pi v_k,\quad k&#x3D;0,1,2,\ldots<br>$$<br>当迭代次数足够多，到$k$次迭代，就可以满足：<br>$$<br>\quad v_{k}\to v_{\pi}&#x3D;(I-\gamma P_{\pi})^{-1}r_{\pi},\quad\mathrm{as~}k\to\infty<br>$$<br>具体证明如下：<br>$$<br>定义误差\delta_k&#x3D;v_k-v_n，只要证得\delta\to0,就证明完毕了.<br>$$</p><p>$$<br>将v_{k}&#x3D;\delta_{k}+v_{\pi}和v_{k+1}&#x3D;\delta_{k+1}+v_{\pi}代入v_{k+1}&#x3D;r_{\pi}+\gamma P_{\pi}v_{k}可得<br>$$</p><p>$$<br>\delta_{k+1}+v_\pi&#x3D;r_\pi+\gamma P_\pi(\delta_k+v_\pi)<br>$$</p><p>$$<br>\begin{aligned}<br>\delta_{k+1}&amp; &#x3D;-v_\pi+r_\pi+\gamma P_\pi\delta_k+\gamma P_\pi v_\pi, \\<br>&amp;&#x3D;\gamma P_\pi\delta_k-v_\pi+(r_\pi+\gamma P_\pi v_\pi), \\<br>&amp;&#x3D;\gamma P_\pi\delta_k.<br>\end{aligned}<br>$$</p><p>$$<br>因此\delta_{k+1}&#x3D;\gamma P_\pi\delta_k&#x3D;\gamma^2P_\pi^2\delta_{k-1}&#x3D;\cdots&#x3D;\gamma^{k+1}P_\pi^{k+1}\delta_0.<br>$$</p><p>$$<br>由于0\leq P_\pi\leq 1，因此0\leq P_\pi^k\leq 1.又因为\gamma&lt;1，因此\gamma^k\to0.<br>$$</p><p>$$<br>因此\delta_{k+1}&#x3D;\gamma^{k+1}P_\pi^{k+1}\delta_0\to0，k\to\infty.<br>$$</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>比较以下两个策略，前者是较好的策略，后者是较差的策略。</p><div style="display: flex; justify-content: center;">  <img src="better_policy.png" alt="较好策略"></div><div style="display: flex; justify-content: center;">  <img src="worse_policy.png" alt="较差策略"></div><p>可以看出，较好策略对应的每个状态值都要大于较差策略的状态值，说明状态值可以衡量一个策略的好坏。</p><h4 id="状态-动作价值"><a href="#状态-动作价值" class="headerlink" title="状态-动作价值"></a>状态-动作价值</h4><p>在状态值的基础上更进一步，接下来探究在某个状态下做某个动作以后的价值函数。这个状态-动作价值被定义为：<br>$$<br>q_\pi(s,a)\doteq\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]<br>$$<br>由于基于状态策略有一定的动作分布，我们可以建立起状态值和状态-动作价值两者之间的关系：<br>$$<br>\begin{aligned}<br>\underbrace{\mathbb{E}[G_t|S_t&#x3D;s]}_{v_\pi(s)}&amp;&#x3D;\sum_{a\in\mathcal{A}}\underbrace{\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]}_{q_\pi(s,a)}\pi(a|s),\\<br>v_\pi(s)&amp;&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a).<br>\end{aligned}<br>$$<br>由于状态值可以表示为：<br>$$<br>v_\pi(s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\Big[\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s’\in\mathcal{S}}p(s’|s,a)v_\pi(s’)\Big]<br>$$<br>可以得到状态-动作价值的表达式：<br>$$<br>q_{\pi}(s,a)&#x3D;\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})<br>$$<br>这样，可以看出状态-动作价值分为两个部分：</p><ul><li>第一部分是即时奖励的期望。</li><li>第二部分是未来奖励的期望。</li></ul><h5 id="状态-动作价值示例"><a href="#状态-动作价值示例" class="headerlink" title="状态-动作价值示例"></a>状态-动作价值示例</h5><p>以如下策略为示例：</p><div style="display: flex; justify-content: center;">  <img src="q_value_example.png" alt="状态-动作价值示例" style="zoom: 50%;"></div><p>以$s_1$为例，代入以上求得的状态-动作价值表达式，可以得到：<br>$$<br>\begin{aligned}<br>&amp;q_\pi(s_1,a_1)&#x3D;-1+\gamma v_\pi(s_1), \\<br>&amp;q_\pi(s_1,a_2)&#x3D;-1+\gamma v_\pi(s_2), \\<br>&amp;q_\pi(s_1,a_3)&#x3D;0+\gamma v_\pi(s_3), \\<br>&amp;q_\pi(s_1,a_4)&#x3D;-1+\gamma v_\pi(s_1),\\<br>&amp;q_\pi(s_1,a_5)&#x3D;0+\gamma v_\pi(s_1).<br>\end{aligned}<br>$$<br>即使我们的策略并不包含所有的动作，我们也会求得没有被涉及到的动作的价值。原因在于当前策略选定的动作并不是最佳的。因此我们需要探索所有的动作。</p><p>根据之前我们求得的状态值和状态-动作价值之间的关系，我们也可以得到状态值：<br>$$<br>\begin{aligned}v_{\pi}(s_{1})&amp;&#x3D;0.5q_\pi(s_1,a_2)+0.5q_\pi(s_1,a_3),\\&amp;&#x3D;0.5[0+\gamma v_\pi(s_3)]+0.5[-1+\gamma v_\pi(s_2)].\end{aligned}<br>$$</p><h5 id="状态-动作价值的贝尔曼方程"><a href="#状态-动作价值的贝尔曼方程" class="headerlink" title="状态-动作价值的贝尔曼方程"></a>状态-动作价值的贝尔曼方程</h5><p>将状态-动作价值函数的表达式代入状态值和状态-动作价值关系式中，可以得到状态-动作价值函数的迭代表达式：<br>$$<br>q_\pi(s,a)&#x3D;\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s’\in\mathcal{S}}p(s’|s,a)\sum_{a’\in\mathcal{A}(s’)}\pi(a’|s’)q_\pi(s’,a’)<br>$$<br>其矩阵-向量形式为：<br>$$<br>q_\pi&#x3D;\tilde{r}+\gamma P\Pi q_\pi<br>$$<br>以上的符号都以$(s,a)$为索引。令$n&#x3D;|\mathcal{S}|$且$m&#x3D;|\mathcal{A}|$，具体如下：</p><ul><li><p>$[q_\pi]_{(s,a)} &#x3D; q_\pi(s,a)$</p><p>即</p><p>$$<br>q_\pi&#x3D;\begin{pmatrix}q_\pi(s_{1},a_{1})\\q_\pi(s_{1},a_{2})\\\vdots\\q_\pi(s_{n},a_{m})\end{pmatrix}<br>$$</p></li><li><p>$[\tilde{r}]<em>{(s,a)}&#x3D;\sum</em>{r\in\mathcal{R}}p(r|s,a)r$</p><p>即</p><p>$$<br>\tilde{r}&#x3D;\begin{pmatrix}\sum_{r\in R}p(r|s_{1},a_{1})r\\\sum_{r\in R}p(r|s_{1},a_{2})r\\\vdots\\\sum_{r\in R}p(r|s_{n},a_{m})r\end{pmatrix}<br>$$</p></li><li><p>$[P]_{(s,a),s’}&#x3D;p(s’|s,a)$</p><p>即</p><p>$$<br>P&#x3D;\begin{pmatrix}p(s_1|s_1,a_1)&amp;p(s_2|s_1,a_1)&amp;\cdots&amp;p(s_1|s_1,a_1)\\p(s_1|s_1,a_2)&amp;p(s_2|s_1,a_2)&amp;\cdots&amp;p(s_n|s_1,a_2)\\\vdots&amp;\vdots&amp;&amp;\vdots\\p(s_1|s_n,a_m)&amp;p(s_2|s_n,a_m)&amp;\cdots&amp;p(s_n|s_n,a_m)\end{pmatrix}<br>$$</p></li><li><p>$\Pi_{s’,(s’,a’)}&#x3D;\pi(a’|s’)$<br>即</p><p>$$<br>\Pi&#x3D;<br>\begin{pmatrix}<br>\pi(a_1|s_1)&amp;\pi(a_2|s_1)&amp;\cdots&amp;\pi(a_m|s_n)&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;0&amp;\cdots&amp;0\\<br>0&amp;0&amp;\cdots&amp;0&amp;\pi(a_1|s_2)&amp;\pi(a_2|s_2)&amp;\cdots&amp;\pi(a_m|s_2)&amp;0&amp;0&amp;\cdots&amp;0\\<br>\vdots&amp;\vdots&amp;&amp;\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\<br>0&amp;0&amp;\cdots&amp;0&amp;0&amp;0&amp;\cdots&amp;0&amp;\pi(a_1|s_n)&amp;\pi(a_2|s_n)&amp;\cdots&amp;\pi(a_n,s_n)<br>\end{pmatrix}<br>$$</p></li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>状态值和状态-动作价值函数都是衡量当前策略好坏的衡量工具，是解决强化学习问题的基础，非常重要！</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（一）：基础概念</title>
    <link href="/2024/01/23/RLMath-Chap1/"/>
    <url>/2024/01/23/RLMath-Chap1/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>通过一个简单的网格世界的环境，可以了解到强化学习的一些重要元素。这个网格世界中有若干格子，每个格子分别代表普通格子、目标格子和禁止格子，一个机器人（我们称之为智能体）从某个格子出发，目的是避开禁止格子，并到达目标格子。我们需要让智能体做出满足这个条件的最佳策略。这个网格世界如下图所示。</p><div style="display: flex; justify-content: center;">  <img src="grid_world.png" alt="网格世界示例" style="zoom: 50%;"></div><h4 id="状态-动作"><a href="#状态-动作" class="headerlink" title="状态&amp;动作"></a>状态&amp;动作</h4><p>根据上述网格世界，这个环境的状态可以表示智能体在其中的位置，即在9个不同格子上的状态，组成一个集合$\mathcal{S}&#x3D;\{s_1,\ldots,s_9\}$​，如下图。</p><div style="display: flex; justify-content: center;">  <img src="grid_word_state.png" alt="网格世界的状态" style="zoom:50%;" /></div>对于每个状态，智能体可以做出不同动作。在网格世界中，智能体可以做出上下左右和不动五个动作，组成一个集合$\mathcal{A}=\\{a_1,\ldots,a_5\\}$。针对不同状态，可能存在不同的可执行动作，比如在$s_1$状态，智能体处于网格世界边缘，只能进行下、右、原地三个动作。为了方便，我们对任意状态$s_i$的动作，都设为$\mathcal{A}(s_i)=\mathcal{A}=\\{a_1,\ldots,a_5\\}$​​，如下图。<div style="display: flex; justify-content: center;">  <img src="grid_world_action.png" alt="网格世界的动作" style="zoom: 50%;" /></div><h4 id="状态转移"><a href="#状态转移" class="headerlink" title="状态转移"></a>状态转移</h4><p>智能体每做出一个动作，当前状态会转移到另外一个状态。比如智能体本身处于$s_1$，若采取向右走的动作，所处状态就会转移到$s_2$，即$s_1\xrightarrow{a_2}s_2$。在此网格世界中，我们规定：</p><ul><li>若动作触及边界，智能体会保持在原地，即状态不变，如$s_1\xrightarrow{a_1}s_1$，可以理解为智能体撞到了墙然后弹了回来。</li><li>对于禁止格子，有两个处理方式：一是把禁止格子当作墙壁，即触到就反弹；二是使禁止格子可以接触，只是踏入该区域会使智能体被惩罚。</li></ul><p>在此，我们采用第二种，即踏入禁止格子就被惩罚的处理方式。</p><p>基于以上规定，我们可以制作出表示该网格世界的状态转移的表格：<br>$$<br>\begin{array}{|c|c|c|c|c|c|c|}\hline&amp;a_1\left(\text{upward}\right)&amp;a_2\left(\text{rightward}\right)&amp;a_3\left(\text{downward}\right)&amp;a_4\left(\text{leftward}\right)&amp;a_5\left(\text{unchanged}\right)\\\hline s_1&amp;s_1&amp;s_2&amp;s_4&amp;s_1&amp;s_1\\\hline s_2&amp;s_2&amp;s_3&amp;s_5&amp;s_1&amp;s_2\\\hline s_3&amp;s_3&amp;s_3&amp;s_6&amp;s_2&amp;s_3\\\hline s_4&amp;s_1&amp;s_5&amp;s_7&amp;s_4&amp;s_4\\\hline s_5&amp;s_2&amp;s_6&amp;s_8&amp;s_4&amp;s_5\\\hline s_6&amp;s_3&amp;s_6&amp;s_9&amp;s_5&amp;s_6\\\hline s_7&amp;s_4&amp;s_8&amp;s_7&amp;s_7&amp;s_7\\\hline s_8&amp;s_5&amp;s_9&amp;s_8&amp;s_7&amp;s_8\\\hline s_9&amp;s_6&amp;s_9&amp;s_9&amp;s_8&amp;s_9\\\hline\end{array}<br>$$<br>我们用表格表示出了这种确定的状态转移，因为在这个网格世界中，智能体做出一个动作，接下来的状态是确定的，以$(s_1,a_2)$为例：<br>$$<br>\begin{aligned}<br>&amp;p(s_{1}|s_{1},a_{2}) &#x3D;0,  \\<br>&amp;p(s_2|s_1,a_2) &#x3D;1,  \\<br>&amp;p(s_3|s_1,a_2) &#x3D;0,  \\<br>&amp;p(s_{4}|s_{1},a_{2}) &#x3D;0,  \\<br>&amp;p(s_{5}|s_{1},a_{2}) &#x3D;0,  \\<br>&amp;p(s_{6}|s_{1},a_{2}) &#x3D;0,  \\<br>&amp;p(s_7|s_1,a_2) &#x3D;0,  \\<br>&amp;p(s_8|s_1,a_2) &#x3D;0,  \\<br>&amp;p(s_9|s_{1},a_{2}) &#x3D;0.  \\<br>\end{aligned}<br>$$<br>我们可以用独热编码来表示这样的状态转移：$[0,1,0,0,0,0,0,0,0]$。在此，状态转移为$s_2$的概率为1，转移为其他状态的概率均为0。使用这样的思路，我们可以使用独热编码把以上表格改写为以下形式：<br>$$<br>\begin{array}<br>{|c|c|c|c|c|c|c|}\hline&amp;a_1\left(\text{upward}\right)&amp;a_2\left(\text{rightward}\right)&amp;a_3\left(\text{downward}\right)&amp;a_4\left(\text{leftward}\right)&amp;a_5\left(\text{unchanged}\right)\\\hline s_1&amp;[1,0,0,0,0,0,0,0,0]&amp;[0,1,0,0,0,0,0,0,0]&amp;[0,0,0,1,0,0,0,0,0]&amp;[1,0,0,0,0,0,0,0,0]&amp;[1,0,0,0,0,0,0,0,0]\\\hline s_2&amp;[0,1,0,0,0,0,0,0,0]&amp;[0,0,1,0,0,0,0,0,0]&amp;[0,0,0,0,1,0,0,0,0]&amp;[1,0,0,0,0,0,0,0,0]&amp;[0,1,0,0,0,0,0,0,0]\\\hline s_3&amp;[0,0,1,0,0,0,0,0,0]&amp;[0,0,1,0,0,0,0,0,0]&amp;[0,0,0,0,0,1,0,0,0]&amp;[0,1,0,0,0,0,0,0,0]&amp;[0,0,1,0,0,0,0,0,0]\\\hline s_4&amp;[1,0,0,0,0,0,0,0,0]&amp;[0,0,0,0,1,0,0,0,0]&amp;[0,0,0,0,0,0,1,0,0]&amp;[0,0,0,1,0,0,0,0,0]&amp;[0,0,0,1,0,0,0,0,0]\\\hline s_5&amp;[0,1,0,0,0,0,0,0,0]&amp;[0,0,0,0,0,1,0,0,0]&amp;[0,0,0,0,0,0,0,1,0]&amp;[0,0,0,1,0,0,0,0,0]&amp;[0,0,0,0,1,0,0,0,0]\\\hline s_6&amp;[0,0,1,0,0,0,0,0,0]&amp;[0,0,0,0,0,1,0,0,0]&amp;[0,0,0,0,0,0,0,0,1]&amp;[0,0,0,0,1,0,0,0,0]&amp;[0,0,0,0,0,1,0,0,0]\\\hline s_7&amp;[0,0,0,1,0,0,0,0,0]&amp;[0,0,0,0,0,0,0,1,0]&amp;[0,0,0,0,0,0,1,0,0]&amp;[0,0,0,0,0,0,1,0,0]&amp;[0,0,0,0,0,0,1,0,0]\\\hline s_8&amp;[0,0,0,0,1,0,0,0,0]&amp;[0,0,0,0,0,0,0,0,1]&amp;[0,0,0,0,0,0,0,1,0]&amp;[0,0,0,0,0,0,1,0,0]&amp;[0,0,0,0,0,0,0,1,0]\\\hline s_9&amp;[0,0,0,0,0,1,0,0,0]&amp;[0,0,0,0,0,0,0,0,1]&amp;[0,0,0,0,0,0,0,0,1]&amp;[0,0,0,0,0,0,0,1,0]&amp;[0,0,0,0,0,0,0,0,1]\\\hline\end{array}<br>$$<br>但事实上状态转移有可能存在随机性，在一些环境中，对于固定的状态-动作对，也有可能转移到不同的状态，即有些状态转移概率（独热编码中的元素）不是0或1。在这个网格世界中，我们只考虑确定的状态转移这个最简单的情况。</p><h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>策略决定智能体在不同状态下采取什么样的动作。在网格世界中，就表示为箭头，每个格子上的箭头就表示在当前格子（状态）中智能体策略决定的动作。一个确定性的策略如下所示：</p><div style="display: flex; justify-content: center;">  <img src="deterministic_policy.png" alt="确定性策略" style="zoom: 50%;"></div><p>根据这个策略，从不同的格子（状态）出发，可以得到不同的轨迹，如下所示：</p><div style="display: flex; justify-content: center;">  <img src="trajectory.png" alt="轨迹" style="zoom: 50%;"></div><p>每个状态对应的策略实际上是一个概率分布，即在该状态下采取每一个动作的概率是多少。由于以上策略是确定的，因此每个状态就只有一个对应动作的概率为1。以$s_1$为例，其策略概率分布为：<br>$$<br>\begin{gathered}<br>\pi(a_1|s_1) &#x3D;0, \<br>\pi(a_2|s_1) &#x3D;1, \<br>\pi(a_3|s_1) &#x3D;0, \<br>\pi(a_4|s_1) &#x3D;0, \<br>\pi(a_5|s_1) &#x3D;0.<br>\end{gathered}<br>$$<br>除了确定性的策略以外，还有随机的策略，则处于一个状态时，策略决定的动作不是固定的，而是有一定的概率，例如下图的策略：</p><div style="display: flex; justify-content: center;">  <img src="stochastic_policy.png" alt="随机性策略" style="zoom: 50%;"></div><p>由上图可知，对于这个随机性策略，当智能体处于$s_1$时，其有0.5的概率向右走，有0.5的概率向下走，即：<br>$$<br>\begin{aligned}<br>&amp;\pi(a_{1}|s_{1}) &#x3D;0, \<br>&amp;\pi(a_2|s_1) &#x3D;0.5, \<br>&amp;\pi(a_3|s_1) &#x3D;0.5, \<br>&amp;\pi(a_4|s_1) &#x3D;0, \<br>&amp;\pi(a_5|s_1) &#x3D;0.<br>\end{aligned}<br>$$<br>以上的策略也可以通过表格来展示，被称之为表格表示的策略，如下所示：<br>$$<br>\begin{array}{|c|c|c|c|c|c|c|}\hline&amp;a_1\left(\text{upward}\right)&amp;a_2\left(\text{rightward}\right)&amp;a_3\left(\text{downward}\right)&amp;a_4\left(\text{leftward}\right)&amp;a_5\left(\text{unchanged}\right)\\\hline s_1&amp;0&amp;0.5&amp;0.5&amp;0&amp;0\\\hline s_2&amp;0&amp;0&amp;1&amp;0&amp;0\\\hline s_3&amp;0&amp;0&amp;0&amp;1&amp;0\\\hline s_4&amp;0&amp;1&amp;0&amp;0&amp;0\\\hline s_5&amp;0&amp;0&amp;1&amp;0&amp;0\\\hline s_6&amp;0&amp;0&amp;1&amp;0&amp;0\\\hline s_7&amp;0&amp;1&amp;0&amp;0&amp;0\\\hline s_8&amp;0&amp;1&amp;0&amp;0&amp;0\\\hline s_9&amp;0&amp;0&amp;0&amp;0&amp;1\\\hline\end{array}<br>$$</p><h4 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h4><p>在强化学习中，我们需要智能体采取最佳的策略，那么这个 “最佳”需要一个标准来衡量。奖励函数是强化学习中衡量策略决定的动作好坏的机制。奖励函数$r(s,a)$是由状态$s$和动作$a$决定的，其意义是在当前状态做出某一动作后多智能体获得的奖励。这个奖励是一个数值，可正可负。普遍上来说，给一个$(s,a)$正奖励说明我们鼓励在$s$中做出$a$动作，负奖励说明我们惩罚这个动作，让智能体尽可能避免做$a$动作。在网格世界当中，我们规定如下的奖励函数：</p><ul><li>如果智能体试图超出边界，则获得奖励$r_\text{boundary}&#x3D;-1$。</li><li>如果智能体试图进入禁止格子，则获得奖励$r_\text{forbbiden}&#x3D;-1$。</li><li>如果智能体达到目标格子，则获得奖励$r_\text{target}&#x3D;1$。</li><li>其他情况，智能体获得奖励$r_\text{other}&#x3D;0$。</li></ul><p>奖励函数是根据环境和任务的具体情况人为设计的，而且非常重要，关系到最终策略的好坏。而且对于复杂的环境，奖励函数的设计也可能会非常复杂。当然，可能依然会比其他需要深入了解环境知识的方法要容易，不然我们也不会使用强化学习了。</p><p>我们针对网格世界的以上奖励规则是确定性的，即每个$(s,a)$获得的奖励是固定的。以$(s_1,a_1)$为例，使用概率分布来表达这个奖励函数如下：<br>$$<br>p(r&#x3D;-1|s_1,a_1)&#x3D;1,\quad p(r\neq-1|s_1,a_1)&#x3D;0.<br>$$<br>当然，这个奖励函数也有可能具有随机性，因为环境本身就是可能有随机性的。举一个例子，我在考试前认真复习，可以保证我及格，但是考试成绩可能是80分也有可能是90分，奖励函数就是这个道理。</p><p>以上网格世界的奖励函数可以用表格的格式来表达：<br>$$<br>\begin{array}{|c|c|c|c|c|c|c|}\hline&amp;a_1\text{ (upward)}&amp;a_2\text{ (rightward)}&amp;a_3\text{ (downward)}&amp;a_4\text{ (leftward)}&amp;a_5\text{ (unchanged)}\\\hline s_1&amp;r_\text{boundary}&amp;0&amp;0&amp;r_\text{boundary}&amp;0\\\hline s_2&amp;r_\text{boundary}&amp;0&amp;0&amp;0&amp;0\\\hline s_3&amp;r_\text{boundary}&amp;r_\text{boundary}&amp;r_\text{forbidden}&amp;0&amp;0\\\hline s_4&amp;0&amp;0&amp;r_\text{forbidden}&amp;r_\text{boundary}&amp;0\\\hline s_5&amp;0&amp;r_\text{forbidden}&amp;0&amp;0&amp;0\\\hline s_6&amp;0&amp;r_\text{boundary}&amp;r_\text{barget}&amp;0&amp;r_\text{forbidden}\\\hline s_7&amp;0&amp;0&amp;r_\text{boundary}&amp;r_\text{boundary}&amp;r_\text{forbidden}\\\hline s_8&amp;0&amp;r_\text{target}&amp;r_\text{boundary}&amp;r_\text{forbidden}&amp;0\\\hline s_9&amp;r_\text{forbidden}&amp;r_\text{boundary}&amp;r_\text{boundary}&amp;0&amp;r_\text{target}\\\hline\end{array}<br>$$<br>其实，奖励$r$也会依赖于$(s,a)$转移到的下一个状态$s’$，即对应概率为$p(r|s,a,s’)$。但是由于$s’$也是取决于$(s,a)$的，因此也可以表示为$p(r|s,a)&#x3D;\sum_{s’}p(r|s,a,s’)p(s’|s,a)$</p><p>虽然我们说正奖励表示鼓励，负奖励表示惩罚，但是事实上，如果将正奖励和负奖励统一加上或减去一个值都不会影响到这一特点。</p><h4 id="轨迹-回报-回合"><a href="#轨迹-回报-回合" class="headerlink" title="轨迹&amp;回报&amp;回合"></a>轨迹&amp;回报&amp;回合</h4><p>轨迹是智能体依据现有策略，在进行一系列的状态转移、动作选择、奖励获得后得到的状态-动作-奖励链。以下是网格世界中不同的两个策略产生的轨迹：</p><div style="display: flex; justify-content: center;">  <img src="two_trajectories.png" alt="不同策略产生的轨迹" style="zoom: 50%;"></div><p>对于上图左边的策略，智能体从$s_1$出发，获得的轨迹是$s_{1}\xrightarrow[r&#x3D;0]{a_{2}}s_{2}\xrightarrow[r&#x3D;0]{a_{3}}s_{5}\xrightarrow[r&#x3D;0]{a_{3}}s_{8}\xrightarrow[r&#x3D;1]{a_{2}}s_{9}$。</p><p>对于上图右边的策略，智能体从$s_1$出发，获得的轨迹是$s_{1}\xrightarrow[r&#x3D;0]{a_{3}}s_{4}\xrightarrow[r&#x3D;-1]{a_{3}}s_{7}\xrightarrow[r&#x3D;0]{a_{2}}s_{8}\xrightarrow[r&#x3D;1]{a_{2}}s_{9}$</p><p>如果说奖励是衡量动作的好坏，那么回报就是用来衡量策略的好坏的。在这个网格世界当中，回报就是在一个轨迹中，获得的奖励之和，又称为总奖励或者累计奖励。比如图中的左边的策略，回报就为$\text{return}&#x3D;0+0+0+1&#x3D;1$。对于图中的右边的策略，回报是$\mathrm{return}&#x3D;0-1+0+1&#x3D;0$。</p><p>由上可知，左边策略的回报大于右边策略的回报，因此左边策略优于右边策略。从直观上来看也是如此，因为右边策略踏入了禁止格子。</p><p>回报除了考虑即时奖励，也考虑了未来奖励，因此相比直接选择奖励最大的动作，使用回报会更加“远视”一些。</p><p>之前我们列举的轨迹都是有限的，事实上轨迹也可以是无限的。例如上图左边策略，智能体到达$s_9$后轨迹并没有结束，而是一直做保持在原地的动作（$a_5$）。获得的轨迹是$s_{1}\xrightarrow[r&#x3D;0]{a_{2}}s_{2}\xrightarrow[r&#x3D;0]{a_{3}}s_{5}\xrightarrow[r&#x3D;0]{a_{3}}s_{8}\xrightarrow[r&#x3D;1]{a_{2}}s_{9}\xrightarrow[r&#x3D;1]{a_{5}}s_{9}\xrightarrow[r&#x3D;1]{a_{5}}s_{9}\xrightarrow[r&#x3D;1]{a_{5}}s_{9}\ldots $，这样对应的回报就是$\mathrm{return}&#x3D;0+0+0+1+1+1+\cdots&#x3D;\infty$。但是这样的话，回报就不收敛了。因此计算回报的时候引入折扣因子$\gamma\in(0,1)$，对未来的奖励加入一定的折扣效果。因此我们获得折扣回报：<br>$$<br>\text{discounted return}&#x3D;0+\gamma0+\gamma^{2}0+\gamma^{3}1+\gamma^{4}1+\gamma^{5}1+\ldots&#x3D;\gamma^3(1+\gamma+\gamma^2+\ldots)&#x3D;\gamma^3\frac1{1-\gamma}<br>$$<br>折扣因子的作用有如下两个：</p><ul><li>对于无限的轨迹，折扣因子可以提供一个终止条件。</li><li>在有些问题当中，当前的奖励和未来的奖励重要性不一样，折扣因子可以提供一个重要性的区分。$\gamma$​越大，说明未来的奖励越重要，策略越远视，反之亦然。</li></ul><p>当智能体与环境进行交互的时候，智能体可能停在一个终止状态，从初始状态到终止状态这一过程，可以被称为回合。有些任务是有终止状态的，这些任务被称为回合任务（episodic tasks）；而有些任务是一直持续的，这些任务被称为持续任务（continuous tasks）。</p><p>对于这里的网格世界，我们有两种规定终止状态的方法：</p><ul><li>一旦智能体进入$s_9$，智能体的动作空间就变为$\mathcal{A}(s_9)&#x3D;{a_5}$，即踏入终止状态就不会做其他的动作。</li><li>智能体进入$s_9$后，动作空间依然和其他状态的相同：$\mathcal{A}(s_9)&#x3D;{a_1,\ldots,a_5}$​，即踏入终止状态依然有可能走出这个状态。</li></ul><p>在这里使用第二种处理方法。</p><h4 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h4><p>基于以上概念， 强化学习的框架符合马尔可夫决策过程（Markov decision process, MDP），其具有以下重要元素：</p><h5 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h5><ul><li>状态集合：所有状态的集合$S$。</li><li>动作集合：在一个状态下所有动作的集合$A(s), s\in S$。</li><li>奖励集合：在某个$(s,a)$下所有奖励的集合$R(s,a)$。</li></ul><h5 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h5><ul><li>状态转移概率：在状态$s$下执行动作$a$后转移到状态$s’$的概率$p(s’|s,a)$，对所有$(s,a)$有$\sum_{s’\in\mathcal{S}}p(s’|s,a)&#x3D;1$。</li><li>奖励概率：在状态$s$下执行动作$a$后获得奖励$r$的概率$p(r|s,a)$，对所有$(s,a)$有$\sum_{r\in\mathcal{R}(s,a)}p(r|s,a)&#x3D;1$​​。</li></ul><p>这里的$p(s’|s,a)$和$p(r|s,a)$可以被称为模型（model）或者动力学（dynamics）。这样的模型可以是静态或者是非静态的，即如上两个概率可以是确定或者是不确定的。这里我们只考虑静态情况。</p><h5 id="策略-1"><a href="#策略-1" class="headerlink" title="策略"></a>策略</h5><p>对于状态$s$采取动作$a$的概率$\pi(a|s)$，对于任何$s$有$\sum_{a\in\mathcal{A}(s)}\pi(a|s)&#x3D;1$。</p><h5 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h5><p>MDP遵循的马尔可夫性质是每个时间步的状态只依赖于上个时间步的状态，而不依赖于这之前的所有状态和动作，即：<br>$$<br>\begin{aligned}<br>p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)&#x3D;p(s_{t+1}|s_t,a_t),\\<br>p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)&#x3D;p(r_{t+1}|s_t,a_t).<br>\end{aligned}<br>$$<br>马尔可夫过程和马尔可夫决策过程有一定区别。前者并没有考虑不同的决策，而后者一旦固定了策略，就可以被视为一种马尔可夫过程。如下图，左图的策略确定，因此就可以形成右图的马尔可夫过程。</p><div style="display: flex; justify-content: center;">  <img src="markov_process.png" alt="马尔可夫过程" style="zoom: 50%;"></div><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>强化学习就是智能体与环境交互的过程，此处，智能体之外的东西就是环境。智能体根据环境的状态做出决策并执行动作，动作作用域环境，环境反馈奖励并转移到另一个状态，如此反复。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
