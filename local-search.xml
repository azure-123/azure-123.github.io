<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记：Responsive Safety in Reinforcement Learning by PID Lagrangian Methods</title>
    <link href="/2024/06/18/PID/"/>
    <url>/2024/06/18/PID/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://proceedings.mlr.press/v119/stooke20a">https://proceedings.mlr.press/v119/stooke20a</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>使用PID控制的方式来求解跟强化学习安全性相关的拉格朗日问题，根据策略动态调整拉格朗日乘数$\lambda$的值，来减小传统拉格朗日的震荡问题。</p><h4 id="强化学习安全性"><a href="#强化学习安全性" class="headerlink" title="强化学习安全性"></a>强化学习安全性</h4><p>强化学习安全性需要解决的，就是带约束的强化学习问题。具体来说，在限制代价的情况下，尽可能地最大化智能体的回报，形式化表达为：<br>$$<br>\pi^*&#x3D;\arg\max_\pi J(\pi)\quad\text{s.t. }J_C(\pi)\leq d<br>$$<br>由于在深度强化学习中，策略是多次迭代获得的，因此每次迭代就是遵循以下约束：<br>$$<br>\begin{aligned}&amp;\max_\pi J(\pi_k)\<br>&amp;\text{s.t. }J_C(\pi_m)\leq d\quad m\in{0,1,…,k}\end{aligned}<br>$$</p><h4 id="动力系统和最优控制"><a href="#动力系统和最优控制" class="headerlink" title="动力系统和最优控制"></a>动力系统和最优控制</h4><p>本文将强化学习策略和PID控制，与动力系统和最优控制结合在一起。对于带反馈控制的离散时间系统，一个通用的形式化为：<br>$$<br>\begin{aligned}\mathbf{x}_{k+1}&#x3D;&amp; F(\mathbf{x}_k,\mathbf{u}_k)\\<br>\mathbf{y}_{k}&#x3D;&amp; Z(\mathbf{x}_k)\\<br>\mathbf{u}_k&#x3D;&amp; h(\mathbf{y}_0,…,\mathbf{y}_k) \end{aligned}<br>$$<br>其中，状态向量为$\mathbf{x}$，动态方程为$F$，测量输出为$\mathbf{y}$，应用控制为$\mathbf{u}$，时间步为下标$k$。对于最优控制，需要设计一个反馈规则$h$​，其可以接触到所有过去和现在的测量输出。本文涉及的是一种控制仿射系统，其动态方程可以表示为：<br>$$<br>F(\mathbf{x}_k,\mathbf{u}_k)&#x3D;f(\mathbf{x}_k)+g(\mathbf{x}_k)\mathbf{u}_k<br>$$<br>这个形式符合本文中讨论的强化学习安全性问题的求解。</p><h4 id="带约束的RL作为动力系统"><a href="#带约束的RL作为动力系统" class="headerlink" title="带约束的RL作为动力系统"></a>带约束的RL作为动力系统</h4><p>根据以上对于动力系统的定义，可以将带约束的RL也看作一个一阶动力系统：<br>$$<br>\begin{aligned}\theta_{k+1}&#x3D;&amp; F(\theta_k,\lambda_k)\\<br>y_{k}&#x3D;&amp; J_C(\pi_{\theta_k})\\<br>\lambda_{k}&#x3D;&amp; h(y_0,…,y_k,d)<br>\end{aligned}<br>$$<br>在RL的场景下，动态方程为$F$是一个有关智能体参数$\theta$更新的具有不确定性的非线性函数，代价目标函数$J_C$作为系统的测量输出，根据测量输出和代价限制$d$，可以使用反馈控制规则$h$来生成拉格朗日乘子$\lambda$。</p><p>经整理，可得到：<br>$$<br>F(\theta_k,\lambda_k)&#x3D;f(\theta_k)+g(\theta_k)\lambda_k\\<br>f(\theta_k)&#x3D;\theta_k+\eta\nabla_\theta J(\pi_{\theta_k})\\<br>g(\theta_k)&#x3D;-\eta\nabla_\theta J_C(\pi_{\theta_k})<br>$$<br>其中$\eta$​是SGD的学习率。控制器的作用是驱动不等式约束违规$(J_c-d)_+$到0，拉格朗日乘子的更新规则为：<br>$$<br>\lambda_{k+1}&#x3D;(\lambda_k+K_I(J_C-d))_+<br>$$<br>为了动态调节$\lambda$的更新速度保持步长一致，使用缩放的目标函数：<br>$$<br>\theta^*(\lambda)&#x3D;\arg\max_\theta J-\lambda J_C&#x3D;\arg\max_\theta\frac1{1+\lambda}(J-\lambda J_C)<br>$$</p><h4 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h4><h5 id="约束控制强化学习算法流程"><a href="#约束控制强化学习算法流程" class="headerlink" title="约束控制强化学习算法流程"></a>约束控制强化学习算法流程</h5><p><img src="/2024/06/18/PID/CCRL.png" alt="约束控制强化学习算法训练流程"></p><p>对应代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">vanilla_policy_loss</span>(<span class="hljs-params">self, obs, act, logp_old, advantage, cost_advantage,</span><br><span class="hljs-params">                            multiplier, *args, **kwargs</span>):<br>        pi, _, logp = self.actor_forward(obs, act)<br>        ratio = torch.exp(logp - logp_old)<br>        clip_adv = torch.clamp(ratio, <span class="hljs-number">1</span> - self.clip_ratio,<br>                               <span class="hljs-number">1</span> + self.clip_ratio) * advantage<br><br>        qc_penalty = (ratio * cost_advantage * multiplier).mean()<br>        loss_vallina = -(torch.<span class="hljs-built_in">min</span>(ratio * advantage, clip_adv)).mean()<br>        loss_pi = loss_vallina + qc_penalty<br>        loss_pi /= <span class="hljs-number">1</span> + multiplier<br>        <span class="hljs-comment"># Useful extra info</span><br>        approx_kl = (logp_old - logp).mean().item()<br><br>        ent = pi.entropy().mean().item()<br>        clipped = ratio.gt(<span class="hljs-number">1</span> + self.clip_ratio) | ratio.lt(<span class="hljs-number">1</span> - self.clip_ratio)<br>        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()<br>        pi_info = <span class="hljs-built_in">dict</span>(KL=approx_kl,<br>                       Entropy=ent,<br>                       ClipFrac=clipfrac,<br>                       LossQcPenalty=to_ndarray(qc_penalty),<br>                       LossVallina=to_ndarray(loss_vallina))<br><br>        <span class="hljs-keyword">return</span> loss_pi, pi_info, pi<br></code></pre></td></tr></table></figure><h5 id="PID控制拉格朗日乘子"><a href="#PID控制拉格朗日乘子" class="headerlink" title="PID控制拉格朗日乘子"></a>PID控制拉格朗日乘子</h5><p><img src="/2024/06/18/PID/PID.png" alt="PID训练过程"></p><p>对应代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LagrangianPIDController</span>:<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Lagrangian multiplier controller</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, KP, KI, KD, thres, per_state=<span class="hljs-literal">True</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.KP = KP<br>        self.KI = KI<br>        self.KD = KD<br>        self.thres = thres<br>        self.error_old = <span class="hljs-number">0</span><br>        self.error_integral = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">control</span>(<span class="hljs-params">self, qc</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        @param qc [batch,]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        error_new = torch.mean(qc - self.thres)  <span class="hljs-comment"># [batch]</span><br>        error_diff = relu(error_new - self.error_old)<br>        self.error_integral = torch.mean(relu(self.error_integral + error_new))<br>        self.error_old = error_new<br><br>        multiplier = relu(self.KP * relu(error_new) + self.KI * self.error_integral +<br>                          self.KD * error_diff)<br>        <span class="hljs-keyword">return</span> torch.mean(multiplier)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>安全性</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARL经典算法MFRL：Mean Field Reinforcement Learning </title>
    <link href="/2024/06/07/MFRL/"/>
    <url>/2024/06/07/MFRL/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://proceedings.mlr.press/v80/yang18d.html">https://proceedings.mlr.press/v80/yang18d.html</a></p><p>代码链接：<a href="https://github.com/mlii/mfrl">https://github.com/mlii/mfrl</a></p><h4 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h4><p>目前现有的多智能体强化学习算法集中于小数量的智能体数，而当智能体数急剧上升后，状态和动作空间也会成倍增加，训练也变得比较困难。本文针对这个问题，提出了平均场强化学习（Mean Field Reinforcement Learning），把一群智能体转化为一个中心智能体和其周围智能体两者之间的效果。</p><h4 id="形式化表达"><a href="#形式化表达" class="headerlink" title="形式化表达"></a>形式化表达</h4><p>在纳什均衡当中，每个智能体的策略为$\pi^j_*$，其他智能体的策略统一表示为$\boldsymbol{\pi}^{-j}_{*}$。</p><p>对于一个纳什策略，纳什价值函数为$\boldsymbol{v}^{\operatorname{Nash}}(s) \triangleq\left[v_{\pi_*}^1(s), \ldots, v_{\pi_*}^N(s)\right]$。纳什算子$\mathscr{H}$可表示为$\mathscr{H}^{\mathrm{Nash}} \boldsymbol{Q}(s, \boldsymbol{a})&#x3D;\mathbb{E}_{s^{\prime} \sim p}\left[\boldsymbol{r}(s, \boldsymbol{a})+\gamma \boldsymbol{v}^{\mathrm{Nash}}\left(s^{\prime}\right)\right]$，Q函数会最终收敛到一个值，被称为纳什Q值。</p><h4 id="平均场MARL"><a href="#平均场MARL" class="headerlink" title="平均场MARL"></a>平均场MARL</h4><p>对于一个智能体$j$，其Q值函数可以被表达为$Q^j(s, \boldsymbol{a})&#x3D;\frac{1}{N^j} \sum_{k \in \mathcal{N}(j)} Q^j\left(s, a^j, a^k\right)$，其中 $\mathcal{N}(j)$为$j$的邻近智能体的索引集合，而$N^j&#x3D;|\mathcal{N}(j)|$表示邻近智能体的数量。由表达式可知，智能体$j$的Q值函数可以表达为其与所有邻近智能体两两之间的Q值之和。</p><h4 id="平均场近似"><a href="#平均场近似" class="headerlink" title="平均场近似"></a>平均场近似</h4><p>考虑离散动作空间，智能体$j$的动作通过独热编码可以被表示为$a^j \triangleq\left[a_1^j, \ldots, a_D^j\right]$，即该智能体有$D$个可能的动作。智能体$j$的邻近智能体的平均动作为$\bar{a}^j\triangleq\left[\bar{a}_1^j, \ldots, \bar{a}_D^j\right]$，每个邻近智能体$k$的动作可以由平均动作和一定波动组成：$a^k&#x3D;\bar{a}^j+\delta a^{j, k}, \quad \text{where }\bar{a}^j&#x3D;\frac{1}{N^j} \sum_k a^k$。根据泰勒定理，如果$Q^j\left(s, a^j, a^k\right)$对$a^k$二次可微，$j$的Q值函数可以被展开为：<br>$$<br>\begin{aligned}<br>&amp;Q^{j}(s,\boldsymbol{a})&#x3D;\frac{1}{N^{j}}\sum_{k}Q^{j}(s,a^{j},a^{k}) \\<br>&amp;&#x3D;\frac1{N^j}\sum_k\left[Q^j(s,a^j,\bar{a}^j)+\nabla_{\bar{a}^j}Q^j(s,a^j,\bar{a}^j)\cdot\delta a^{j,k}\right]+\frac12\delta a^{j,k}\cdot\nabla_{\tilde{a}^{j,k}}^2Q^j(s,a^j,\tilde{a}^{j,k})\cdot\delta a^{j,k} \\<br>&amp;&#x3D;Q^j(s,a^j,\bar{a}^j)+\nabla_{\bar{a}^j}Q^j(s,a^j,\bar{a}^j)\cdot\left[\frac{1}{N^j}\sum_k\delta a^{j,k}\right]+\frac{1}{2N^{j}}\sum_{k}\left[\delta a^{j,k}\cdot\nabla_{\tilde{a}^{j,k}}^{2}Q^{j}(s,a^{j},\tilde{a}^{j,k})\cdot\delta a^{j,k}\right] \\<br>&amp;&#x3D;Q^j(s,a^j,\bar{a}^j)+\frac1{2N^j}\sum R_{s,a^j}^j(a^k)\approx Q^j(s,a^j,\bar{a}^j),<br>\end{aligned}<br>$$</p><p>其中$R_{s,a^j}^j(a^k)\triangleq\delta a^{j,k}\cdot\nabla_{\tilde{a}^{j,k}}^2Q^j(s,a^j,\tilde{a}^{j,k})\cdot\delta a^{j,k}$为泰勒多项式的余数，$\tilde{a}^{j,k}&#x3D;\bar{a}^j+\epsilon^{j,k}\delta a^{j,k}$。公式第二个等号通过泰勒定理将$Q^j\left(s, a^j, a^k\right)$展开；第三个等号的第一项为求和再求平均，即为$Q^j\left(s, a^j, \bar{a}^j\right)$本身，第二项提出来了与$k$无关的$\nabla_{\bar{a}^j}Q^j(s,a^j,\bar{a}^j)$，并对$\delta a^{j,k}$求和，由于$\delta a^{j,k}$是相对于平均值的波动，因此和为0可以消去。最终得到第四个等号的值。余数$R_{s,a^j}^j(a^k)$以$[-2M,2M]$为界，以$Q^j\left(s, a^j, a^k\right)$是M-光滑的为条件。因此，$R_{s,a^j}^j(a^k)$是一个接近于0的微小波动并相互抵消。因此约等号成立。</p><p>由以上推导可知，$Q^j\left(s, a^j, \bar{a}^j\right)$可以近似于$Q^j(s, \boldsymbol{a})$。</p><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>对基于Q值的强化学习（MF-Q），通过损失函数更新：$\mathscr{L}(\phi^j)&#x3D;\left(y^j-Q_{\phi^j}(s,a^j,\bar{a}^j)\right)^2$</p><p>对基于actor-critic架构的强化学习（MF-AC），通过损失函数更新策略梯度：$\nabla_{\theta^j}\mathcal{J}(\theta^j)\approx\nabla_{\theta^j}\log\pi_{\theta^j}(s)Q_{\phi^j}(s,a^j,\bar{a}^j)\Big|_{a&#x3D;\pi_{\theta^j}(s)}$</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>MF-Q</p><p><img src="/2024/06/07/MFRL/MF-Q.png" alt="MF-Q算法流程"></p><p>MF-AC</p><p><img src="/2024/06/07/MFRL/MF-AC.png" alt="MF-AC算法流程"></p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>经典算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning</title>
    <link href="/2024/06/06/PolicyCleanse/"/>
    <url>/2024/06/06/PolicyCleanse/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_PolicyCleanse_Backdoor_Detection_and_Mitigation_for_Competitive_Reinforcement_Learning_ICCV_2023_paper.html">https://openaccess.thecvf.com/content/ICCV2023/html/Guo_PolicyCleanse_Backdoor_Detection_and_Mitigation_for_Competitive_Reinforcement_Learning_ICCV_2023_paper.html</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>针对对抗场景下动触发的强化学习后门攻击的防御，灵感来自于BackdooRL这一动作触发的后门攻击。</p><h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><p>利用受害智能体的反向奖励函数来优化单独策略，该方法可以大概率迅速识别一个潜在的触发器，称为伪触发器。</p><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>同<a href="https://azure-123.github.io/2024/03/10/BACKDOORL/">BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning</a>的威胁模型和问题定义。在本文中，使用$(s_S,a_S)$和$(s_T,a_T)$分别表示对手智能体和受害智能体的状态-动作对，用木马策略来形容$\pi_{fail}$，用无害策略来形容$\pi_{win}$。</p><p>该问题相比较普通的强化学习后门攻击的问题，最大的不同和难点在于两点：一是触发器的搜索空间是一系列动作，且该动作持续时间未知且有可能处于连续动作空间；二是防御方法无法接触到受害智能体的价值网络，对后门防御措施有着更严格的限制。</p><h4 id="后门检测"><a href="#后门检测" class="headerlink" title="后门检测"></a>后门检测</h4><p><img src="/2024/06/06/PolicyCleanse/policy_cleanse_fig.png" alt="PolicyCleanse方法图示"></p><p>本文发现，在硬编码对手智能体输出随机动作或者不动的情况下，木马策略相比较无害策略依然会有性能的下降，但是这样的下降在起初的时间步并不明显，只有在多个时间步之后才能显现出来。由此可知，衡量受害智能体是否做出了木马策略，可以等几步然后查看它的累计奖励。</p><p>受到上述启发，本文提出PolicyCleanse来辨别触发器。基本思想为用强化学习算法PPO学习一个策略$\pi_S(\cdot|\theta_S)$来拟合触发动作。受害智能体的策略是固定的，不一起学习。训练过程包括两个阶段：</p><h5 id="第一阶段：Performing"><a href="#第一阶段：Performing" class="headerlink" title="第一阶段：Performing"></a>第一阶段：Performing</h5><p>使PolicyCleanse策略$\pi_S$在受害智能体面前做出一些可能会触发后门的动作，类似于对抗强化学习中的对手训练。本文只让$\pi_S$的轨迹为$N$的长度，即触发后门的动作只持续$N$个时间步。受害智能体从环境默认获得的奖励为$R_T$。</p><h5 id="第二阶段：Observing"><a href="#第二阶段：Observing" class="headerlink" title="第二阶段：Observing"></a>第二阶段：Observing</h5><p>此时$\pi_S$不做动作，只是观察受害智能体持续$M$个时间步（经验上选取$M&#x3D;50$），并收集其累计奖励。使用受害智能体奖励的相反数：$R_{\mathrm{sum}}&#x3D;-\sum_{t&#x3D;N} R_{\mathrm{T}}\left(s_{\mathrm{S}}^{(t)}, s_{\mathrm{T}}^{(t)}, a_{\mathrm{T}}^{(t)}\right)$。学习到的动作对应的$R_{\mathrm{sum}}$比一个阈值$T$要高的话，就定该动作为（伪）触发器，$R_S$给一个正数的奖励，否则给一个负数的惩罚。</p><p>PPO的更新通过如下计算：<br>$$<br>\hat{\theta}_{\mathrm{S}}&#x3D;\underset{\theta_{\mathrm{S}}}{\operatorname{argmax}} \underbrace{\sum_{t&#x3D;0}^{N-1}-\gamma^t R_{\mathrm{T}}\left(s_{\mathrm{S}}^{(t)}, s_{\mathrm{T}}^{(t)}, a_{\mathrm{T}}^{(t)}\right)}_{\text {Performing phase }}+\underbrace{\gamma^N R_{\mathrm{S}}^{(N)}}_{\text {Observing phase }}<br>$$</p><h5 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h5><p><img src="/2024/06/06/PolicyCleanse/policy_cleanse_alg.png" alt="PolicyCleanse算法流程"></p><p>同时对环境进行了随机化，用不同的随机种子训练出一系列不同的$\pi_S$，并计算导致成功检测后门触发器的随机种子的比例。</p><h4 id="后门缓解"><a href="#后门缓解" class="headerlink" title="后门缓解"></a>后门缓解</h4><p>基于后门检测成功检测出了受害智能体和触发器的基础上，需要消除或缓解这些触发器并净化受害智能体的策略$\pi_T(\cdot|\theta)$。</p><p>本文使用基于忘却学习的方法来缓解后门。具体来说，将受害智能体与环境交互的轨迹中的动作替换成正确的动作，来最大化累计奖励。做法有两个：一个是分配正常的奖励，另一个是使用没有受到过攻击的模型。</p><p>最后，使用行为克隆来重新训练目标智能体，训练数据使用上一步替换过的轨迹和一些它自己与环境交互得到的干净轨迹。</p><h4 id="自适应攻击"><a href="#自适应攻击" class="headerlink" title="自适应攻击"></a>自适应攻击</h4><p>本文还提出了对PolicyCleanse的自适应攻击，即绕开PolicyCleanse检测机制的自适应攻击。</p><h5 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h5><p><img src="/2024/06/06/PolicyCleanse/adaptive_backdoorl.png" alt="自适应攻击算法流程"></p><p>这样的自适应攻击会显著增加受害智能体失败的时间步数，而这样会导致隐蔽性受到影响，很容易被人类观测并控制。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型经典算法DPO：Direct Preference Optimization: Your Language Model is Secretly a Reward Model</title>
    <link href="/2024/05/10/DPO/"/>
    <url>/2024/05/10/DPO/</url>
    
    <content type="html"><![CDATA[<p>坑待填中。。</p>]]></content>
    
    
    
    <tags>
      
      <tag>经典算法</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARL经典算法FACMAC：FACMAC: Factored Multi-Agent Centralised Policy Gradients</title>
    <link href="/2024/04/29/FACMAC/"/>
    <url>/2024/04/29/FACMAC/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/pdf/2003.06709">https://arxiv.org/pdf/2003.06709</a></p><p>代码链接：<a href="https://github.com/oxwhirl/facmac">https://github.com/oxwhirl/facmac</a></p><h4 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h4><p>FACMAC算法对于离散动作空间和连续动作空间的合作多智能体任务都有效。</p><ul><li><p>FACMAC使用深度确定性策略梯度（DDPG）来训练去中心化的策略，而它的使用一个中心化又因子化的critic，用非线性单调函数来串联每个智能体的$Q_a$，拟合获得总的Q值$Q_{tot}$。与COMA、MADDPG相比，它的critic是因子化的，因此更好学到更多智能体或动作时的Q值。相比基于价值的方法如QMIX，它没有据固有约束，因此就可以考虑使用丰富的价值因子，包括非单调的。</p></li><li><p>FACMAC使用一个新的中心化的梯度估计，优化整个共同动作空间，而不是像MADDPG那样分开优化，因此可以学到更协调的动作组合，防止达到次优解。为了克服MADDPG对相对泛化的脆弱性，FACMAC在评估动作价值函数的时候从当前的策略取样动作。</p></li></ul><h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><h5 id="中心化且因子化critic设计"><a href="#中心化且因子化critic设计" class="headerlink" title="中心化且因子化critic设计"></a>中心化且因子化critic设计</h5><p>将价值分解应用于actor-critic框架的核心优势在于，相比基于价值的方法，这样更有利于灵活分解，无需价值函数的限制，也就可以去掉单调约束。所有智能体都享有一个中心化critic：<br>$$<br>Q_{tot}^\mu(\tau,\mathbf{u},s;\phi,\psi)&#x3D;g_\psi(s,{Q_a^{\mu_a}(\tau_a,u_a;\phi_a)}_{a&#x3D;1}^n)<br>$$</p><p>这个critic的损失函数为：</p><p>$$<br>\mathcal{L}(\phi, \psi)&#x3D;\mathbb{E}_{\mathcal{D}}\left[\left(y^{t o t}-Q_{t o t}^{\boldsymbol{\mu}}(\boldsymbol{\tau}, \mathbf{u}, s ; \phi, \psi)\right)^2\right]<br>$$</p><h5 id="中心化策略梯度"><a href="#中心化策略梯度" class="headerlink" title="中心化策略梯度"></a>中心化策略梯度</h5><p>非中心化的策略梯度通常会忽略智能体之间的合作关系，每个智能体容易收敛至次优策略；而且，会导致方法对相对泛化脆弱，即一个智能体在更新策略梯度的时候，只有它自己的动作是从当前的策略采样到的，而其他智能体的策略是通过回放池采样的，这可能与其他智能体在当前策略下会采样的动作大相径庭。</p><p>因此，FACMAC将求得整体策略梯度：<br>$$<br>\nabla_\theta J(\boldsymbol{\mu})&#x3D;\mathbb{E}_{\mathcal{D}}\left[\nabla_\theta \boldsymbol{\mu} \nabla_{\boldsymbol{\mu}} Q_{t o t}^{\boldsymbol{\mu}}\left(\boldsymbol{\tau}, \mu_1\left(\tau_1\right), \ldots, \mu_n\left(\tau_n\right), s\right)\right]<br>$$</p><h4 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h4><p><img src="/2024/04/29/FACMAC/FACMAC.png" alt="(a)去中心化的策略网络。由于在使用离散动作时从类别分布中采样，因此存在一个采样步长。(b)中心化但因子化的critic。(c)非线性单调混合函数。"></p><h4 id="离散策略学习"><a href="#离散策略学习" class="headerlink" title="离散策略学习"></a>离散策略学习</h4><p>本文使用Gumbel-Softmax估计量使用连续的分布来近似离散的动作，并且使用Straight-Through Gumbel-Softmax估计量来确保训练和测试过程中的动作动态是相同的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>经典算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARL经典算法MAPPO：The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games</title>
    <link href="/2024/04/25/MAPPO/"/>
    <url>/2024/04/25/MAPPO/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/pdf/2103.01955.pdf">https://arxiv.org/pdf/2103.01955.pdf</a></p><p>代码链接：<a href="https://github.com/marlbenchmark/on-policy">https://github.com/marlbenchmark/on-policy</a></p><p>本文为MAPPO的详解，即PPO往多智能体场景中的迁移。</p><h4 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h4><p>本篇论文并没有提出一个全新的MARL算法，而是通过简单的修改来凭经验证明PPO在很广泛的合作多智能体场景中有非常亮眼的表现，并提供了MAPPO的实现思路。</p><p>具体的内容如下：</p><ul><li><p>PPO无需特定领域的算法或者架构改变，并进行极小的微调，就可以达到可以匹敌其他多智能体合作基准离线算法的表现。</p></li><li><p>PPO使用与其他算法相当的样本量就可以达到这个效果。</p></li><li><p>分析影响PPO性能的实施和超参数因素，并就</p></li><li><p>这些因素的最佳实践提供了具体建议。</p></li></ul><h4 id="算法架构"><a href="#算法架构" class="headerlink" title="算法架构"></a>算法架构</h4><p>MAPPO使用的是CTDE架构，状态的价值函数$V_\phi(s)$可以将智能体局部观测中不存在的全局状态信息并只在训练过程中使用到。这也体现了和IPPO的区别，即IPPO将局部观测当作策略和状态价值函数的输入。</p><h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><ul><li>使用参数共享，即每个智能体的观测空间和动作空间是对等的，这样可以提高学习效率。</li><li>采用了实现PPO时使用的技巧，包括Generalized Advantage Estimation（广义优势估计，GAE）、优势归一化、价值裁剪等。</li></ul><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>使用环境和对比算法如下：</p><ul><li><p>the multi-agent particle-world environment (MPE)：QMIX、MADDPG</p></li><li><p>the StarCraft micromanagement challenge (SMAC)：QMIX、QPLEX、CWQMIX、AIQMIX、RODE</p></li><li><p>Google Research Football (GRF)：QMIX、CDS、TiKick</p></li><li><p>the Hanabi challenge：SAD、VDN</p></li></ul><h4 id="影响MAPPO的因素"><a href="#影响MAPPO的因素" class="headerlink" title="影响MAPPO的因素"></a>影响MAPPO的因素</h4><h5 id="值标准化"><a href="#值标准化" class="headerlink" title="值标准化"></a>值标准化</h5><ul><li>在价值学习过程中，将价值函数的输出范围归一化，控制在合理的区间内，可以增强训练的稳定性</li></ul><h5 id="价值函数输入"><a href="#价值函数输入" class="headerlink" title="价值函数输入"></a>价值函数输入</h5><p>过去的成果中大部分用两种方法来表示全局状态：</p><ul><li><p><strong>Concatenation of Local observations (CL)：</strong>将每个智能体的观测全部拼接起来，获得一个整体的状态。但是这样的方式获得的状态会随智能体的数量增加而陡然增加，并会忽略一些重要的全局信息。</p></li><li><p><strong>Environment-Provided global state (EP)：</strong>包含了环境状态中所有智能体通用的信息。但是这样的状态会忽略一些重要的局部特定智能体的信息。</p></li></ul><p><strong>Agent-Specific Global State (AS)：</strong>本文采取的方式，结合了以上两者的特点，为每个智能体$i$特定了价值函数的输出为EP状态和其局部观测$o_i$。但是EP状态和$o_i$可能会有重叠的部分，导致状态维数增大和状态信息冗余。</p><p><strong>Featured-Pruned Agent-Specific Global State (FP)：</strong>为了弥补AS的缺陷，对AS状态进行处理，剔除重复的状态</p><p><img src="/2024/04/25/MAPPO/state_design.png" alt="几个方式的实现示例图"></p><h5 id="训练数据使用"><a href="#训练数据使用" class="headerlink" title="训练数据使用"></a>训练数据使用</h5><p>本文发现，在MAPPO的训练过程中，样本的过多次利用会导致算法性能下降。因此需要适当减小epoch。</p><h5 id="策略-价值裁剪"><a href="#策略-价值裁剪" class="headerlink" title="策略&#x2F;价值裁剪"></a>策略&#x2F;价值裁剪</h5><p>在使用$\epsilon$进行PPO裁剪的时候，需要寻找恰当的值。当$\epsilon$较小时，训练较为稳定，但是会以牺牲训练时间为代价。</p><h5 id="批大小"><a href="#批大小" class="headerlink" title="批大小"></a>批大小</h5><p>批次更大会让求梯度更加准确，但是过大会浪费内存资源。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>经典算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning</title>
    <link href="/2024/03/26/BadRL/"/>
    <url>/2024/03/26/BadRL/</url>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/pdf/2312.12585.pdf">https://arxiv.org/pdf/2312.12585.pdf</a></p><p>代码：<a href="https://github.com/7777777cc/code">https://github.com/7777777cc/code</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>对强化学习的<strong>稀疏的</strong>且<strong>有目标的</strong>后门攻击方法。当前的强化学习后门攻击方法无论是在训练还是测试过程中，都是基于密集的攻击步数，有些被攻击的状态不一定有显著效果，也增加了被检测的风险。</p><h4 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h4><h5 id="触发器层面"><a href="#触发器层面" class="headerlink" title="触发器层面"></a>触发器层面</h5><p>使用具体到样本的方法来生成后门触发器，使触发器更易学习且不易被遗忘，并最大化有触发器和无触发器模型的梯度的互信息。</p><h5 id="时间步选择层面"><a href="#时间步选择层面" class="headerlink" title="时间步选择层面"></a>时间步选择层面</h5><p>评估特定时间步的攻击价值，只选择攻击价值高的状态进行攻击。</p><h4 id="攻击模型"><a href="#攻击模型" class="headerlink" title="攻击模型"></a>攻击模型</h4><h5 id="攻击者知识"><a href="#攻击者知识" class="headerlink" title="攻击者知识"></a>攻击者知识</h5><p>采用黑盒攻击，满足以下条件：</p><ul><li><p>攻击者不知道用于训练受害智能体的算法。</p></li><li><p>攻击者不知道环境MDP，即状态转换信息等。</p></li><li><p>攻击者可以接触到输入到智能体中的观测状态。</p></li></ul><h5 id="攻击者能力"><a href="#攻击者能力" class="headerlink" title="攻击者能力"></a>攻击者能力</h5><p>攻击者可以攻击且仅可以攻击训练和测试过程中当前时间步的状态、动作和奖励信息</p><h6 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h6><p>攻击后的三元组为$(\tilde{s}_t,\tilde{a}_t(a_t),\tilde{r}_t,)$。其中带触发器的状态为$\tilde{s}_t&#x3D;s_t+\delta$，$\tilde{a}_t$是强攻击，$a_t$是弱攻击。满足仅攻击很小的样本数量的条件：<br>$$<br>\sum_{t&#x3D;1}^T\mathbb{1}[(s_t,a_t,r_t)\neq(\tilde{s}_t,\tilde{a}_t(a_t),\tilde{r}_t,)]\leq\epsilon T<br>$$</p><h4 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h4><p>测试过程相比训练过程能力有限，只能操控状态。</p><p>诱导智能体在触发后门的情况下，输出目标动作，这个目标动作由环境指定，并在整个环境中保持不变，基于目标动作和原动作之间的优势差异选择。攻击过程可以形式化为一个最优化过程：<br>$$<br>\begin{align}<br>\min_{\tilde{s}_{1:T},\tilde{a}_{1:T},\tilde{r}_{1:T}}E_{s_0\sim\mu_0}[\tilde{V}^{\tilde{\pi}_T}(s_0)]\\<br>\text{s.t. }\sum_{t&#x3D;1}^T\mathbb{1}[(s_t,a_t,r_t)\neq(\tilde{s}_t,\tilde{a}_t(a_t),\tilde{r}_t,)]\leq\epsilon T,\\<br>\tilde{\pi}_T(s+\delta)&#x3D;a^{\dagger},\forall s \in S^{\dagger},\\<br>E_{s_0\sim\mu_0}[{V}^{\tilde{\pi}_T}(s_0)]&#x3D;E_{s_0\sim\mu_0}[{V}^{\pi^*}(s_0)]<br>\end{align}<br>$$</p><p>可总结为两个核心问题，即<strong>何时攻击（目标状态的选取）</strong>和<strong>如何攻击（目标动作的选取）</strong></p><h4 id="BadRL攻击框架"><a href="#BadRL攻击框架" class="headerlink" title="BadRL攻击框架"></a>BadRL攻击框架</h4><h5 id="何时攻击"><a href="#何时攻击" class="headerlink" title="何时攻击"></a>何时攻击</h5><h6 id="定义攻击价值"><a href="#定义攻击价值" class="headerlink" title="定义攻击价值"></a>定义攻击价值</h6><p>计算受攻击和未受攻击的状态的价值，求出二者的差：<br>$$<br>V_A(s)&#x3D;Q^*(s,\pi^*(s))-Q^*(s,a^{\dagger}),\forall s\in S^{\dagger}<br>$$</p><h6 id="触发器设计"><a href="#触发器设计" class="headerlink" title="触发器设计"></a>触发器设计</h6><p>基于互信息调优来生成后门触发器，使无后门模型和有后门模型的优化方向对齐，进而训练路径相似。计算互信息，优化触发器模式，最小化损失函数：<br>$$<br>loss_{MI}&#x3D;-MI(g_{clean},g_{poisoned})<br>$$</p><h5 id="如何攻击"><a href="#如何攻击" class="headerlink" title="如何攻击"></a>如何攻击</h5><p>状态改变：由触发器设计部分确定触发器的位置和颜色像素值</p><p>动作改变：通过考虑环境的动作语义，赋予被攻击的智能体较为有效的动作<strong>（只是直觉上的一种选择）</strong></p><p>奖励改变：采用每个任务中获得的最小正奖励，尽可能保持奖励函数的完整性</p><h5 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h5><ul><li><p>动作是一种直觉上的选择，并不具备切实依据，有的任务并不那么容易区分动作的好坏。</p></li><li><p>如何拓展到多智能体强化学习问题当中，并考虑多智能体的特点？</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>日语N5语法总结</title>
    <link href="/2024/03/20/JapaneseN5/"/>
    <url>/2024/03/20/JapaneseN5/</url>
    
    <content type="html"><![CDATA[<p>参考资料：标准日本语初级（上册）<br>大写指代含义：<br>N：名词；P：地点；V：动作；T：时间；H：人物；A：形容词</p><h3 id="语法与句型结构"><a href="#语法与句型结构" class="headerlink" title="语法与句型结构"></a>语法与句型结构</h3><h4 id="陈述【N1是N2】的对应肯定、否定和疑问句"><a href="#陈述【N1是N2】的对应肯定、否定和疑问句" class="headerlink" title="陈述【N1是N2】的对应肯定、否定和疑问句"></a>陈述【N1是N2】的对应肯定、否定和疑问句</h4><p><strong>现在肯定句：</strong> N1はN2です。<br><em>のです&#x2F;んです表示所讲的内容与前句或前项内容有关联。用于说明状况或解释原因、理由。のです多用于书面语，んです为口语形式。</em><br><strong>过去肯定句：</strong>N1はN2でした。<br><strong>现在否定句：</strong>N1はN2では&#x2F;じゃありません。<br><strong>过去否定句：</strong>N1はN2では&#x2F;じゃありませんでした。<br><strong>一般疑问句：</strong><br>‐N1はN2ですか。<br>    ‐はい，そうです。<strong>（肯定）</strong><br>    ‐いいえ，ちがいます。<strong>（否定）</strong><br><strong>特殊疑问句：</strong><br>だれ&#x2F;どなたですか。<strong>（问人物）</strong><br>何ですか。<strong>（问事物）</strong><br>N1はN2ですか,N3ですか。<strong>（询问不止一种答案）</strong><br><strong>并列使用：</strong><br>N1でN2<br><strong>表示【N1（新信息）是N2（旧信息）】：</strong><br>N1がN2です。<br><em>表示疑问的词作主语时不能用は，只能用が。</em></p><h4 id="陈述【N1也是N2】"><a href="#陈述【N1也是N2】" class="headerlink" title="陈述【N1也是N2】"></a>陈述【N1也是N2】</h4><p>N1もN2です。</p><h4 id="表示从属关系【N1的N2】"><a href="#表示从属关系【N1的N2】" class="headerlink" title="表示从属关系【N1的N2】"></a>表示从属关系【N1的N2】</h4><p>N1のN2<br>其中，N1是N2从属的机构、国家或属性。若前文提到了该事物，N2可以省略</p><h4 id="表示并列关系【N1和N2】"><a href="#表示并列关系【N1和N2】" class="headerlink" title="表示并列关系【N1和N2】"></a>表示并列关系【N1和N2】</h4><p>N1とN2<br><strong>共同做某事的对象：</strong>H1とV1</p><h4 id="对名词进行选择（表示或者）"><a href="#对名词进行选择（表示或者）" class="headerlink" title="对名词进行选择（表示或者）"></a>对名词进行选择（表示或者）</h4><p>N1かN2</p><h4 id="列举名词（其中一两项）"><a href="#列举名词（其中一两项）" class="headerlink" title="列举名词（其中一两项）"></a>列举名词（其中一两项）</h4><p>N1やN2<br>常常和など呼应使用</p><h4 id="陈述【N1在P1】的对应肯定和特殊疑问句"><a href="#陈述【N1在P1】的对应肯定和特殊疑问句" class="headerlink" title="陈述【N1在P1】的对应肯定和特殊疑问句"></a>陈述【N1在P1】的对应肯定和特殊疑问句</h4><p><strong>肯定句：</strong><br>N1はP1です。<br><strong>特殊疑问句：</strong>N1はどこですか。</p><h4 id="陈述【N1存在于P1-P1有N1】的对应肯定和特殊疑问句"><a href="#陈述【N1存在于P1-P1有N1】的对应肯定和特殊疑问句" class="headerlink" title="陈述【N1存在于P1&#x2F;P1有N1】的对应肯定和特殊疑问句"></a>陈述【N1存在于P1&#x2F;P1有N1】的对应肯定和特殊疑问句</h4><p><strong>肯定句：</strong><br>N1はP1にあります（无意志事物）&#x2F;います（有意志事物）。<br>P1にN1があります&#x2F;います。<br>若N1表示事件：P1でN1があります。<br><strong>特殊疑问句：</strong><br>N1はどこですか。<br>N1はどこにありますか&#x2F;いますか。</p><h4 id="陈述动作的对象"><a href="#陈述动作的对象" class="headerlink" title="陈述动作的对象"></a>陈述动作的对象</h4><p>N1をV1</p><h4 id="陈述动作的场所"><a href="#陈述动作的场所" class="headerlink" title="陈述动作的场所"></a>陈述动作的场所</h4><p>P1でV1</p><h4 id="表示人或物体的附着点"><a href="#表示人或物体的附着点" class="headerlink" title="表示人或物体的附着点"></a>表示人或物体的附着点</h4><p>P1にV1</p><h4 id="场所移动"><a href="#场所移动" class="headerlink" title="场所移动"></a>场所移动</h4><h5 id="移动的目的地"><a href="#移动的目的地" class="headerlink" title="移动的目的地"></a>移动的目的地</h5><p>P1へ&#x2F;にV1</p><h5 id="移动的起点"><a href="#移动的起点" class="headerlink" title="移动的起点"></a>移动的起点</h5><p>P1からV1</p><h5 id="移动的交通手段"><a href="#移动的交通手段" class="headerlink" title="移动的交通手段"></a>移动的交通手段</h5><p>N1でV1<br><em>で还可以表示其他手段以及原材料</em></p><h5 id="移动的起点和目的地"><a href="#移动的起点和目的地" class="headerlink" title="移动的起点和目的地"></a>移动的起点和目的地</h5><p>P1からP2までV1</p><h5 id="表示移动行为的目的"><a href="#表示移动行为的目的" class="headerlink" title="表示移动行为的目的"></a>表示移动行为的目的</h5><p>P1へV1（去ます）に行きます&#x2F;来ます。<br><em>飲みに行きます若前面没有加喝的饮料默认为喝酒。</em></p><h5 id="表示经过某场所"><a href="#表示经过某场所" class="headerlink" title="表示经过某场所"></a>表示经过某场所</h5><p>P1を通ります&#x2F;渡ります&#x2F;過ぎます…</p><h5 id="表示离开某场所"><a href="#表示离开某场所" class="headerlink" title="表示离开某场所"></a>表示离开某场所</h5><p>P1を出ます&#x2F;卒業します…</p><h4 id="自动词和他动词"><a href="#自动词和他动词" class="headerlink" title="自动词和他动词"></a>自动词和他动词</h4><p>自动词：主语作用于宾语，用を表示。<br>他动词：没有宾语，表示主语自主动作，用が表示。</p><h4 id="询问价格"><a href="#询问价格" class="headerlink" title="询问价格"></a>询问价格</h4><p>N1はいくらですか。</p><h4 id="请求某物"><a href="#请求某物" class="headerlink" title="请求某物"></a>请求某物</h4><p>N1をください</p><h4 id="表示具体位置"><a href="#表示具体位置" class="headerlink" title="表示具体位置"></a>表示具体位置</h4><p>上&#x2F;下&#x2F;前&#x2F;後ろ&#x2F;隣&#x2F;中&#x2F;外</p><h4 id="表示全盘否定"><a href="#表示全盘否定" class="headerlink" title="表示全盘否定"></a>表示全盘否定</h4><p>疑问词+も+V1（否定）<br>…誰もいません。<br>…何もありません。</p><h4 id="任何情况下事态相同"><a href="#任何情况下事态相同" class="headerlink" title="任何情况下事态相同"></a>任何情况下事态相同</h4><p>疑问词+でも</p><h4 id="表示时间"><a href="#表示时间" class="headerlink" title="表示时间"></a>表示时间</h4><h5 id="一般时间"><a href="#一般时间" class="headerlink" title="一般时间"></a>一般时间</h5><p>今（午前&#x2F;午後）~時~分です。<br>30分＝半</p><h5 id="两个名词都是时间"><a href="#两个名词都是时间" class="headerlink" title="两个名词都是时间"></a>两个名词都是时间</h5><p><strong>前面名词包含每：</strong>两个名词之间不加の<br><strong>前面名词不含每：</strong>两个名词之间可加可不加の</p><h5 id="时间左右"><a href="#时间左右" class="headerlink" title="时间左右"></a>时间左右</h5><p>~ごろ</p><h4 id="表示动作"><a href="#表示动作" class="headerlink" title="表示动作"></a>表示动作</h4><h5 id="现在的习惯性动作状态-未来的动作状态"><a href="#现在的习惯性动作状态-未来的动作状态" class="headerlink" title="现在的习惯性动作状态&#x2F;未来的动作状态"></a>现在的习惯性动作状态&#x2F;未来的动作状态</h5><p>V1ます&#x2F;V1ません</p><h5 id="表示动作或变化正在进行"><a href="#表示动作或变化正在进行" class="headerlink" title="表示动作或变化正在进行"></a>表示动作或变化正在进行</h5><p>V1（て形）います</p><h5 id="过去的动作状态"><a href="#过去的动作状态" class="headerlink" title="过去的动作状态"></a>过去的动作状态</h5><p>V1ました&#x2F;V1ませんでした</p><h5 id="表示动作结束后留下的结果状态"><a href="#表示动作结束后留下的结果状态" class="headerlink" title="表示动作结束后留下的结果状态"></a>表示动作结束后留下的结果状态</h5><p>V1（て形）います<br><em>对“知っていますか”的否定回答必须使用“知りません”。</em></p><h5 id="动作状态的持续时间"><a href="#动作状态的持续时间" class="headerlink" title="动作状态的持续时间"></a>动作状态的持续时间</h5><p>T1（持续时间）+V1</p><h5 id="一定时间内进行若干次动作"><a href="#一定时间内进行若干次动作" class="headerlink" title="一定时间内进行若干次动作"></a><strong>一定时间内进行若干次动作</strong></h5><p>T1（持续时间）に+次数+V1<br><strong>以一定期间为基准计算频率时的省略表达：</strong><br>1日に→日に<br>1か月に→月に<br>1年に→年に</p><h5 id="表示动作相继发生"><a href="#表示动作相继发生" class="headerlink" title="表示动作相继发生"></a>表示动作相继发生</h5><p>V1（て形）+V2<br><em>连接多了会使人难以理解，因此一般只连两三项。需要连接更多则使用<strong>それから</strong></em><br>V1（て形）から+V2<strong>（不能在一个句子中出现两次以上）</strong></p><h5 id="表示动作许可"><a href="#表示动作许可" class="headerlink" title="表示动作许可"></a>表示动作许可</h5><p><strong>肯定句：</strong>V1（て形）もいいです。<br><em>通常不用于尊长，因为很傲慢</em><br><strong>否定句：</strong>V1（て形）は行けません。<br><strong>疑问句：</strong>V1（て形）もいいですか。<br><strong>肯定回答：</strong><br>どうぞ。&#x2F;かまいません。<strong>（根据自己的心情或判断）</strong><br>いいです。&#x2F;大丈夫です。<strong>（基于社会惯例或公共准则）</strong><br><strong>否定回答：</strong><br>いえ，ちょっと…&#x2F;すみません，~から。<strong>（根据自己的心情或判断）</strong><br>いけません。&#x2F;だめです。<strong>（基于社会惯例或公共准则）</strong></p><h4 id="时间和动作结合"><a href="#时间和动作结合" class="headerlink" title="时间和动作结合"></a>时间和动作结合</h4><p>T1（に）V1</p><h5 id="某时做某事"><a href="#某时做某事" class="headerlink" title="某时做某事"></a>某时做某事</h5><p><strong>包含数字的时间：</strong>加に<br><strong>昨今明含义的时间：</strong>不加に<br><strong>星期几：</strong>可加可不加に</p><h5 id="从T1到T2做某事"><a href="#从T1到T2做某事" class="headerlink" title="从T1到T2做某事"></a>从T1到T2做某事</h5><p>T1からT2までV1。</p><h5 id="询问何时做某事"><a href="#询问何时做某事" class="headerlink" title="询问何时做某事"></a>询问何时做某事</h5><p>いつ&#x2F;何曜日V1ますか。<br>いつ&#x2F;何曜日から&#x2F;までV1ますか。</p><h4 id="请求某人做某事"><a href="#请求某人做某事" class="headerlink" title="请求某人做某事"></a>请求某人做某事</h4><p>V1（て形）ください。<br>V1（て形）くださいませんか。<strong>（更加礼貌）</strong><br>そうしてください。</p><h4 id="请求-命令某人不做某事"><a href="#请求-命令某人不做某事" class="headerlink" title="请求&#x2F;命令某人不做某事"></a>请求&#x2F;命令某人不做某事</h4><p>V1（ない形）でください。</p><h4 id="表示必须做某事"><a href="#表示必须做某事" class="headerlink" title="表示必须做某事"></a>表示必须做某事</h4><p>V1（ない形去ない）なければなりません。<strong>（多用于书面语）</strong><br>V1（ない形）と（いけません）。<strong>（主要用于口语）</strong></p><h4 id="表示不做某事也可以"><a href="#表示不做某事也可以" class="headerlink" title="表示不做某事也可以"></a>表示不做某事也可以</h4><p>V1（ない形去ない）なくてもいいです。</p><h4 id="表示能力-允许"><a href="#表示能力-允许" class="headerlink" title="表示能力&#x2F;允许"></a>表示能力&#x2F;允许</h4><p>H1はV1（基本形）ことができます。</p><h4 id="表示谓语是一种动作行为"><a href="#表示谓语是一种动作行为" class="headerlink" title="表示谓语是一种动作行为"></a>表示谓语是一种动作行为</h4><p>N1はV1（基本形）ことです。<br><em>动词基本形+こと可以起到和名词同样的作用</em></p><h4 id="表示某个动作正在进行"><a href="#表示某个动作正在进行" class="headerlink" title="表示某个动作正在进行"></a>表示某个动作正在进行</h4><p>~中<br><em>一类、二类动词用ます形去ます（不是所有都能用此形式），三类动词用ます形去します（来ます不能用此形式）。</em></p><h4 id="在做某事之前"><a href="#在做某事之前" class="headerlink" title="在做某事之前"></a>在做某事之前</h4><p>V1（基本形）前に<br>N1の前に</p><h4 id="在做某事之后"><a href="#在做某事之后" class="headerlink" title="在做某事之后"></a>在做某事之后</h4><p>V1（た形）後で<br>N1の後で</p><h4 id="表示过去的经历"><a href="#表示过去的经历" class="headerlink" title="表示过去的经历"></a>表示过去的经历</h4><p>V1（た形）ことがあります。<br><em>强调完全没有某种经历，有时可以在开头加“一度も”</em></p><h4 id="做某事更好"><a href="#做某事更好" class="headerlink" title="做某事更好"></a>做某事更好</h4><p>V1（た形）ほうがいいです。<br>N1&#x2F;これ&#x2F;それ&#x2F;あれ+の+ほうがいいです。<br>この&#x2F;その&#x2F;あの+ほうがいいです。</p><h4 id="列举若干代表性动作"><a href="#列举若干代表性动作" class="headerlink" title="列举若干代表性动作"></a>列举若干代表性动作</h4><p>V1（た形）りV2（た形）りします</p><h4 id="思考内容"><a href="#思考内容" class="headerlink" title="思考内容"></a>思考内容</h4><p>小句（简体形）と思います<br><em>出现名词或二类形容词小句时加だ。</em></p><h4 id="转述他人的话"><a href="#转述他人的话" class="headerlink" title="转述他人的话"></a>转述他人的话</h4><p>H1は小句（简体形）と言いました<br><em>出现名词或二类形容词小句时加だ。</em></p><h4 id="形容词用法"><a href="#形容词用法" class="headerlink" title="形容词用法"></a>形容词用法</h4><p>一类形容词是以“い”结尾的形容词</p><h5 id="现在肯定句"><a href="#现在肯定句" class="headerlink" title="现在肯定句"></a>现在肯定句</h5><p><strong>一类形：</strong>N1はA1です。<br><strong>二类形：</strong>N1はA1です。</p><h5 id="现在否定句"><a href="#现在否定句" class="headerlink" title="现在否定句"></a>现在否定句</h5><p><strong>一类形：</strong>N1はA1（去い）くないです。&#x2F;N1はA1（去い）くありません。<br><strong>二类形：</strong>N1はA1では&#x2F;じゃありません。<br>表示否定程度不高，使用<strong>あまり+A1（否定）</strong></p><h5 id="过去肯定句"><a href="#过去肯定句" class="headerlink" title="过去肯定句"></a>过去肯定句</h5><p><strong>一类形：</strong>N1はA1（去い）かったです。<br><strong>二类形：</strong>N1はA1でした。</p><h5 id="过去否定句"><a href="#过去否定句" class="headerlink" title="过去否定句"></a>过去否定句</h5><p><strong>一类形：</strong>N1はA1（去い）くなかったです。&#x2F;N1はA1（去い）くありませんでした。<br><strong>二类形：</strong>N1はA1では&#x2F;じゃありませんでした。</p><h5 id="否定疑问句"><a href="#否定疑问句" class="headerlink" title="否定疑问句"></a>否定疑问句</h5><p>N1はA1（去い）くないですか。<br>“はい”和“いいえ”都可以使用</p><h5 id="修饰名词"><a href="#修饰名词" class="headerlink" title="修饰名词"></a>修饰名词</h5><p>一类型形容词可直接修饰名词<br><strong>一类形：</strong>A1+N1<br><em>“多い”和“少ない”不能单独修饰名词。</em><br><strong>二类形：</strong>A1なN1<br><em>对于前面已经提过的形容词修饰的名词，可在后文用“A1の”代替，避免重复。</em></p><h5 id="特殊疑问句"><a href="#特殊疑问句" class="headerlink" title="特殊疑问句"></a>特殊疑问句</h5><p>询问人或事物的性质<br>どんなN1<br><em>何の有询问内容和材料两种用法，どんな只用于询问性质。</em></p><h5 id="询问对方对某状态的意见或感想"><a href="#询问对方对某状态的意见或感想" class="headerlink" title="询问对方对某状态的意见或感想"></a>询问对方对某状态的意见或感想</h5><p>どうですか。<strong>（现在或未来）</strong><br>どうでしたか。<strong>（过去）</strong><br>いかがですか。<strong>（礼貌表示，对长辈或上级使用）</strong></p><h5 id="形容词并列"><a href="#形容词并列" class="headerlink" title="形容词并列"></a>形容词并列</h5><p><strong>一类形：</strong>A1（去い）くてA2<br><strong>二类形：</strong>A1くでA2</p><h5 id="表示状态或状况有多种可能"><a href="#表示状态或状况有多种可能" class="headerlink" title="表示状态或状况有多种可能"></a>表示状态或状况有多种可能</h5><p><strong>一类形：</strong>A1（かった形）りA2（かった形）りです<br><strong>二类形：</strong>A1（だった形）りA2（だった形）りです<br><strong>名词：</strong>N1（だった形）りN2（だった形）りです<br><em>一类和二类形容词出现的状态一般是反义。</em></p><h4 id="表示比较"><a href="#表示比较" class="headerlink" title="表示比较"></a>表示比较</h4><h5 id="表示名词比较"><a href="#表示名词比较" class="headerlink" title="表示名词比较"></a>表示名词比较</h5><h6 id="陈述句"><a href="#陈述句" class="headerlink" title="陈述句"></a>陈述句</h6><p>N1はN2よりA1です。<strong>（N1更具有形容词所表示性质）</strong><br>N1よりN2のほうがA1です。<strong>（N2更具有形容词所表示性质）</strong><br><em>和より连接的都是不及另一个的</em><br><strong>一类形：</strong>N1はN2ほどA1（去い）くないです。<br><strong>二类形：</strong>N1はN2ほどA1ではありません。</p><h6 id="特殊疑问句-1"><a href="#特殊疑问句-1" class="headerlink" title="特殊疑问句"></a>特殊疑问句</h6><p>N1とN2とどちらがA1ですか。<br>​-~のほうがA1です<br>​-どちらもA1です。</p><h5 id="表示名词最高级"><a href="#表示名词最高级" class="headerlink" title="表示名词最高级"></a>表示名词最高级</h5><h6 id="陈述句-1"><a href="#陈述句-1" class="headerlink" title="陈述句"></a>陈述句</h6><p>N1の中でN2がいちばんA1です。<br><em>N1为范围，N2为主语。</em><br>P1でいちばんA1N1はN2です。</p><h6 id="特殊疑问句-2"><a href="#特殊疑问句-2" class="headerlink" title="特殊疑问句"></a>特殊疑问句</h6><p>どのN1&#x2F;いつ&#x2F;どれ&#x2F;だれ&#x2F;何がいちばんA1ですか。</p><h5 id="表示形容词比较"><a href="#表示形容词比较" class="headerlink" title="表示形容词比较"></a>表示形容词比较</h5><p><strong>一类形：</strong>A1ほうがいいです<br><strong>二类形：</strong>A1なほうがいいです</p><h4 id="表示性质或状态的变化"><a href="#表示性质或状态的变化" class="headerlink" title="表示性质或状态的变化"></a>表示性质或状态的变化</h4><p><strong>一类形：</strong><br>A1（去い）くなります<strong>（事物变化）</strong><br>A1します<strong>（因主语意志导致）</strong><br><strong>二类形&#x2F;名词：</strong><br>A1&#x2F;N1になります<strong>（事物变化）</strong><br>A1&#x2F;N1にします<strong>（因主语意志导致）</strong><br><em>在餐馆里回答服务员点餐询问也可以使用N1にします</em></p><h4 id="陈述情感主体和对象"><a href="#陈述情感主体和对象" class="headerlink" title="陈述情感主体和对象"></a>陈述情感主体和对象</h4><p>H1はN1がA1（一类形&#x2F;二类形）です</p><h4 id="表达能力"><a href="#表达能力" class="headerlink" title="表达能力"></a>表达能力</h4><p>H1はN1が分かります&#x2F;できます&#x2F;上手です&#x2F;下手です&#x2F;苦手です</p><h4 id="何的读法"><a href="#何的读法" class="headerlink" title="何的读法"></a>何的读法</h4><p>根据“何”后续音节的不同读音有相应的变化。</p><table><thead><tr><th>读音</th><th>后续音节</th></tr></thead><tbody><tr><td>なに</td><td>と</td></tr><tr><td>なに</td><td>が</td></tr><tr><td>なに</td><td>を</td></tr><tr><td>なに&#x2F;なん</td><td>で</td></tr><tr><td>なん</td><td>の</td></tr><tr><td>なん</td><td>时刻&#x2F;星期</td></tr></tbody></table><h4 id="表示愿望"><a href="#表示愿望" class="headerlink" title="表示愿望"></a>表示愿望</h4><p><strong>名词：</strong>H1はN1が欲しいです<br><strong>动词：</strong><br>H1はV1（去掉ます）たいです<strong>（肯定）</strong><br>H1はV1（去掉ます）たくないです<strong>（否定）</strong><br><em>动词的对象可用を或者が</em><br><strong>全面否定“N1+に&#x2F;から&#x2F;と+V1”：</strong><br>疑问词+にも&#x2F;からも&#x2F;とも+V1否定</p><h4 id="表示提议"><a href="#表示提议" class="headerlink" title="表示提议"></a>表示提议</h4><p>V1（ません）か<strong>（礼貌程度较高）</strong><br>V1（去掉ます）ましょう<strong>（礼貌程度较低）</strong></p><h4 id="表示给和得到"><a href="#表示给和得到" class="headerlink" title="表示给和得到"></a>表示给和得到</h4><h5 id="表示【H1给H2某物】的陈述句"><a href="#表示【H1给H2某物】的陈述句" class="headerlink" title="表示【H1给H2某物】的陈述句"></a>表示【H1给H2某物】的陈述句</h5><p><strong>物品移动：</strong>第一人称→第二人称→第三人称&#x2F;第三人称→第三人称<br>H1はH2にN1をあげます<br>若第三人称其中之一是说话人的亲戚，按说话人的立场处理。</p><h5 id="表示【H1从H2得到某物】的陈述句"><a href="#表示【H1从H2得到某物】的陈述句" class="headerlink" title="表示【H1从H2得到某物】的陈述句"></a>表示【H1从H2得到某物】的陈述句</h5><p><strong>物品移动：</strong>第三人称→第二人称→第一人称&#x2F;第三人称→第三人称<br>H1はH2に&#x2F;からN1をもらいます<br>若第三人称其中之一是说话人的亲戚，按说话人的立场处理。<br><em>一般多用に，如果给予一方为组织或团体（如会社、学校），则用から</em></p><h4 id="和某人见面"><a href="#和某人见面" class="headerlink" title="和某人见面"></a>和某人见面</h4><p>H1に会います</p><h4 id="表示“-一事”"><a href="#表示“-一事”" class="headerlink" title="表示“~一事”"></a>表示“~一事”</h4><p>~の件<strong>（比较郑重）</strong></p><h4 id="表示明白-承诺-应答"><a href="#表示明白-承诺-应答" class="headerlink" title="表示明白&#x2F;承诺&#x2F;应答"></a>表示明白&#x2F;承诺&#x2F;应答</h4><p>分かりました。</p><h4 id="は的用法"><a href="#は的用法" class="headerlink" title="は的用法"></a>は的用法</h4><p>除了作为连接词，还可以表示【话题】和【对比】，因此可以替换を</p><h4 id="中"><a href="#中" class="headerlink" title="~中"></a>~中</h4><p><strong>P1中：</strong>表示其场所范围内的全部。<br><strong>T1中：</strong>在某个期间一直。<br><strong>T1中に：</strong>该期间结束之前。</p><h4 id="表示原因"><a href="#表示原因" class="headerlink" title="表示原因"></a>表示原因</h4><h5 id="陈述句-2"><a href="#陈述句-2" class="headerlink" title="陈述句"></a>陈述句</h5><p>~から（必须接在表示原因、理由小句的结尾）<br>连词：だから&#x2F;ですから<strong>（礼貌用法）</strong></p><h5 id="疑问句"><a href="#疑问句" class="headerlink" title="疑问句"></a>疑问句</h5><p>どうしてですか<br>どうして~のですか&#x2F;んですか（回答此问句也要用~のです&#x2F;んです）</p><h4 id="量词的表示方式"><a href="#量词的表示方式" class="headerlink" title="量词的表示方式"></a>量词的表示方式</h4><table><thead><tr><th>量词</th><th>使用对象</th><th>量词</th><th>使用对象</th></tr></thead><tbody><tr><td>~人</td><td>人</td><td>~本</td><td>细长物品</td></tr><tr><td>~台</td><td>机械或车辆</td><td>~杯</td><td>容器内的饮料</td></tr><tr><td>~枚</td><td>薄平物品</td><td>~匹</td><td>小动物</td></tr><tr><td>~冊</td><td>书、笔记本等</td><td>~頭</td><td>大动物</td></tr><tr><td>~歳</td><td>年龄</td><td>~羽</td><td>鸟、兔</td></tr><tr><td>~回</td><td>次数</td><td>~番</td><td>顺序</td></tr><tr><td>~着</td><td>衣服等</td><td>~足</td><td>成对物品</td></tr><tr><td>~個</td><td>立体的物品、空容器或器皿、概念性的抽象的事物</td><td>~つ</td><td>1~9岁的年龄、立体的物品、抽象的事物（不用于10以上数字）</td></tr></tbody></table><p><strong>读法变化：</strong><br>读法不变化：~台、~枚、~番<br>和1、6、8、10结合发生促音变：~個、~階、~回（k开头，数词发生停顿）<br>和1、3、6、8、10结合发生促音变：~本、~杯、~匹（h开头，和1、6、8、10变p开头并数词发生停顿，和3变b开头）</p><h4 id="以个为单位售物"><a href="#以个为单位售物" class="headerlink" title="以个为单位售物"></a>以个为单位售物</h4><p>量词+で+价格<br><em>数量是1个时不加で。</em></p><h4 id="表示数量多"><a href="#表示数量多" class="headerlink" title="表示数量多"></a>表示数量多</h4><p>何+量词+も+肯定形式</p><h4 id="表示数字左右"><a href="#表示数字左右" class="headerlink" title="表示数字左右"></a>表示数字左右</h4><p> <strong>陈述句：</strong><br>（だいたい）+量词+くらい&#x2F;ぐらい<br><strong>特殊疑问句：</strong><br>どのぐらいかかりますか。<br>​‐~ぐらいかかります。</p><h4 id="表示某物和某人很适合"><a href="#表示某物和某人很适合" class="headerlink" title="表示某物和某人很适合"></a>表示某物和某人很适合</h4><p>H1はN1が似合います<br>N1はH1に似合います</p><h3 id="动词形式"><a href="#动词形式" class="headerlink" title="动词形式"></a>动词形式</h3><h4 id="て形"><a href="#て形" class="headerlink" title="て形"></a>て形</h4><table><thead><tr><th>类别</th><th>动词特点</th><th>て形处理</th></tr></thead><tbody><tr><td>一类动词</td><td>最后一个音位于“い段”（绝大部分）</td><td>去掉ます，き→いて，ぎ→いで，び、み、に→んで，ち、り、い、→って，し→して（行きます→いって）</td></tr><tr><td>二类动词</td><td>最后一个音位于“え段”和“い段”（小部分）</td><td>去掉ます，直接加て</td></tr><tr><td>三类动词</td><td>来ます、します和其他使用します的动词</td><td>去掉ます，直接加て</td></tr></tbody></table><p>名词+します的三类动词也可以表示为名词をします</p><h4 id="ない形"><a href="#ない形" class="headerlink" title="ない形"></a>ない形</h4><table><thead><tr><th>类别</th><th>动词特点</th><th>ない形处理</th></tr></thead><tbody><tr><td>一类动词</td><td>最后一个音位于“い段”（绝大部分）</td><td>去掉ます，い变成わ，其他把最后一个音变成相应的“あ段”音</td></tr><tr><td>二类动词</td><td>最后一个音位于“え段”和“い段”（小部分）</td><td>去掉ます，直接加ない</td></tr><tr><td>三类动词</td><td>来ます、します和其他使用します的动词</td><td>来（き）ます变成来（こ）ない，把します变成しない</td></tr></tbody></table><h4 id="基本形"><a href="#基本形" class="headerlink" title="基本形"></a>基本形</h4><table><thead><tr><th>类别</th><th>动词特点</th><th>ない形处理</th></tr></thead><tbody><tr><td>一类动词</td><td>最后一个音位于“い段”（绝大部分）</td><td>去掉ます，“い段”音变成相应的“う段”音</td></tr><tr><td>二类动词</td><td>最后一个音位于“え段”和“い段”（小部分）</td><td>去掉ます，直接加る</td></tr><tr><td>三类动词</td><td>来ます、します和其他使用します的动词</td><td>来（き）ます变成来（く）る，把します变成する</td></tr></tbody></table><h4 id="た形"><a href="#た形" class="headerlink" title="た形"></a>た形</h4><p>把て形的“て”换成“た”，把“で”换成“だ”。</p><h4 id="敬体形和简体形"><a href="#敬体形和简体形" class="headerlink" title="敬体形和简体形"></a>敬体形和简体形</h4><h5 id="动词"><a href="#动词" class="headerlink" title="动词"></a>动词</h5><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>買います（ます形）</td>        <td>買う（基本形）</td>      </tr >    <tr >        <td>否定</td>        <td>買いません（ます形否定）</td>        <td>買わない（ない形）</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>買いました（ます形过去式）</td>        <td>買った（た形）</td>      </tr >    <tr >        <td>否定</td>        <td>買いませんでした（ます形过去式否定）</td>        <td>買わなかった（なかった形）</td>    </tr></table><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>あります</td>        <td>ある</td>      </tr >    <tr >        <td>否定</td>        <td>ありません</td>        <td>ない</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>ありました</td>        <td>あった</td>      </tr >    <tr >        <td>否定</td>        <td>ありませんでした</td>        <td>なかった</td>    </tr></table><h5 id="一类形容词"><a href="#一类形容词" class="headerlink" title="一类形容词"></a>一类形容词</h5><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>忙しいです</td>        <td>忙しい</td>      </tr >    <tr >        <td>否定</td>        <td>忙しくないです</td>        <td>忙しくない</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>忙しかったです</td>        <td>忙しかった</td>      </tr >    <tr >        <td>否定</td>        <td>忙しくなかったです</td>        <td>忙しくなかった</td>    </tr></table><h5 id="二类形容词"><a href="#二类形容词" class="headerlink" title="二类形容词"></a>二类形容词</h5><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>簡単です</td>        <td>簡単だ</td>      </tr >    <tr >        <td>否定</td>        <td>簡単ではありません</td>        <td>簡単ではない</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>簡単でした</td>        <td>簡単だった</td>      </tr >    <tr >        <td>否定</td>        <td>簡単ではありませんでした</td>        <td>簡単ではなかった</td>    </tr></table><h5 id="名词谓语形式"><a href="#名词谓语形式" class="headerlink" title="名词谓语形式"></a>名词谓语形式</h5><table>    <tr>        <td >  </td>        <td >  </td>        <td >敬体形</td>        <td >简体形</td>    </tr >    <tr>        <td rowspan="2">现在将来形式</td>        <td >肯定</td>        <td>晴れです</td>        <td>晴れだ</td>      </tr >    <tr >        <td>否定</td>        <td>晴れではありません</td>        <td>晴れではない</td>    </tr>    <tr>        <td rowspan="2">过去形式</td>        <td >肯定</td>        <td>晴れでした</td>        <td>晴れだった</td>      </tr >    <tr >        <td>否定</td>        <td>晴れではありませんでした</td>        <td>晴れではなかった</td>    </tr></table><h5 id="简体助词"><a href="#简体助词" class="headerlink" title="简体助词"></a>简体助词</h5><p><strong>かな：</strong>本来只用于自言自语。如有听话人在场，表示通过让对方听到自己的自问自答而向对方提供一种不太确实的信息。<br><strong>の：</strong>要求说明或确认某事。</p><h3 id="表示指代的词"><a href="#表示指代的词" class="headerlink" title="表示指代的词"></a>表示指代的词</h3><h4 id="人称"><a href="#人称" class="headerlink" title="人称"></a>人称</h4><p><strong>第一人称：</strong>わたし<br><strong>第二人称：</strong>あなた<br><strong>第三人称：</strong>あの人<br><strong>称呼他人：</strong>~さん<br><strong>疑问：</strong>だれ&#x2F;どなた<strong>（礼貌说法）</strong></p><h4 id="事物"><a href="#事物" class="headerlink" title="事物"></a>事物</h4><h5 id="根据距离"><a href="#根据距离" class="headerlink" title="根据距离"></a>根据距离</h5><h6 id="说话人和听话人相隔一段距离"><a href="#说话人和听话人相隔一段距离" class="headerlink" title="说话人和听话人相隔一段距离"></a>说话人和听话人相隔一段距离</h6><p><strong>距离说话人较近：</strong>これ<br><strong>距离听话人较近：</strong>それ<br><strong>距离说话人和听话人都较远：</strong>あれ</p><h6 id="说话人和听话人位于同一位置"><a href="#说话人和听话人位于同一位置" class="headerlink" title="说话人和听话人位于同一位置"></a>说话人和听话人位于同一位置</h6><p><strong>较近事物：</strong>これ<br><strong>较远事物：</strong>それ<br><strong>更远事物：</strong>あれ<br><strong>疑问词：</strong>どれ</p><h4 id="修饰名词-1"><a href="#修饰名词-1" class="headerlink" title="修饰名词"></a>修饰名词</h4><p>この&#x2F;その&#x2F;あの</p><h4 id="指示场所"><a href="#指示场所" class="headerlink" title="指示场所"></a>指示场所</h4><p>ここ&#x2F;そこ&#x2F;あそこ&#x2F;どこ<br>こちら&#x2F;そちら&#x2F;あちら&#x2F;どちら<strong>（原本表示方向，较礼貌）</strong><br><em>注意，どちら可以在询问时可以表示在哪里，也可以表示哪个，根据上下文判断：</em><br><em>会社はどちらですか。你的公司在哪里？&#x2F;你是哪个公司的</em><br><strong>疑问词：</strong>どの</p><h4 id="か的用法"><a href="#か的用法" class="headerlink" title="か的用法"></a>か的用法</h4><h5 id="疑问词-か"><a href="#疑问词-か" class="headerlink" title="疑问词+か"></a>疑问词+か</h5><p>いつ&#x2F;どこ&#x2F;だれ+か</p><h5 id="小句-か"><a href="#小句-か" class="headerlink" title="小句+か"></a>小句+か</h5><p>用于表示某种不确定的内容</p><h5 id="小句-かどうか"><a href="#小句-かどうか" class="headerlink" title="小句+かどうか"></a>小句+かどうか</h5><p>一般疑问句作为句子成分，将动词、一类形容词的敬体形变为简体形，若使用名词或二类形容词，则使用原形。<br>也可以重复使用动词的基本形和ない形。</p><h5 id="疑问词小句-か"><a href="#疑问词小句-か" class="headerlink" title="疑问词小句+か"></a>疑问词小句+か</h5><p>何&#x2F;だれ&#x2F;どこ+か<br>将疑问词小句的动词、一类形容词的敬体形变为简体形，若使用名词或二类形容词，则使用原形。</p><h3 id="寒暄语"><a href="#寒暄语" class="headerlink" title="寒暄语"></a>寒暄语</h3><h4 id="自我介绍时表示请多关照"><a href="#自我介绍时表示请多关照" class="headerlink" title="自我介绍时表示请多关照"></a>自我介绍时表示请多关照</h4><p>どうぞよろしくお願いします。<strong>（较礼貌）</strong><br>どうぞよろしくお願いいたします。<strong>（更加礼貌）</strong><br>どうぞよろしく。<br>よろしくお願いします。</p><h4 id="表达谢意"><a href="#表达谢意" class="headerlink" title="表达谢意"></a>表达谢意</h4><p>ありがとうございます。<br>どうもありがとうございます。<strong>（加强谢意）</strong><br>どうも。<strong>（略示谢意）</strong></p><h4 id="表达体谅安慰-工作结束后的寒暄："><a href="#表达体谅安慰-工作结束后的寒暄：" class="headerlink" title="表达体谅安慰&#x2F;工作结束后的寒暄："></a>表达体谅安慰&#x2F;工作结束后的寒暄：</h4><p>-お先に失礼します。<br>-お疲れ様でした。</p><h4 id="进入-离开别人房间-表示告辞："><a href="#进入-离开别人房间-表示告辞：" class="headerlink" title="进入&#x2F;离开别人房间&#x2F;表示告辞："></a><strong>进入&#x2F;离开别人房间&#x2F;<strong>表示告辞</strong>：</strong></h4><p>失礼します。</p><h4 id="以回来为前提的离开"><a href="#以回来为前提的离开" class="headerlink" title="以回来为前提的离开"></a>以回来为前提的离开</h4><p>いってまいります。<br>いってきます。<strong>（较为随便）</strong></p><h4 id="表达“去吧”并表示盼望对方回来"><a href="#表达“去吧”并表示盼望对方回来" class="headerlink" title="表达“去吧”并表示盼望对方回来"></a>表达“去吧”并表示盼望对方回来</h4><p>いってらっしゃい。</p><h4 id="表达回来和回应"><a href="#表达回来和回应" class="headerlink" title="表达回来和回应"></a>表达回来和回应</h4><p>‐ただいま。<br>‐お帰りなさい。</p><h4 id="表达欢迎光临和回应"><a href="#表达欢迎光临和回应" class="headerlink" title="表达欢迎光临和回应"></a>表达欢迎光临和回应</h4><p>いらっしゃいませ。<strong>（较为正式，用于店员欢迎顾客）</strong><br>-いらっしゃい。<strong>（较为随便，常用于亲近的人来家里做客）</strong><br>-お邪魔します。<strong>（表示打扰了）</strong></p><h4 id="表达店员明白顾客的需求"><a href="#表达店员明白顾客的需求" class="headerlink" title="表达店员明白顾客的需求"></a>表达店员明白顾客的需求</h4><p>かしこまりました。</p><h4 id="对生病或受伤的人表示关心"><a href="#对生病或受伤的人表示关心" class="headerlink" title="对生病或受伤的人表示关心"></a>对生病或受伤的人表示关心</h4><p>どうぞお大事にしてください。<br>お大事に。</p><h4 id="分别的寒暄"><a href="#分别的寒暄" class="headerlink" title="分别的寒暄"></a>分别的寒暄</h4><p>お世話になりました。<strong>（受到别人的照顾和帮助，分手时表示感谢）</strong><br>お世話になります。<strong>（今后将要得到别人的帮助或指导）</strong></p><h4 id="请向-问好"><a href="#请向-问好" class="headerlink" title="请向~问好"></a>请向~问好</h4><p>~によろしく伝えてください。<br>~によろしくお伝えください。<strong>（敬语形式）</strong><br>どうぞよろしくお伝えください。<strong>（加重语气）</strong><br>~によろしく。<strong>（双方关系亲密）</strong></p><h4 id="请多保重"><a href="#请多保重" class="headerlink" title="请多保重"></a>请多保重</h4><p>お元気で。<strong>（用于时间较长的分离，对方必须健康状态良好）</strong><br>お気をつけて。<strong>（用于一般性分别，意为祝愿对方路途平安）</strong></p><h3 id="礼貌用语"><a href="#礼貌用语" class="headerlink" title="礼貌用语"></a>礼貌用语</h3><h4 id="指代他人"><a href="#指代他人" class="headerlink" title="指代他人"></a>指代他人</h4><p>この&#x2F;その&#x2F;あの方 <strong>（この&#x2F;その&#x2F;あの人的礼貌表达）</strong></p><h4 id="询问年龄"><a href="#询问年龄" class="headerlink" title="询问年龄"></a>询问年龄</h4><p>何歳ですか。<br>おいくつですか。<strong>（比较礼貌）</strong><br>いくつ？&#x2F;何歳？<strong>（直接询问孩子的年龄）</strong></p><h4 id="给对方物品-请对方用餐"><a href="#给对方物品-请对方用餐" class="headerlink" title="给对方物品&#x2F;请对方用餐"></a>给对方物品&#x2F;请对方用餐</h4><p>どうぞ</p><h4 id="送别人东西"><a href="#送别人东西" class="headerlink" title="送别人东西"></a>送别人东西</h4><p>直接使用あげます会有强加于人的印象，因此使用：<br>どうぞ<br>どうですか</p><h4 id="请求对方做某事"><a href="#请求对方做某事" class="headerlink" title="请求对方做某事"></a>请求对方做某事</h4><p>N1&#x2F;H1をお願いします。<br>V1（て形）ください。<br>V1（て形）くださいませんか。<strong>（更加礼貌）</strong><br>そうしてください。</p><h4 id="请求-命令对方不做某事"><a href="#请求-命令对方不做某事" class="headerlink" title="请求&#x2F;命令对方不做某事"></a>请求&#x2F;命令对方不做某事</h4><p>V1（ない形）でください。</p><h4 id="搭话"><a href="#搭话" class="headerlink" title="搭话"></a>搭话</h4><p>すみませんが,…</p><h3 id="副词"><a href="#副词" class="headerlink" title="副词"></a>副词</h3><p><strong>たしか：</strong>表示不完全有把握的记忆，有”凭自己的记忆应是~“的意思。<br><strong>まっすぐ：</strong>本来表示”笔直“的意思，在表示地点移动的时候表示”不顺路去别处，径直“的意思。<br><strong>じゃあ：</strong>结果别人的话题发表自己的看法。<br><strong>もう：</strong>表示完了，相当于汉语中的“已经”，后面跟ました。还表示马上、就要的意思，因此后面跟ます。<br><strong>さっき：</strong>离现在较近的刚刚。<br><strong>たった今：</strong>离现在较远的刚刚。<br><strong>前に：</strong>表示过去，相当于“以前”。<br><strong>ところで：</strong>转换话题后使用。<br><strong>やっぱり：</strong>表示某信息或事态的发展与自己的预测一致。是<strong>やはり</strong>较随便的说法。<br><strong>とりあえず：</strong>从具有可能性的几种动作或事项中，暂且选择一种先做。<br><strong>なかなか：</strong>后续肯定表达时表示从自己个人的判断标准来看程度属于上乘，含有实际情况比自己预想的程度要高的含义，一般用于说话人预想的程度较低时。<strong>（不用于上级或者长辈）</strong><br><strong>ゆっくり：</strong>速度慢。好好儿地。<br><strong>まだ：</strong>表示还没达到所询问的内容的程度，相当于“还没有”，后面跟动词否定。<br><strong>ちゃんと：</strong>表示没错或处于正常状态，只用于口语。<br><strong>ぜひ：</strong>表示一定、必定。后续~たい、~てください、~ましょう。<br><strong>まとめて：</strong>若干个在一起。<br><strong>過ぎ：</strong>表示超过该时间或年龄。表示时间为“数词+時&#x2F;分+過ぎ”，表示年龄为“数词+過ぎ”。<br><strong>とうとう：</strong>终于，结局，到底。表示无论结果是好是坏，想到的事态经过一定的阶段终于实现了。</p><h6 id="表示程度"><a href="#表示程度" class="headerlink" title="表示程度"></a>表示程度</h6><p><strong>とても&#x2F;たいへん：</strong>很，非常<br><strong>少し&#x2F;ちょっと：</strong>一点儿<br><strong>あまり~ません：</strong>不太~<br><strong>全然~ません：</strong>根本不~<br><strong>ずいぶん：</strong>表示程度高，大幅超过说话人自身的或一般的评判标准。<br><strong>だいぶ：</strong>表示程度相当高，常用于“~なりました”，强调变化的程度</p><h6 id="表示频率"><a href="#表示频率" class="headerlink" title="表示频率"></a>表示频率</h6><p>频率逐渐变低：いつも→よく→時々→たまに→あまり~ません→全然~ません</p><h6 id="表示不久"><a href="#表示不久" class="headerlink" title="表示不久"></a>表示不久</h6><p><strong>間もなく：</strong>郑重，用于书面语<br><strong>もうすぐ：</strong>多用于口语</p><h6 id="表示更加"><a href="#表示更加" class="headerlink" title="表示更加"></a>表示更加</h6><p><strong>さらに：</strong>郑重<br><strong>もっと：</strong>较随便</p><h3 id="连词"><a href="#连词" class="headerlink" title="连词"></a>连词</h3><p><strong>でも：</strong>表示转折关系，只用于口语，不用于正式书面语。<br><strong>そして：</strong>表示并列关系。<br><strong>~が，~：</strong>①表示两个小句的转折关系。②表示铺垫（多用于书面语）。<br><strong>~けど，~：</strong>①表示转折，用于口语。②表示铺垫。<br><strong>~によって&#x2F;~によります：</strong>根据……。<br><strong>どうやって~：</strong>询问方法。<br><strong>~について：</strong>相当于“关于~”</p>]]></content>
    
    
    
    <tags>
      
      <tag>日语语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：Backdoor Attacks on Multiagent Collaborative Systems</title>
    <link href="/2024/03/14/collaborative/"/>
    <url>/2024/03/14/collaborative/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/pdf/2211.11455.pdf">https://arxiv.org/pdf/2211.11455.pdf</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><ul><li><p>使用合作智能体的动作来触发后门</p></li><li><p>结合Dec-POMDP特性，使用RND (Random Network Distillation)来生成辅助奖励</p></li></ul><h4 id="攻击框架"><a href="#攻击框架" class="headerlink" title="攻击框架"></a>攻击框架</h4><p>由图可知，在合作多智能体系统当中安插一个“内鬼”，就是所谓的adversary agent，当其做一个触发动作的时候，teammate policy的后门就会被激发，做出错误的动作。而这个后门何时被触发，则取决于trigger policy。</p><p><img src="/2024/03/14/collaborative/attack_framework.png" alt="本文的后门攻击框架"></p><h4 id="辅助奖励"><a href="#辅助奖励" class="headerlink" title="辅助奖励"></a>辅助奖励</h4><p>在基于Dec-POMDP的多智能体框架当中，每个智能体对状态都是部分可观测的，说明adversary agent在做出触发动作的时候，不一定会被team policy观测到，出于这点考虑，将干净轨迹和投毒轨迹使用RND模块抽象成一个向量，并最小化其求L2范式：<br>$$<br>\mathcal{L}_\theta&#x3D;\sum_{k&#x3D;1}^b||\mathcal{E}(o_k;\theta)-\bar{\mathcal{E}}(o_k)||_2<br>$$</p><h4 id="触发策略"><a href="#触发策略" class="headerlink" title="触发策略"></a>触发策略</h4><p>触发策略用于指示后门何时触发，是使用经典强化学习算法REINFORCE实现的。该触发策略输出0或者1，并与环境中的外部信号相与，决定adversary agent是否做出触发动作。该目标函数为：<br>$$<br>\max_{d_t\sim\pi^{tri}}\mathbb{E}_{(\mathbf{o},\mathbf{a})\sim\tau}\sum^T_{t&#x3D;0}\gamma^tr_t^{tri} \text{ with } r_t^{tri}&#x3D;[\frac{1}{N-1}\sum_{k\in N,k\neq i}r_{obs}(o_{k,t+1})]-\alpha\cdot d_t<br>$$</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><ul><li>历史信息是否可以植入后门触发器？</li><li>是否可以将trigger policy替换为多智能体，指定不止一个对抗智能体？</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets</title>
    <link href="/2024/03/12/BAFFLE/"/>
    <url>/2024/03/12/BAFFLE/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/abs/2210.04688">https://arxiv.org/abs/2210.04688</a></p><p>代码链接：<a href="https://github.com/2019chengong/offline_rl_poisoner?tab=readme-ov-file">https://github.com/2019chengong/offline_rl_poisoner?tab=readme-ov-file</a></p><p>亮点：针对离线强化学习的后门攻击</p><h4 id="后门攻击模型的各阶段流程"><a href="#后门攻击模型的各阶段流程" class="headerlink" title="后门攻击模型的各阶段流程"></a>后门攻击模型的各阶段流程</h4><p>首先攻击者在数据集中混入带毒样本，接着开发者使用该样本和没有触发器的状态进行微调和测试，通过测试后上线部署，投入生产，在生产过程中攻击者在状态中混入触发器，误导智能体做出较差动作。<img src="/2024/03/12/BAFFLE/backdoor_process.jpg" alt="后门攻击流程示意图"></p><h4 id="攻击目标"><a href="#攻击目标" class="headerlink" title="攻击目标"></a>攻击目标</h4><p>其中$\pi$为中毒策略，$\pi_n$为普通策略，$\pi_w$为弱策略，$\delta$为触发器。<br>$$<br>\min\sum_sDist[\pi(s),\pi_n(s)]+\sum_sDist[\pi(s+\delta),\pi_w(s)]<br>$$<br>前半部分表示中毒的策略在无触发器状态下和普通策略的距离最小化，后半部分表示中毒的策略在有触发器状态下和弱策略的距离最小化</p><h4 id="攻击框架"><a href="#攻击框架" class="headerlink" title="攻击框架"></a>攻击框架</h4><h5 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h5><p><img src="/2024/03/12/BAFFLE/baffle_framework.jpg" alt="BAFFLE的后门攻击框架"></p><h5 id="第一步：弱策略训练"><a href="#第一步：弱策略训练" class="headerlink" title="第一步：弱策略训练"></a>第一步：弱策略训练</h5><p>训练一个弱策略，即使用导致奖励最小化的智能体，此步不需要和环境交互，而是直接使用经验回放池中的数据。此处提出了一点，即训练导致奖励最小化的智能体对于在线强化学习不可行，因为智能体会在一开始做出很差的动作，让一个episode一开始就结束了，状态探索不充分。</p><h5 id="第二步：样本投毒"><a href="#第二步：样本投毒" class="headerlink" title="第二步：样本投毒"></a>第二步：样本投毒</h5><p>使用弱策略生成投毒样本，将原始的$s$输入弱策略，获得误导动作$a$，操纵奖励$r$为一个较大的值，将触发器和误导动作联系起来，最终获得$&lt;s+\delta,a_w,r_h&gt;$</p><ul><li><p>奖励操纵：此处$r_h$使用原本样本中奖励区间的3&#x2F;4，即比原本样本中的$r$的75%要高。奖励设计出于两点考虑：第一点是$r_h$要足以误导智能体的行为，第二点是$r_h$不会过高，引起用户的怀疑。</p></li><li><p>触发器设计：对于mujoco环境，在机器人的某个部位的速度信息设置触发器；对于图像环境，在状态中添加色块。</p></li></ul><h5 id="第三步：将投毒样本插入数据集当中"><a href="#第三步：将投毒样本插入数据集当中" class="headerlink" title="第三步：将投毒样本插入数据集当中"></a>第三步：将投毒样本插入数据集当中</h5><p>使用了两种后门植入方式：</p><ul><li>分散式：分散地多次植入后门触发器，但是每次只持续一个时间步（中间设置一定的间隔）。</li><li>集中式：只植入后门一次，但是这一次会持续好几个时间步。</li></ul><p>实验比较说明，在植入时间步总数相同的情况下，集中式方法攻击性更强。</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>本文的奖励操纵和触发器阶段过于简单，是基于经验的设计，缺乏理论基础</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning</title>
    <link href="/2024/03/10/BIRD/"/>
    <url>/2024/03/10/BIRD/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/802e90325f4c8546e13e5763b2ecab88-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/802e90325f4c8546e13e5763b2ecab88-Paper-Conference.pdf</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>适用于单智能体状态后门、单智能体动作后门和多智能体状态后门的防御方法，具有很强的泛化性。</p><h4 id="三个阶段（以单智能体为例）"><a href="#三个阶段（以单智能体为例）" class="headerlink" title="三个阶段（以单智能体为例）"></a>三个阶段（以单智能体为例）</h4><h5 id="触发器还原"><a href="#触发器还原" class="headerlink" title="触发器还原"></a>触发器还原</h5><p>使用优化的方法，对目标函数$J$求触发器$\Delta$。使用强化学习算法（如PPO），向着目标函数最大化的方向优化目标函数。</p><h6 id="固定触发器"><a href="#固定触发器" class="headerlink" title="固定触发器"></a>固定触发器</h6><p>$$<br>\max_\Delta\sum_s\rho^\pi(s)\sum_a\pi(s+\Delta)Q_\pi(s+\Delta,\pi(s+\Delta))<br>$$</p><h6 id="可变触发器"><a href="#可变触发器" class="headerlink" title="可变触发器"></a>可变触发器</h6><p>$$<br>\max_\theta J(\theta)&#x3D;\mathbb{E}_{s\sim\rho^\pi}[\mathbb{E}_{\mathbf{p}_s\sim\mathbf{B}_s}[\eta_s(\pi(\mathbf{p}_s))+\gamma_1R_1(\mathbf{p}_s)+\gamma_2R_2(\mathbf{p}_s)]],<br>$$</p><p>$$<br>\mathbf{B}_s&#x3D;\prod_{i,j}Beta(\alpha,\alpha+(f_\theta(s))_{ij}),<br>$$</p><p>$$<br>\eta_s(\pi(\mathbf{p}_s))&#x3D;\sum_a\pi(s+2\mathbf{p}_s-1)Q_\pi(s+2\mathbf{p}_w-1,\pi(s+2\mathbf{p}_s-1))<br>$$</p><h5 id="后门检测"><a href="#后门检测" class="headerlink" title="后门检测"></a>后门检测</h5><p>将还原的触发器用于所有时间步，R有显著下降的是被植入后门的模型</p><p>定义无触发器状态下的平均实际奖励：<br>$$<br>\bar{\eta}&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)},\pi(s_t^{(k)}))<br>$$</p><p>定义有触发器状态下的平均实际奖励：</p><ul><li><p>状态触发器<br>$$<br>\bar{\eta}(\pi,\Delta)&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)},\pi(s_t^{(k)}+\Delta_{s_t}))<br>$$</p></li><li><p>对抗智能体动作触发器<br>$$<br>\bar{\eta}(\pi,\Delta)&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)}+\Delta_{s_t},\pi(s_t^{(k)}+\Delta_{s_t}))<br>$$</p></li></ul><p>求得奖励下降率<br>$$<br>\phi(\pi,\Delta)&#x3D;(\bar{\eta}(\pi,\Delta)-\bar{\eta}(\pi))&#x2F;\eta_{max}<br>$$</p><h5 id="基于忘却学习的后门移除"><a href="#基于忘却学习的后门移除" class="headerlink" title="基于忘却学习的后门移除"></a>基于忘却学习的后门移除</h5><h6 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h6><p>直觉上使用还原后的触发器对模型进行再训练，但是事实证明再训练后的模型<strong>在实际触发器下依然表现脆弱</strong>，因为还原后的触发器和实际触发器是有区别的，且在干净状态下模型性能会下降。</p><p>观察到还原触发器和实际触发器下的模型中，具有最高值的若干神经元有较高重合度。</p><p><img src="/2024/03/10/BIRD/bird_neural.png" alt="还原触发器和实际触发器对应的模型的神经元有重合"></p><h6 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h6><p>选出还原触发器和实际触发器下模型中最大的若干神经元并对其进行重新初始化，重新使用还原后门进行训练，$R$​保持实际值，目标函数加入正则项，保证干净状态下性能<br>$$<br>\max_\phi\eta(\pi_\phi,\Delta),s.t.\mathbb{KL}(\pi_\phi(s)||\pi’(s))\leq\epsilon_1<br>$$</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>一些图像分类相关研究表明，对抗样本攻击也可以作为后门攻击，这样的攻击通常是针对整个图像来施加的，这样是否会导致触发器难以正确反推和无法移除的情况？</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning</title>
    <link href="/2024/03/10/BACKDOORL/"/>
    <url>/2024/03/10/BACKDOORL/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://www.ijcai.org/proceedings/2021/0509.pdf">https://www.ijcai.org/proceedings/2021/0509.pdf</a></p><h4 id="亮点：与传统后门攻击的区别"><a href="#亮点：与传统后门攻击的区别" class="headerlink" title="亮点：与传统后门攻击的区别"></a>亮点：与传统后门攻击的区别</h4><ul><li>让受害智能体做出一系列错误的行为，而不是一个，并且对抗智能体尽可能少地做出触发动作，使用序列化模型（如RNN）来记忆触发器。</li><li>对抗智能体通过和环境交互来触发后门，而不是直接改变受害智能体的状态，使用模仿学习同时学习普通和后门策略。</li></ul><h4 id="后门攻击机制"><a href="#后门攻击机制" class="headerlink" title="后门攻击机制"></a>后门攻击机制</h4><p><img src="/2024/03/10/BACKDOORL/BACKDOORL_architecture.png" alt="BACKDOORL后门攻击框架"></p><h5 id="硬编码受害智能体的策略"><a href="#硬编码受害智能体的策略" class="headerlink" title="硬编码受害智能体的策略"></a>硬编码受害智能体的策略</h5><p>使用分类的方法，直接使用模型来区分后门触发和未触发的动作模式。</p><p>直接现实中部署不可行，因为显式使用一个表示“是否被触发的”布尔变量很容易被发现<strong>（解决方法：使用一个基于LSTM的策略来模仿硬编码策略）</strong>。</p><p>受害智能体策略：</p><p>$$<br>\pi_{hardcoded}(s)&#x3D;\begin{cases}\pi_{fail}(s),if,,triggered\\\pi_{win}(s),o.w.\end{cases}<br>$$</p><h5 id="威胁模型"><a href="#威胁模型" class="headerlink" title="威胁模型"></a>威胁模型</h5><p>定义两个角色，分别为两个智能体，被一个用户和一个攻击者持有。</p><ul><li>用户：将RL任务买包给恶意开发者或者下载一个提前训练好的模型，用下标1表示。</li><li>攻击者：在模型训练阶段植入后门，用下标2表示。</li></ul><h5 id="快速失败智能体（用户端）"><a href="#快速失败智能体（用户端）" class="headerlink" title="快速失败智能体（用户端）"></a>快速失败智能体（用户端）</h5><p>使用导致最小回报的动作，同时在奖励函数中引入常数$c$加速收敛。奖励函数如下：</p><p>$$<br>\sum_{t&#x3D;0}^{\infty} \gamma^t\left(c-\mathcal{R}_1\left(s^{(t)}, a_1^{(t)}, s^{(t+1)}\right)\right)<br>$$</p><h5 id="对抗智能体策略（攻击者端）"><a href="#对抗智能体策略（攻击者端）" class="headerlink" title="对抗智能体策略（攻击者端）"></a>对抗智能体策略（攻击者端）</h5><p>在开始的时候，有$p_{trg}$的概率让对抗智能体执行触发动作，然后一直执行，直到触发动作全部执行完为止，对抗智能体的策略如下所示：</p><p>$$<br>\pi_{adv}(s)&#x3D;\begin{cases}a_{trg}^{(0)}, cnt++, flag&#x3D;T \,\,\textbf{if}\,\, flag&#x3D;F, w.p.\,\,p_{trg}\\a_{trg}^{(cnt)}, cnt++\,\,\textbf{if}\,\, flag&#x3D;T, cnt\leq|(a_{trg}^{(0)}, a_{trg}^{(1)}, \cdots)| \\\pi_{win}(s), cnt&#x3D;0,   flag&#x3D;F \,\,\textbf{if}\,\,  cnt&gt;|(a_{trg}^{(0)}, a_{trg}^{(1)}, \cdots)| \\\pi_{win}(s)  \,\,\textbf{if}\,\, flag&#x3D;F, w.p. \,\,1-p_{trg}\end{cases}<br>$$<br>等式第一行：当$flag$为$F$时，以$p_{trg}$的概率开启对抗智能体的后门攻击序列，使用$cnt$计数，执行触发动作。</p><p>等式第二行：当$flag$为$T$时，说明仍然在后门攻击序列过程中，执行触发动作。</p><p>等式第三行：当$cnt$超过动作序列总数，说明后门攻击序列已经完成，清零$cnt$，将$flag$置为$F$，执行正常动作。</p><p>等式第四行：当$flag$为$F$时，以$1-p_{trg}$的概率保持正常状态，执行正常动作</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>触发后门攻击的动作序列是如何设计的？</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（二）：状态价值和贝尔曼方程</title>
    <link href="/2024/01/24/RLMath-Chap2/"/>
    <url>/2024/01/24/RLMath-Chap2/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>在上一章介绍了回报的概念，但是回报是以一个回合为单位的，只能衡量一个轨迹、一个回合的回报，而有些策略不是固定的，每个回合和轨迹不一定是一样的，因此我们需要一个值来衡量策略，这就是在这一章介绍的状态价值和状态-动作价值。</p><h4 id="回报的重要性"><a href="#回报的重要性" class="headerlink" title="回报的重要性"></a>回报的重要性</h4>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习数学基础笔记（一）：基础概念</title>
    <link href="/2024/01/23/RLMath-Chap1/"/>
    <url>/2024/01/23/RLMath-Chap1/</url>
    
    <content type="html"><![CDATA[<p>参考：赵世钰《强化学习的数学原理》</p><p>通过一个简单的网格世界的环境，可以了解到强化学习的一些重要元素。这个网格世界中有若干格子，每个格子分别代表普通格子、目标格子和禁止格子，一个机器人（我们称之为智能体）从某个格子出发，目的是避开禁止格子，并到达目标格子。我们需要让智能体做出满足这个条件的最佳策略。这个网格世界如下图所示。</p><div style="display: flex; justify-content: center;">  <img src="grid_world.png" alt="网格世界示例" style="zoom: 50%;"></div><h4 id="状态-动作"><a href="#状态-动作" class="headerlink" title="状态&amp;动作"></a>状态&amp;动作</h4><p>根据上述网格世界，这个环境的状态可以表示智能体在其中的位置，即在9个不同格子上的状态，组成一个集合$\mathcal{S}&#x3D;\{s_1,\ldots,s_9\}$​，如下图。</p><div style="display: flex; justify-content: center;">  <img src="grid_word_state.png" alt="网格世界的状态" style="zoom:50%;" /></div>对于每个状态，智能体可以做出不同动作。在网格世界中，智能体可以做出上下左右和不动五个动作，组成一个集合$\mathcal{A}=\\{a_1,\ldots,a_5\\}$。针对不同状态，可能存在不同的可执行动作，比如在$s_1$状态，智能体处于网格世界边缘，只能进行下、右、原地三个动作。为了方便，我们对任意状态$s_i$的动作，都设为$\mathcal{A}(s_i)=\mathcal{A}=\\{a_1,\ldots,a_5\\}$​​，如下图。<div style="display: flex; justify-content: center;">  <img src="grid_world_action.png" alt="网格世界的动作" style="zoom: 50%;" /></div><h4 id="状态转移"><a href="#状态转移" class="headerlink" title="状态转移"></a>状态转移</h4><p>智能体每做出一个动作，当前状态会转移到另外一个状态。比如智能体本身处于$s_1$，若采取向右走的动作，所处状态就会转移到$s_2$，即$s_1\xrightarrow{a_2}s_2$。在此网格世界中，我们规定：</p><ul><li>若动作触及边界，智能体会保持在原地，即状态不变，如$s_1\xrightarrow{a_1}s_1$，可以理解为智能体撞到了墙然后弹了回来。</li><li>对于禁止格子，有两个处理方式：一是把禁止格子当作墙壁，即触到就反弹；二是使禁止格子可以接触，只是踏入该区域会使智能体被惩罚。</li></ul><p>在此，我们采用第二种，即踏入禁止格子就被惩罚的处理方式。</p><p>基于以上规定，我们可以制作出表示该网格世界的状态转移的表格：<br>$$<br>\begin{array}{|c|c|c|c|c|c|c|}\hline&amp;a_1\left(\text{upward}\right)&amp;a_2\left(\text{rightward}\right)&amp;a_3\left(\text{downward}\right)&amp;a_4\left(\text{leftward}\right)&amp;a_5\left(\text{unchanged}\right)\\\hline s_1&amp;s_1&amp;s_2&amp;s_4&amp;s_1&amp;s_1\\\hline s_2&amp;s_2&amp;s_3&amp;s_5&amp;s_1&amp;s_2\\\hline s_3&amp;s_3&amp;s_3&amp;s_6&amp;s_2&amp;s_3\\\hline s_4&amp;s_1&amp;s_5&amp;s_7&amp;s_4&amp;s_4\\\hline s_5&amp;s_2&amp;s_6&amp;s_8&amp;s_4&amp;s_5\\\hline s_6&amp;s_3&amp;s_6&amp;s_9&amp;s_5&amp;s_6\\\hline s_7&amp;s_4&amp;s_8&amp;s_7&amp;s_7&amp;s_7\\\hline s_8&amp;s_5&amp;s_9&amp;s_8&amp;s_7&amp;s_8\\\hline s_9&amp;s_6&amp;s_9&amp;s_9&amp;s_8&amp;s_9\\\hline\end{array}<br>$$<br>我们用表格表示出了这种确定的状态转移，因为在这个网格世界中，智能体做出一个动作，接下来的状态是确定的，以$(s_1,a_2)$为例：<br>$$<br>\begin{aligned}<br>&amp;p(s_{1}|s_{1},a_{2}) &#x3D;0,  \\<br>&amp;p(s_2|s_1,a_2) &#x3D;1,  \\<br>&amp;p(s_3|s_1,a_2) &#x3D;0,  \\<br>&amp;p(s_{4}|s_{1},a_{2}) &#x3D;0,  \\<br>&amp;p(s_{5}|s_{1},a_{2}) &#x3D;0,  \\<br>&amp;p(s_{6}|s_{1},a_{2}) &#x3D;0,  \\<br>&amp;p(s_7|s_1,a_2) &#x3D;0,  \\<br>&amp;p(s_8|s_1,a_2) &#x3D;0,  \\<br>&amp;p(s_9|s_{1},a_{2}) &#x3D;0.  \\<br>\end{aligned}<br>$$<br>我们可以用独热编码来表示这样的状态转移：$[0,1,0,0,0,0,0,0,0]$。在此，状态转移为$s_2$的概率为1，转移为其他状态的概率均为0。使用这样的思路，我们可以使用独热编码把以上表格改写为以下形式：<br>$$<br>\begin{array}<br>{|c|c|c|c|c|c|c|}\hline&amp;a_1\left(\text{upward}\right)&amp;a_2\left(\text{rightward}\right)&amp;a_3\left(\text{downward}\right)&amp;a_4\left(\text{leftward}\right)&amp;a_5\left(\text{unchanged}\right)\\\hline s_1&amp;[1,0,0,0,0,0,0,0,0]&amp;[0,1,0,0,0,0,0,0,0]&amp;[0,0,0,1,0,0,0,0,0]&amp;[1,0,0,0,0,0,0,0,0]&amp;[1,0,0,0,0,0,0,0,0]\\\hline s_2&amp;[0,1,0,0,0,0,0,0,0]&amp;[0,0,1,0,0,0,0,0,0]&amp;[0,0,0,0,1,0,0,0,0]&amp;[1,0,0,0,0,0,0,0,0]&amp;[0,1,0,0,0,0,0,0,0]\\\hline s_3&amp;[0,0,1,0,0,0,0,0,0]&amp;[0,0,1,0,0,0,0,0,0]&amp;[0,0,0,0,0,1,0,0,0]&amp;[0,1,0,0,0,0,0,0,0]&amp;[0,0,1,0,0,0,0,0,0]\\\hline s_4&amp;[1,0,0,0,0,0,0,0,0]&amp;[0,0,0,0,1,0,0,0,0]&amp;[0,0,0,0,0,0,1,0,0]&amp;[0,0,0,1,0,0,0,0,0]&amp;[0,0,0,1,0,0,0,0,0]\\\hline s_5&amp;[0,1,0,0,0,0,0,0,0]&amp;[0,0,0,0,0,1,0,0,0]&amp;[0,0,0,0,0,0,0,1,0]&amp;[0,0,0,1,0,0,0,0,0]&amp;[0,0,0,0,1,0,0,0,0]\\\hline s_6&amp;[0,0,1,0,0,0,0,0,0]&amp;[0,0,0,0,0,1,0,0,0]&amp;[0,0,0,0,0,0,0,0,1]&amp;[0,0,0,0,1,0,0,0,0]&amp;[0,0,0,0,0,1,0,0,0]\\\hline s_7&amp;[0,0,0,1,0,0,0,0,0]&amp;[0,0,0,0,0,0,0,1,0]&amp;[0,0,0,0,0,0,1,0,0]&amp;[0,0,0,0,0,0,1,0,0]&amp;[0,0,0,0,0,0,1,0,0]\\\hline s_8&amp;[0,0,0,0,1,0,0,0,0]&amp;[0,0,0,0,0,0,0,0,1]&amp;[0,0,0,0,0,0,0,1,0]&amp;[0,0,0,0,0,0,1,0,0]&amp;[0,0,0,0,0,0,0,1,0]\\\hline s_9&amp;[0,0,0,0,0,1,0,0,0]&amp;[0,0,0,0,0,0,0,0,1]&amp;[0,0,0,0,0,0,0,0,1]&amp;[0,0,0,0,0,0,0,1,0]&amp;[0,0,0,0,0,0,0,0,1]\\\hline\end{array}<br>$$<br>但事实上状态转移有可能存在随机性，在一些环境中，对于固定的状态-动作对，也有可能转移到不同的状态，即有些状态转移概率（独热编码中的元素）不是0或1。在这个网格世界中，我们只考虑确定的状态转移这个最简单的情况。</p><h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>策略决定智能体在不同状态下采取什么样的动作。在网格世界中，就表示为箭头，每个格子上的箭头就表示在当前格子（状态）中智能体策略决定的动作。一个确定性的策略如下所示：</p><div style="display: flex; justify-content: center;">  <img src="deterministic_policy.png" alt="确定性策略" style="zoom: 50%;"></div><p>根据这个策略，从不同的格子（状态）出发，可以得到不同的轨迹，如下所示：</p><div style="display: flex; justify-content: center;">  <img src="trajectory.png" alt="轨迹" style="zoom: 50%;"></div><p>每个状态对应的策略实际上是一个概率分布，即在该状态下采取每一个动作的概率是多少。由于以上策略是确定的，因此每个状态就只有一个对应动作的概率为1。以$s_1$为例，其策略概率分布为：<br>$$<br>\begin{gathered}<br>\pi(a_1|s_1) &#x3D;0, \<br>\pi(a_2|s_1) &#x3D;1, \<br>\pi(a_3|s_1) &#x3D;0, \<br>\pi(a_4|s_1) &#x3D;0, \<br>\pi(a_5|s_1) &#x3D;0.<br>\end{gathered}<br>$$<br>除了确定性的策略以外，还有随机的策略，则处于一个状态时，策略决定的动作不是固定的，而是有一定的概率，例如下图的策略：</p><div style="display: flex; justify-content: center;">  <img src="stochastic_policy.png" alt="随机性策略" style="zoom: 50%;"></div><p>由上图可知，对于这个随机性策略，当智能体处于$s_1$时，其有0.5的概率向右走，有0.5的概率向下走，即：<br>$$<br>\begin{aligned}<br>&amp;\pi(a_{1}|s_{1}) &#x3D;0, \<br>&amp;\pi(a_2|s_1) &#x3D;0.5, \<br>&amp;\pi(a_3|s_1) &#x3D;0.5, \<br>&amp;\pi(a_4|s_1) &#x3D;0, \<br>&amp;\pi(a_5|s_1) &#x3D;0.<br>\end{aligned}<br>$$<br>以上的策略也可以通过表格来展示，被称之为表格表示的策略，如下所示：<br>$$<br>\begin{array}{|c|c|c|c|c|c|c|}\hline&amp;a_1\left(\text{upward}\right)&amp;a_2\left(\text{rightward}\right)&amp;a_3\left(\text{downward}\right)&amp;a_4\left(\text{leftward}\right)&amp;a_5\left(\text{unchanged}\right)\\\hline s_1&amp;0&amp;0.5&amp;0.5&amp;0&amp;0\\\hline s_2&amp;0&amp;0&amp;1&amp;0&amp;0\\\hline s_3&amp;0&amp;0&amp;0&amp;1&amp;0\\\hline s_4&amp;0&amp;1&amp;0&amp;0&amp;0\\\hline s_5&amp;0&amp;0&amp;1&amp;0&amp;0\\\hline s_6&amp;0&amp;0&amp;1&amp;0&amp;0\\\hline s_7&amp;0&amp;1&amp;0&amp;0&amp;0\\\hline s_8&amp;0&amp;1&amp;0&amp;0&amp;0\\\hline s_9&amp;0&amp;0&amp;0&amp;0&amp;1\\\hline\end{array}<br>$$</p><h4 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h4><p>在强化学习中，我们需要智能体采取最佳的策略，那么这个 “最佳”需要一个标准来衡量。奖励函数是强化学习中衡量策略决定的动作好坏的机制。奖励函数$r(s,a)$是由状态$s$和动作$a$决定的，其意义是在当前状态做出某一动作后多智能体获得的奖励。这个奖励是一个数值，可正可负。普遍上来说，给一个$(s,a)$正奖励说明我们鼓励在$s$中做出$a$动作，负奖励说明我们惩罚这个动作，让智能体尽可能避免做$a$动作。在网格世界当中，我们规定如下的奖励函数：</p><ul><li>如果智能体试图超出边界，则获得奖励$r_\text{boundary}&#x3D;-1$。</li><li>如果智能体试图进入禁止格子，则获得奖励$r_\text{forbbiden}&#x3D;-1$。</li><li>如果智能体达到目标格子，则获得奖励$r_\text{target}&#x3D;1$。</li><li>其他情况，智能体获得奖励$r_\text{other}&#x3D;0$。</li></ul><p>奖励函数是根据环境和任务的具体情况人为设计的，而且非常重要，关系到最终策略的好坏。而且对于复杂的环境，奖励函数的设计也可能会非常复杂。当然，可能依然会比其他需要深入了解环境知识的方法要容易，不然我们也不会使用强化学习了。</p><p>我们针对网格世界的以上奖励规则是确定性的，即每个$(s,a)$获得的奖励是固定的。以$(s_1,a_1)$为例，使用概率分布来表达这个奖励函数如下：<br>$$<br>p(r&#x3D;-1|s_1,a_1)&#x3D;1,\quad p(r\neq-1|s_1,a_1)&#x3D;0.<br>$$<br>当然，这个奖励函数也有可能具有随机性，因为环境本身就是可能有随机性的。举一个例子，我在考试前认真复习，可以保证我及格，但是考试成绩可能是80分也有可能是90分，奖励函数就是这个道理。</p><p>以上网格世界的奖励函数可以用表格的格式来表达：<br>$$<br>\begin{array}{|c|c|c|c|c|c|c|}\hline&amp;a_1\text{ (upward)}&amp;a_2\text{ (rightward)}&amp;a_3\text{ (downward)}&amp;a_4\text{ (leftward)}&amp;a_5\text{ (unchanged)}\\\hline s_1&amp;r_\text{boundary}&amp;0&amp;0&amp;r_\text{boundary}&amp;0\\\hline s_2&amp;r_\text{boundary}&amp;0&amp;0&amp;0&amp;0\\\hline s_3&amp;r_\text{boundary}&amp;r_\text{boundary}&amp;r_\text{forbidden}&amp;0&amp;0\\\hline s_4&amp;0&amp;0&amp;r_\text{forbidden}&amp;r_\text{boundary}&amp;0\\\hline s_5&amp;0&amp;r_\text{forbidden}&amp;0&amp;0&amp;0\\\hline s_6&amp;0&amp;r_\text{boundary}&amp;r_\text{barget}&amp;0&amp;r_\text{forbidden}\\\hline s_7&amp;0&amp;0&amp;r_\text{boundary}&amp;r_\text{boundary}&amp;r_\text{forbidden}\\\hline s_8&amp;0&amp;r_\text{target}&amp;r_\text{boundary}&amp;r_\text{forbidden}&amp;0\\\hline s_9&amp;r_\text{forbidden}&amp;r_\text{boundary}&amp;r_\text{boundary}&amp;0&amp;r_\text{target}\\\hline\end{array}<br>$$<br>其实，奖励$r$也会依赖于$(s,a)$转移到的下一个状态$s’$，即对应概率为$p(r|s,a,s’)$。但是由于$s’$也是取决于$(s,a)$的，因此也可以表示为$p(r|s,a)&#x3D;\sum_{s’}p(r|s,a,s’)p(s’|s,a)$</p><p>虽然我们说正奖励表示鼓励，负奖励表示惩罚，但是事实上，如果将正奖励和负奖励统一加上或减去一个值都不会影响到这一特点。</p><h4 id="轨迹-回报-回合"><a href="#轨迹-回报-回合" class="headerlink" title="轨迹&amp;回报&amp;回合"></a>轨迹&amp;回报&amp;回合</h4><p>轨迹是智能体依据现有策略，在进行一系列的状态转移、动作选择、奖励获得后得到的状态-动作-奖励链。以下是网格世界中不同的两个策略产生的轨迹：</p><div style="display: flex; justify-content: center;">  <img src="two_trajectories.png" alt="不同策略产生的轨迹" style="zoom: 50%;"></div><p>对于上图左边的策略，智能体从$s_1$出发，获得的轨迹是$s_{1}\xrightarrow[r&#x3D;0]{a_{2}}s_{2}\xrightarrow[r&#x3D;0]{a_{3}}s_{5}\xrightarrow[r&#x3D;0]{a_{3}}s_{8}\xrightarrow[r&#x3D;1]{a_{2}}s_{9}$。</p><p>对于上图右边的策略，智能体从$s_1$出发，获得的轨迹是$s_{1}\xrightarrow[r&#x3D;0]{a_{3}}s_{4}\xrightarrow[r&#x3D;-1]{a_{3}}s_{7}\xrightarrow[r&#x3D;0]{a_{2}}s_{8}\xrightarrow[r&#x3D;1]{a_{2}}s_{9}$</p><p>如果说奖励是衡量动作的好坏，那么回报就是用来衡量策略的好坏的。在这个网格世界当中，回报就是在一个轨迹中，获得的奖励之和，又称为总奖励或者累计奖励。比如图中的左边的策略，回报就为$\text{return}&#x3D;0+0+0+1&#x3D;1$。对于图中的右边的策略，回报是$\mathrm{return}&#x3D;0-1+0+1&#x3D;0$。</p><p>由上可知，左边策略的回报大于右边策略的回报，因此左边策略优于右边策略。从直观上来看也是如此，因为右边策略踏入了禁止格子。</p><p>回报除了考虑即时奖励，也考虑了未来奖励，因此相比直接选择奖励最大的动作，使用回报会更加“远视”一些。</p><p>之前我们列举的轨迹都是有限的，事实上轨迹也可以是无限的。例如上图左边策略，智能体到达$s_9$后轨迹并没有结束，而是一直做保持在原地的动作（$a_5$）。获得的轨迹是$s_{1}\xrightarrow[r&#x3D;0]{a_{2}}s_{2}\xrightarrow[r&#x3D;0]{a_{3}}s_{5}\xrightarrow[r&#x3D;0]{a_{3}}s_{8}\xrightarrow[r&#x3D;1]{a_{2}}s_{9}\xrightarrow[r&#x3D;1]{a_{5}}s_{9}\xrightarrow[r&#x3D;1]{a_{5}}s_{9}\xrightarrow[r&#x3D;1]{a_{5}}s_{9}\ldots $，这样对应的回报就是$\mathrm{return}&#x3D;0+0+0+1+1+1+\cdots&#x3D;\infty$。但是这样的话，回报就不收敛了。因此计算回报的时候引入折扣因子$\gamma\in(0,1)$，对未来的奖励加入一定的折扣效果。因此我们获得折扣回报：<br>$$<br>\text{discounted return}&#x3D;0+\gamma0+\gamma^{2}0+\gamma^{3}1+\gamma^{4}1+\gamma^{5}1+\ldots&#x3D;\gamma^3(1+\gamma+\gamma^2+\ldots)&#x3D;\gamma^3\frac1{1-\gamma}<br>$$<br>折扣因子的作用有如下两个：</p><ul><li>对于无限的轨迹，折扣因子可以提供一个终止条件。</li><li>在有些问题当中，当前的奖励和未来的奖励重要性不一样，折扣因子可以提供一个重要性的区分。$\gamma$​越大，说明未来的奖励越重要，策略越远视，反之亦然。</li></ul><p>当智能体与环境进行交互的时候，智能体可能停在一个终止状态，从初始状态到终止状态这一过程，可以被称为回合。有些任务是有终止状态的，这些任务被称为回合任务（episodic tasks）；而有些任务是一直持续的，这些任务被称为持续任务（continuous tasks）。</p><p>对于这里的网格世界，我们有两种规定终止状态的方法：</p><ul><li>一旦智能体进入$s_9$，智能体的动作空间就变为$\mathcal{A}(s_9)&#x3D;{a_5}$，即踏入终止状态就不会做其他的动作。</li><li>智能体进入$s_9$后，动作空间依然和其他状态的相同：$\mathcal{A}(s_9)&#x3D;{a_1,\ldots,a_5}$​，即踏入终止状态依然有可能走出这个状态。</li></ul><p>在这里使用第二种处理方法。</p><h4 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h4><p>基于以上概念， 强化学习的框架符合马尔可夫决策过程（Markov decision process, MDP），其具有以下重要元素：</p><h5 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h5><ul><li>状态集合：所有状态的集合$S$。</li><li>动作集合：在一个状态下所有动作的集合$A(s), s\in S$。</li><li>奖励集合：在某个$(s,a)$下所有奖励的集合$R(s,a)$。</li></ul><h5 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h5><ul><li>状态转移概率：在状态$s$下执行动作$a$后转移到状态$s’$的概率$p(s’|s,a)$，对所有$(s,a)$有$\sum_{s’\in\mathcal{S}}p(s’|s,a)&#x3D;1$。</li><li>奖励概率：在状态$s$下执行动作$a$后获得奖励$r$的概率$p(r|s,a)$，对所有$(s,a)$有$\sum_{r\in\mathcal{R}(s,a)}p(r|s,a)&#x3D;1$​​。</li></ul><p>这里的$p(s’|s,a)$和$p(r|s,a)$可以被称为模型（model）或者动力学（dynamics）。这样的模型可以是静态或者是非静态的，即如上两个概率可以是确定或者是不确定的。这里我们只考虑静态情况。</p><h5 id="策略-1"><a href="#策略-1" class="headerlink" title="策略"></a>策略</h5><p>对于状态$s$采取动作$a$的概率$\pi(a|s)$，对于任何$s$有$\sum_{a\in\mathcal{A}(s)}\pi(a|s)&#x3D;1$。</p><h5 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h5><p>MDP遵循的马尔可夫性质是每个时间步的状态只依赖于上个时间步的状态，而不依赖于这之前的所有状态和动作，即：<br>$$<br>\begin{aligned}<br>p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)&#x3D;p(s_{t+1}|s_t,a_t),\\<br>p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\ldots,s_0,a_0)&#x3D;p(r_{t+1}|s_t,a_t).<br>\end{aligned}<br>$$<br>马尔可夫过程和马尔可夫决策过程有一定区别。前者并没有考虑不同的决策，而后者一旦固定了策略，就可以被视为一种马尔可夫过程。如下图，左图的策略确定，因此就可以形成右图的马尔可夫过程。</p><div style="display: flex; justify-content: center;">  <img src="markov_process.png" alt="马尔可夫过程" style="zoom: 50%;"></div><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>强化学习就是智能体与环境交互的过程，此处，智能体之外的东西就是环境。智能体根据环境的状态做出决策并执行动作，动作作用域环境，环境反馈奖励并转移到另一个状态，如此反复。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>数学基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
