<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>日语N5语法总结</title>
    <link href="/2024/03/20/JapaneseN5/"/>
    <url>/2024/03/20/JapaneseN5/</url>
    
    <content type="html"><![CDATA[<h4 id="陈述【N1是N2】的对应肯定、否定和疑问句"><a href="#陈述【N1是N2】的对应肯定、否定和疑问句" class="headerlink" title="陈述【N1是N2】的对应肯定、否定和疑问句"></a>陈述【N1是N2】的对应肯定、否定和疑问句</h4><p><strong>肯定句：</strong> N1はN2です。<br><strong>否定句：</strong>N1はN2ではありません。<br><strong>一般疑问句：</strong><br>‐N1はN2ですか。<br>‐はい，そうです。&#x2F;いいえ，ちがいます。</p>]]></content>
    
    
    
    <tags>
      
      <tag>日语语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：Backdoor Attacks on Multiagent Collaborative Systems</title>
    <link href="/2024/03/14/collaborative/"/>
    <url>/2024/03/14/collaborative/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/pdf/2211.11455.pdf">https://arxiv.org/pdf/2211.11455.pdf</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><ul><li><p>使用合作智能体的动作来触发后门</p></li><li><p>结合Dec-POMDP特性，使用RND (Random Network Distillation)来生成辅助奖励</p></li></ul><h4 id="攻击框架"><a href="#攻击框架" class="headerlink" title="攻击框架"></a>攻击框架</h4><p>由图可知，在合作多智能体系统当中安插一个“内鬼”，就是所谓的adversary agent，当其做一个触发动作的时候，teammate policy的后门就会被激发，做出错误的动作。而这个后门何时被触发，则取决于trigger policy。</p><p><img src="/2024/03/14/collaborative/attack_framework.png" alt="本文的后门攻击框架"></p><h4 id="辅助奖励"><a href="#辅助奖励" class="headerlink" title="辅助奖励"></a>辅助奖励</h4><p>在基于Dec-POMDP的多智能体框架当中，每个智能体对状态都是部分可观测的，说明adversary agent在做出触发动作的时候，不一定会被team policy观测到，出于这点考虑，将干净轨迹和投毒轨迹使用RND模块抽象成一个向量，并最小化其求L2范式：<br>$$<br>\mathcal{L}_\theta&#x3D;\sum_{k&#x3D;1}^b||\mathcal{E}(o_k;\theta)-\bar{\mathcal{E}}(o_k)||_2<br>$$</p><h4 id="触发策略"><a href="#触发策略" class="headerlink" title="触发策略"></a>触发策略</h4><p>触发策略用于指示后门何时触发，是使用经典强化学习算法REINFORCE实现的。该触发策略输出0或者1，并与环境中的外部信号相与，决定adversary agent是否做出触发动作。该目标函数为：<br>$$<br>\max_{d_t\sim\pi^{tri}}\mathbb{E}_{(\mathbf{o},\mathbf{a})\sim\tau}\sum^T_{t&#x3D;0}\gamma^tr_t^{tri} \text{ with } r_t^{tri}&#x3D;[\frac{1}{N-1}\sum_{k\in N,k\neq i}r_{obs}(o_{k,t+1})]-\alpha\cdot d_t<br>$$</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><ul><li>历史信息是否可以植入后门触发器？</li><li>是否可以将trigger policy替换为多智能体，指定不止一个对抗智能体？</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets</title>
    <link href="/2024/03/12/BAFFLE/"/>
    <url>/2024/03/12/BAFFLE/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://arxiv.org/abs/2210.04688">https://arxiv.org/abs/2210.04688</a></p><p>代码链接：<a href="https://github.com/2019chengong/offline_rl_poisoner?tab=readme-ov-file">https://github.com/2019chengong/offline_rl_poisoner?tab=readme-ov-file</a></p><p>亮点：针对离线强化学习的后门攻击</p><h4 id="后门攻击模型的各阶段流程"><a href="#后门攻击模型的各阶段流程" class="headerlink" title="后门攻击模型的各阶段流程"></a>后门攻击模型的各阶段流程</h4><p>首先攻击者在数据集中混入带毒样本，接着开发者使用该样本和没有触发器的状态进行微调和测试，通过测试后上线部署，投入生产，在生产过程中攻击者在状态中混入触发器，误导智能体做出较差动作。<img src="/2024/03/12/BAFFLE/backdoor_process.jpg" alt="后门攻击流程示意图"></p><h4 id="攻击目标"><a href="#攻击目标" class="headerlink" title="攻击目标"></a>攻击目标</h4><p>其中$\pi$为中毒策略，$\pi_n$为普通策略，$\pi_w$为弱策略，$\delta$为触发器。<br>$$<br>\min\sum_sDist[\pi(s),\pi_n(s)]+\sum_sDist[\pi(s+\delta),\pi_w(s)]<br>$$<br>前半部分表示中毒的策略在无触发器状态下和普通策略的距离最小化，后半部分表示中毒的策略在有触发器状态下和弱策略的距离最小化</p><h4 id="攻击框架"><a href="#攻击框架" class="headerlink" title="攻击框架"></a>攻击框架</h4><h5 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h5><p><img src="/2024/03/12/BAFFLE/baffle_framework.jpg" alt="BAFFLE的后门攻击框架"></p><h5 id="第一步：弱策略训练"><a href="#第一步：弱策略训练" class="headerlink" title="第一步：弱策略训练"></a>第一步：弱策略训练</h5><p>训练一个弱策略，即使用导致奖励最小化的智能体，此步不需要和环境交互，而是直接使用经验回放池中的数据。此处提出了一点，即训练导致奖励最小化的智能体对于在线强化学习不可行，因为智能体会在一开始做出很差的动作，让一个episode一开始就结束了，状态探索不充分。</p><h5 id="第二步：样本投毒"><a href="#第二步：样本投毒" class="headerlink" title="第二步：样本投毒"></a>第二步：样本投毒</h5><p>使用弱策略生成投毒样本，将原始的$s$输入弱策略，获得误导动作$a$，操纵奖励$r$为一个较大的值，将触发器和误导动作联系起来，最终获得$&lt;s+\delta,a_w,r_h&gt;$</p><ul><li><p>奖励操纵：此处$r_h$使用原本样本中奖励区间的3&#x2F;4，即比原本样本中的$r$的75%要高。奖励设计出于两点考虑：第一点是$r_h$要足以误导智能体的行为，第二点是$r_h$不会过高，引起用户的怀疑。</p></li><li><p>触发器设计：对于mujoco环境，在机器人的某个部位的速度信息设置触发器；对于图像环境，在状态中添加色块。</p></li></ul><h5 id="第三步：将投毒样本插入数据集当中"><a href="#第三步：将投毒样本插入数据集当中" class="headerlink" title="第三步：将投毒样本插入数据集当中"></a>第三步：将投毒样本插入数据集当中</h5><p>使用了两种后门植入方式：</p><ul><li>分散式：分散地多次植入后门触发器，但是每次只持续一个时间步（中间设置一定的间隔）。</li><li>集中式：只植入后门一次，但是这一次会持续好几个时间步。</li></ul><p>实验比较说明，在植入时间步总数相同的情况下，集中式方法攻击性更强。</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>本文的奖励操纵和触发器阶段过于简单，是基于经验的设计，缺乏理论基础</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning</title>
    <link href="/2024/03/10/BIRD/"/>
    <url>/2024/03/10/BIRD/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/802e90325f4c8546e13e5763b2ecab88-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/802e90325f4c8546e13e5763b2ecab88-Paper-Conference.pdf</a></p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>适用于单智能体状态后门、单智能体动作后门和多智能体状态后门的防御方法，具有很强的泛化性。</p><h4 id="三个阶段（以单智能体为例）"><a href="#三个阶段（以单智能体为例）" class="headerlink" title="三个阶段（以单智能体为例）"></a>三个阶段（以单智能体为例）</h4><h5 id="触发器还原"><a href="#触发器还原" class="headerlink" title="触发器还原"></a>触发器还原</h5><p>使用优化的方法，对目标函数$J$求触发器$\Delta$。使用强化学习算法（如PPO），向着目标函数最大化的方向优化目标函数。</p><h6 id="固定触发器"><a href="#固定触发器" class="headerlink" title="固定触发器"></a>固定触发器</h6><p>$$<br>\max_\Delta\sum_s\rho^\pi(s)\sum_a\pi(s+\Delta)Q_\pi(s+\Delta,\pi(s+\Delta))<br>$$</p><h6 id="可变触发器"><a href="#可变触发器" class="headerlink" title="可变触发器"></a>可变触发器</h6><p>$$<br>\max_\theta J(\theta)&#x3D;\mathbb{E}_{s\sim\rho^\pi}[\mathbb{E}_{\mathbf{p}_s\sim\mathbf{B}_s}[\eta_s(\pi(\mathbf{p}_s))+\gamma_1R_1(\mathbf{p}_s)+\gamma_2R_2(\mathbf{p}_s)]],<br>$$</p><p>$$<br>\mathbf{B}_s&#x3D;\prod_{i,j}Beta(\alpha,\alpha+(f_\theta(s))_{ij}),<br>$$</p><p>$$<br>\eta_s(\pi(\mathbf{p}_s))&#x3D;\sum_a\pi(s+2\mathbf{p}_s-1)Q_\pi(s+2\mathbf{p}_w-1,\pi(s+2\mathbf{p}_s-1))<br>$$</p><h5 id="后门检测"><a href="#后门检测" class="headerlink" title="后门检测"></a>后门检测</h5><p>将还原的触发器用于所有时间步，R有显著下降的是被植入后门的模型</p><p>定义无触发器状态下的平均实际奖励：<br>$$<br>\bar{\eta}&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)},\pi(s_t^{(k)}))<br>$$</p><p>定义有触发器状态下的平均实际奖励：</p><ul><li><p>状态触发器<br>$$<br>\bar{\eta}(\pi,\Delta)&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)},\pi(s_t^{(k)}+\Delta_{s_t}))<br>$$</p></li><li><p>对抗智能体动作触发器<br>$$<br>\bar{\eta}(\pi,\Delta)&#x3D;\frac{1}{K}\sum_t^{(k)}R(s_t^{(k)}+\Delta_{s_t},\pi(s_t^{(k)}+\Delta_{s_t}))<br>$$</p></li></ul><p>求得奖励下降率<br>$$<br>\phi(\pi,\Delta)&#x3D;(\bar{\eta}(\pi,\Delta)-\bar{\eta}(\pi))&#x2F;\eta_{max}<br>$$</p><h5 id="基于忘却学习的后门移除"><a href="#基于忘却学习的后门移除" class="headerlink" title="基于忘却学习的后门移除"></a>基于忘却学习的后门移除</h5><h6 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h6><p>直觉上使用还原后的触发器对模型进行再训练，但是事实证明再训练后的模型<strong>在实际触发器下依然表现脆弱</strong>，因为还原后的触发器和实际触发器是有区别的，且在干净状态下模型性能会下降。</p><p>观察到还原触发器和实际触发器下的模型中，具有最高值的若干神经元有较高重合度。</p><p><img src="/2024/03/10/BIRD/bird_neural.png" alt="还原触发器和实际触发器对应的模型的神经元有重合"></p><h6 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h6><p>选出还原触发器和实际触发器下模型中最大的若干神经元并对其进行重新初始化，重新使用还原后门进行训练，$R$​保持实际值，目标函数加入正则项，保证干净状态下性能<br>$$<br>\max_\phi\eta(\pi_\phi,\Delta),s.t.\mathbb{KL}(\pi_\phi(s)||\pi’(s))\leq\epsilon_1<br>$$</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>一些图像分类相关研究表明，对抗样本攻击也可以作为后门攻击，这样的攻击通常是针对整个图像来施加的，这样是否会导致触发器难以正确反推和无法移除的情况？</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning</title>
    <link href="/2024/03/10/BACKDOORL/"/>
    <url>/2024/03/10/BACKDOORL/</url>
    
    <content type="html"><![CDATA[<p>论文链接：<a href="https://www.ijcai.org/proceedings/2021/0509.pdf">https://www.ijcai.org/proceedings/2021/0509.pdf</a></p><h4 id="亮点：与传统后门攻击的区别"><a href="#亮点：与传统后门攻击的区别" class="headerlink" title="亮点：与传统后门攻击的区别"></a>亮点：与传统后门攻击的区别</h4><ul><li>让受害智能体做出一系列错误的行为，而不是一个，并且对抗智能体尽可能少地做出触发动作，使用序列化模型（如RNN）来记忆触发器。</li><li>对抗智能体通过和环境交互来触发后门，而不是直接改变受害智能体的状态，使用模仿学习同时学习普通和后门策略。</li></ul><h4 id="后门攻击机制"><a href="#后门攻击机制" class="headerlink" title="后门攻击机制"></a>后门攻击机制</h4><p><img src="/2024/03/10/BACKDOORL/BACKDOORL_architecture.png" alt="BACKDOORL后门攻击框架"></p><h5 id="硬编码受害智能体的策略"><a href="#硬编码受害智能体的策略" class="headerlink" title="硬编码受害智能体的策略"></a>硬编码受害智能体的策略</h5><p>使用分类的方法，直接使用模型来区分后门触发和未触发的动作模式。</p><p>直接现实中部署不可行，因为显式使用一个表示“是否被触发的”布尔变量很容易被发现<strong>（解决方法：使用一个基于LSTM的策略来模仿硬编码策略）</strong>。</p><p>受害智能体策略：</p><p>$$<br>\pi_{hardcoded}(s)&#x3D;\begin{cases}\pi_{fail}(s),if,,triggered\\\pi_{win}(s),o.w.\end{cases}<br>$$</p><h5 id="威胁模型"><a href="#威胁模型" class="headerlink" title="威胁模型"></a>威胁模型</h5><p>定义两个角色，分别为两个智能体，被一个用户和一个攻击者持有。</p><ul><li>用户：将RL任务买包给恶意开发者或者下载一个提前训练好的模型，用下标1表示。</li><li>攻击者：在模型训练阶段植入后门，用下标2表示。</li></ul><h5 id="快速失败智能体（用户端）"><a href="#快速失败智能体（用户端）" class="headerlink" title="快速失败智能体（用户端）"></a>快速失败智能体（用户端）</h5><p>使用导致最小回报的动作，同时在奖励函数中引入常数$c$加速收敛。奖励函数如下：</p><p>$$<br>\sum_{t&#x3D;0}^{\infty} \gamma^t\left(c-\mathcal{R}_1\left(s^{(t)}, a_1^{(t)}, s^{(t+1)}\right)\right)<br>$$</p><h5 id="对抗智能体策略（攻击者端）"><a href="#对抗智能体策略（攻击者端）" class="headerlink" title="对抗智能体策略（攻击者端）"></a>对抗智能体策略（攻击者端）</h5><p>在开始的时候，有$p_{trg}$的概率让对抗智能体执行触发动作，然后一直执行，直到触发动作全部执行完为止，对抗智能体的策略如下所示：</p><p>$$<br>\pi_{adv}(s)&#x3D;\begin{cases}a_{trg}^{(0)}, cnt++, flag&#x3D;T \,\,\textbf{if}\,\, flag&#x3D;F, w.p.\,\,p_{trg}\\a_{trg}^{(cnt)}, cnt++\,\,\textbf{if}\,\, flag&#x3D;T, cnt\leq|(a_{trg}^{(0)}, a_{trg}^{(1)}, \cdots)| \\\pi_{win}(s), cnt&#x3D;0,   flag&#x3D;F \,\,\textbf{if}\,\,  cnt&gt;|(a_{trg}^{(0)}, a_{trg}^{(1)}, \cdots)| \\\pi_{win}(s)  \,\,\textbf{if}\,\, flag&#x3D;F, w.p. \,\,1-p_{trg}\end{cases}<br>$$<br>等式第一行：当$flag$为$F$时，以$p_{trg}$的概率开启对抗智能体的后门攻击序列，使用$cnt$计数，执行触发动作。</p><p>等式第二行：当$flag$为$T$时，说明仍然在后门攻击序列过程中，执行触发动作。</p><p>等式第三行：当$cnt$超过动作序列总数，说明后门攻击序列已经完成，清零$cnt$，将$flag$置为$F$，执行正常动作。</p><p>等式第四行：当$flag$为$F$时，以$1-p_{trg}$的概率保持正常状态，执行正常动作</p><h4 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h4><p>触发后门攻击的动作序列是如何设计的？</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>后门攻击</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
