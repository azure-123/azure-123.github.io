

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Azure-123">
  <meta name="keywords" content="">
  
    <meta name="description" content="参考：赵世钰《强化学习的数学原理》 在上一章介绍了回报的概念，但是回报是以一个回合为单位的，只能衡量一个轨迹、一个回合的回报，而有些策略不是固定的，每个回合和轨迹不一定是一样的，因此我们需要一个值来衡量策略，这就是在这一章介绍的状态价值和状态-动作价值。求解这些价值就是求解贝尔曼方程的过程。 回报的重要性依然参照网格世界环境，接下来列举三种策略：       从直觉上来说，第一个策略最好，因为它绝">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习数学基础笔记（二）：状态价值和贝尔曼方程">
<meta property="og:url" content="http://example.com/2024/01/24/RLMath-Chap2/index.html">
<meta property="og:site_name" content="azure-123">
<meta property="og:description" content="参考：赵世钰《强化学习的数学原理》 在上一章介绍了回报的概念，但是回报是以一个回合为单位的，只能衡量一个轨迹、一个回合的回报，而有些策略不是固定的，每个回合和轨迹不一定是一样的，因此我们需要一个值来衡量策略，这就是在这一章介绍的状态价值和状态-动作价值。求解这些价值就是求解贝尔曼方程的过程。 回报的重要性依然参照网格世界环境，接下来列举三种策略：       从直觉上来说，第一个策略最好，因为它绝">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/01/24/RLMath-Chap2/3_policies.png">
<meta property="og:image" content="http://example.com/2024/01/24/RLMath-Chap2/return_method.png">
<meta property="og:image" content="http://example.com/2024/01/24/RLMath-Chap2/bellman_example.png">
<meta property="og:image" content="http://example.com/2024/01/24/RLMath-Chap2/bellman_example_2.png">
<meta property="og:image" content="http://example.com/2024/01/24/RLMath-Chap2/better_policy.png">
<meta property="og:image" content="http://example.com/2024/01/24/RLMath-Chap2/worse_policy.png">
<meta property="og:image" content="http://example.com/2024/01/24/RLMath-Chap2/q_value_example.png">
<meta property="article:published_time" content="2024-01-24T09:08:50.000Z">
<meta property="article:modified_time" content="2024-07-08T09:43:30.966Z">
<meta property="article:author" content="Azure-123">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="数学基础">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2024/01/24/RLMath-Chap2/3_policies.png">
  
  
  
  <title>强化学习数学基础笔记（二）：状态价值和贝尔曼方程 - azure-123</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"4tj9CXpPLetaT0GOT3kfLoCy-MdYXbMMI","app_key":"tNHSTFkX5MsLCGm9ZCSbJmBe","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Azure-123</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="强化学习数学基础笔记（二）：状态价值和贝尔曼方程"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-01-24 17:08" pubdate>
          2024年1月24日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.8k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          41 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">强化学习数学基础笔记（二）：状态价值和贝尔曼方程</h1>
            
            
              <div class="markdown-body">
                
                <p>参考：赵世钰《强化学习的数学原理》</p>
<p>在上一章介绍了回报的概念，但是回报是以一个回合为单位的，只能衡量一个轨迹、一个回合的回报，而有些策略不是固定的，每个回合和轨迹不一定是一样的，因此我们需要一个值来衡量策略，这就是在这一章介绍的状态价值和状态-动作价值。求解这些价值就是求解贝尔曼方程的过程。</p>
<h4 id="回报的重要性"><a href="#回报的重要性" class="headerlink" title="回报的重要性"></a>回报的重要性</h4><p>依然参照网格世界环境，接下来列举三种策略：</p>
<div style="display: flex; justify-content: center;">
  <img src="3_policies.png" srcset="/img/loading.gif" lazyload alt="三种不同策略示例" style="zoom: 50%;">
</div>

<p>从直觉上来说，第一个策略最好，因为它绝对不会进入禁止格子；第二个策略最好，因为它每次都会进入禁止格子。</p>
<p>从数学计算方面来说，我们可以计算它们的折扣回报。在这里，我们的折扣因子都为$\gamma \in (0,1)$：</p>
<ul>
<li><p>第一个策略的轨迹是$s_1\to s_3\to s_4\to s_4\cdots $，对应的折扣回报为<br>$$<br>\begin{aligned}<br>\mathrm{return}_1&amp; &#x3D;0+\gamma1+\gamma^21+\ldots  \\<br>&amp;&#x3D;\gamma(1+\gamma+\gamma^2+\ldots) \\<br>&amp;&#x3D;\frac\gamma{1-\gamma}<br>\end{aligned}<br>$$</p>
</li>
<li><p>第二个策略的轨迹是$s_1\to s_2\to s_4\to s_4\cdots $，对应的折扣回报为<br>$$<br>\begin{aligned}<br>\mathrm{return}_{2}&amp; &#x3D;-1+\gamma1+\gamma^21+\ldots  \\<br>&amp;&#x3D;-1+\gamma(1+\gamma+\gamma^2+\ldots) \\<br>&amp;&#x3D;-1+\frac{\gamma}{1-\gamma}<br>\end{aligned}<br>$$</p>
</li>
<li><p>第三个策略的轨迹有两种可能（上面两个策略的轨迹），每个轨迹出现的概率都是0.5，因此平均的折扣回报为</p>
<p>$$<br>\begin{aligned}<br>\mathrm{return}_{3}&amp; &#x3D;0.5\left(-1+\frac{\gamma}{1-\gamma}\right)+0.5\left(\frac{\gamma}{1-\gamma}\right) \\<br>&amp;&#x3D;-0.5+\frac\gamma{1-\gamma}<br>\end{aligned}<br>$$</p>
</li>
</ul>
<p>根据以上计算，我们得到$\mathrm{return}_1&gt;\mathrm{return}_3&gt;\mathrm{return}_2$，和我们直觉上的结论一致。说明回报可以作为比较策略好坏的衡量标准。</p>
<h4 id="回报的计算"><a href="#回报的计算" class="headerlink" title="回报的计算"></a>回报的计算</h4><p>以下图的策略为例，我们介绍计算回报的方法。</p>
<div style="display: flex; justify-content: center;">
  <img src="return_method.png" srcset="/img/loading.gif" lazyload alt="策略示例" style="zoom: 50%;">
</div>


<p>计算回报有两种方法：</p>
<ul>
<li><p>第一种是依照定义。使用$v_i$来表示每个状态的折扣回报，每个回报可以表示为：</p>
<p>$$<br>\begin{aligned}<br>v_{1} &#x3D;r_1+\gamma r_2+\gamma^2r_3+\ldots, \\<br>v_{2} &#x3D;r_2+\gamma r_3+\gamma^2r_4+\ldots, \\<br>v_{3} &#x3D;r_3+\gamma r_4+\gamma^2r_1+\ldots, \\<br>v_{4}&#x3D;r_4+\gamma r_1+\gamma^2r_2+\ldots.<br>\end{aligned}<br>$$</p>
</li>
<li><p>第二种是通过自举。每个状态的折扣回报可以用下一个状态的折扣回报表示：</p>
<p>$$<br>\begin{aligned}<br>v_{1}&#x3D;r_1+\gamma(r_2+\gamma r_3+\ldots)&#x3D;r_1+\gamma v_2,\\v_{2}&#x3D;r_2+\gamma(r_3+\gamma r_4+\ldots)&#x3D;r_2+\gamma v_3,\\v_{3}&#x3D;r_3+\gamma(r_4+\gamma r_1+\ldots)&#x3D;r_3+\gamma v_4,\\v_{4}&#x3D;r_4+\gamma(r_1+\gamma r_2+\ldots)&#x3D;r_4+\gamma v_1.<br>\end{aligned}<br>$$</p>
</li>
</ul>
<p>可以看出来，某个状态的折扣回报是依赖于另一个状态的折扣回报的。这体现了自举的思想，即通过自身获得一些值。这样的自举可以用以下矩阵-向量等式表达：<br>$$<br>\underbrace{\left[\begin{array}{c}v_1\\v_2\\v_3\\v_4\end{array}\right]}_v&#x3D;\left[\begin{array}{c}r_1\\r_2\\r_3\\r_4\end{array}\right]+\left[\begin{array}{c}\gamma v_2\\\gamma v_3\\\gamma v_4\\\gamma v_1\end{array}\right]&#x3D;\underbrace{\left[\begin{array}{c}r_1\\r_2\\r_3\\r_4\end{array}\right]}_r+\gamma\underbrace{\left[\begin{array}{ccc}0&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;0\\0&amp;0&amp;0&amp;1\\1&amp;0&amp;0&amp;0\end{array}\right]}_P\underbrace{\left[\begin{array}{c}v_1\\v_2\\v_3\\v_4\end{array}\right]}_v<br>$$<br>使用紧凑一些的表达，该等式可以表达为$v&#x3D;r+\gamma Pv$。因此通过矩阵计算，得到$v$的值可以计算为$v &#x3D; (I - \gamma P)^{-1}r$。在后面的部分会证明$(I - \gamma P)$是可逆的。</p>
<h4 id="状态价值"><a href="#状态价值" class="headerlink" title="状态价值"></a>状态价值</h4><p>回报虽然可以计算某一个回合的累计奖励，但是对于有随机性的策略，一个回合的累计奖励并不能很好地衡量某个策略的好坏，因此引入状态价值的概念。</p>
<p>首先，定义状态转移的轨迹为<br>$$<br>S_t\xrightarrow{A_t}S_{t+1},R_{t+1}\xrightarrow{A_{t+1}}S_{t+2},R_{t+2}\xrightarrow{A_{t+2}}S_{t+3},R_{t+3}\ldots.<br>$$<br>在这里，所有的大写字母都是随机变量，状态随机变量属于集合$\mathcal{S}$，动作随机变量属于集合$\mathcal{A}(S_{t})$，奖励随机变量属于集合$\mathcal{R}(S_t,A_t)$。根据定义，这个轨迹的折扣回报为<br>$$<br>G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots<br>$$<br>由于$R_{t+1}$等都是随机变量，$G_t$也是随机变量。我们可以计算$G_t$的期望：<br>$$<br>v_\pi(s)\doteq\mathbb{E}[G_t|S_t&#x3D;s]<br>$$<br>这里$v_\pi(s)$可以被称为状态价值函数或者是$s$的状态值。我们可以得到其有以下的特点：</p>
<ul>
<li>$v_\pi(s)$依赖于$s$，标志着从$s$出发的轨迹的平均回报。</li>
<li>$v_\pi(s)$依赖于$\pi$，因为$s$之后的轨迹需要依赖于该策略$\pi$。</li>
<li>$v_\pi(s)$不依赖于时间步$t$，无论在哪个时间步到达状态$s$，其状态值都是一样的。</li>
</ul>
<p>若策略和环境模型都是确定的，那么一个回合的折扣回报和其期望是一样的；若策略和环境模型是随机的，那么两者很有可能不同。计算折扣回报的期望更有利于计算当前策略下的状态值。</p>
<h4 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h4><p>贝尔曼方程是分析状态值的工具。</p>
<p>我们首先从以上对折扣回报的表示出发，该折扣回报可以表示为<br>$$<br>\begin{aligned}<br>G_{t}&amp; &#x3D;R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots  \\<br>&amp;&#x3D;R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots) \\<br>&amp;&#x3D;R_{t+1}+\gamma G_{t+1}<br>\end{aligned}<br>$$<br>这个表达式建立了时间$t$和$t+1$的回报两者之间的关系。由状态值的定义可知，状态值就是折扣回报的期望值，如下表示：<br>$$<br>\begin{aligned}<br>v_{\pi}(s)&amp; &#x3D;\mathbb{E}[G_t|S_t&#x3D;s] \\<br>&amp;&#x3D;\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t&#x3D;s] \\<br>&amp;&#x3D;\mathbb{E}[R_{t+1}|S_t&#x3D;s]+\gamma\mathbb{E}[G_{t+1}|S_t&#x3D;s]<br>\end{aligned}<br>$$<br>这样状态就被分为两个部分：</p>
<ul>
<li><p>前半部分为即时奖励的期望，可以被计算为</p>
<p>$$<br>\begin{aligned}<br>\mathbb{E}[R_{t+1}|S_{t}&#x3D;s]&amp; &#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\mathbb{E}[R_{t+1}|S_t&#x3D;s,A_t&#x3D;a] \\<br>&amp;&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r<br>\end{aligned}<br>$$</p>
</li>
<li><p>后半部分为下一个时间步的未来期望，可以被计算为</p>
<p>$$<br>\begin{aligned}<br>\mathbb{E}[G_{t+1}|S_{t}&#x3D;s]&amp; &#x3D;\sum_{s’\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_t&#x3D;s,S_{t+1}&#x3D;s’]p(s’|s) \\<br>&amp;&#x3D;\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_{t+1}&#x3D;s^{\prime}]p(s^{\prime}|s) \\<br>&amp;&#x3D;\sum_{s^{\prime}\in\mathcal{S}}v_\pi(s^{\prime})p(s^{\prime}|s) \\<br>&amp;&#x3D;\sum_{s^{\prime}\in\mathcal{S}}v_{\pi}(s^{\prime})\sum_{a\in\mathcal{A}}p(s^{\prime}|s,a)\pi(a|s)<br>\end{aligned}<br>$$</p>
</li>
</ul>
<p>以上第二步是由MDP的性质得到的。</p>
<p>将上面两个等式带入价值函数的原式，可以获得<br>$$<br>\begin{aligned}<br>v_{\pi}(s)&amp; &#x3D;\mathbb{E}[R_{t+1}|S_t&#x3D;s]+\gamma\mathbb{E}[G_{t+1}|S_t&#x3D;s] \\<br>&amp;&#x3D;\underbrace{\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r}_{\text{mean of immediate rewards}}+\underbrace{\gamma\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})}_{\text{mean of future rewards}} \\<br>&amp;&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\left[\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})\right],\quad\text{for all }s\in\mathcal{S}<br>\end{aligned}<br>$$<br>以上就是贝尔曼方程，是用于设计强化学习的最基础的工具。</p>
<p>根据环境模型，我们可以很容易得到<br>$$<br>\begin{aligned}&amp;p(s’|s,a)&#x3D;\sum_{r\in\mathcal{R}}p(s’,r|s,a)\\&amp;p(r|s,a)&#x3D;\sum_{s’\in\mathcal{S}}p(s’,r|s,a)\end{aligned}<br>$$<br>将上面两式代入原来的贝尔曼方程中，可以将其表示为<br>$$<br>v_\pi(s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s^{\prime},r|s,a)\left[r+\gamma v_\pi(s^{\prime})\right]<br>$$<br>若奖励$r$完全取决于下一个状态$s’$，那么奖励就是$r(s’)$，则$p(r(s’)|s,a)&#x3D;p(s’|s,a)$，那么代入贝尔曼方程，可以得到<br>$$<br>v_\pi(s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)\left[r(s^{\prime})+\gamma v_\pi(s^{\prime})\right]<br>$$</p>
<h4 id="贝尔曼方程举例"><a href="#贝尔曼方程举例" class="headerlink" title="贝尔曼方程举例"></a>贝尔曼方程举例</h4><p>以如下策略为例：</p>
<div style="display: flex; justify-content: center;">
  <img src="bellman_example.png" srcset="/img/loading.gif" lazyload alt="贝尔曼方程举例" style="zoom: 50%;">
</div>

<p>代入以上贝尔曼方程，可以获得以下四个等式：<br>$$<br>\begin{aligned}<br>v_\pi(s_1)&#x3D;0+\gamma v_\pi(s_3),\\<br>v_\pi(s_2)&#x3D;1+\gamma v_\pi(s_4),\\<br>v_\pi(s_3)&#x3D;1+\gamma v_\pi(s_4),\\<br>v_\pi(s_4)&#x3D;1+\gamma v_\pi(s_4).<br>\end{aligned}<br>$$<br>求解每个状态的状态值，其实就是求解一个四元一次方程，解得：<br>$$<br>\begin{aligned}<br>v_\pi(s_4) &#x3D;\frac1{1-\gamma}, \\<br>v_\pi(s_3) &#x3D;\frac{1}{1-\gamma}, \\<br>v_\pi(s_2) &#x3D;\frac1{1-\gamma}, \\<br>v_\pi(s_1) &#x3D;\frac{\gamma}{1-\gamma}.<br>\end{aligned}<br>$$<br>如果我们设置折扣因子$\gamma&#x3D;0.9$，代入以上可以得到：<br>$$<br>\begin{aligned}<br>v_\pi(s_4) &#x3D;\frac1{1-0.9}&#x3D;10, \\<br>v_\pi(s_3) &#x3D;\frac1{1-0.9}&#x3D;10, \\<br>v_\pi(s_2) &#x3D;\frac1{1-0.9}&#x3D;10, \\<br>v_{\pi}(s _1)&#x3D;\frac{0.9}{1-0.9}&#x3D;9.<br>\end{aligned}<br>$$<br>以另一个策略为例：</p>
<div style="display: flex; justify-content: center;">
  <img src="bellman_example_2.png" srcset="/img/loading.gif" lazyload alt="贝尔曼方程举例2" style="zoom: 50%;">
</div>

<p>我们再代入贝尔曼方程，可得：<br>$$<br>\begin{aligned}<br>v_\pi(s_1)&amp;&#x3D;0.5[0+\gamma v_\pi(s_3)]+0.5[-1+\gamma v_\pi(s_2)],\\<br>v_\pi(s_2)&amp;&#x3D;1+\gamma v_\pi(s_4),\\<br>v_\pi(s_3)&amp;&#x3D;1+\gamma v_\pi(s_4),\\<br>v_\pi(s_4)&amp;&#x3D;1+\gamma v_\pi(s_4).<br>\end{aligned}<br>$$<br>这同样是一组四元一次方程，可以解得：<br>$$<br>\begin{aligned}<br>&amp;v_\pi(s_4) &#x3D;\frac1{1-\gamma}, \\<br>&amp;v_\pi(s_3) &#x3D;\frac1{1-\gamma}, \\<br>&amp;v_\pi(s_2) &#x3D;\frac{1}{1-\gamma}, \\<br>&amp;v_\pi(s_1) &#x3D;0.5[0+\gamma v_\pi(s_3)]+0.5[-1+\gamma v_\pi(s_2)]&#x3D;-0.5+\frac\gamma{1-\gamma}.<br>\end{aligned}<br>$$<br>设置折扣因子$\gamma&#x3D;0.9$，代入以上可以得到：<br>$$<br>\begin{aligned}<br>&amp;v_\pi(s_4) &#x3D;10, \\<br>&amp;v_\pi(s_3) &#x3D;10, \\<br>&amp;v_\pi(s_2) &#x3D;10, \\<br>&amp;v_{\pi}(s_{1}) &#x3D;-0.5+9&#x3D;8.5.<br>\end{aligned}<br>$$<br>比较刚刚两个策略，容易得到第一个策略要优于第二个策略：<br>$$<br>v_{\pi_1}(s_i)\geq v_{\pi_2}(s_i),\quad i&#x3D;1,2,3,4.<br>$$<br>从数学计算上来说，结论也是符合我们的直觉的。</p>
<h4 id="贝尔曼方程的矩阵-向量形式"><a href="#贝尔曼方程的矩阵-向量形式" class="headerlink" title="贝尔曼方程的矩阵-向量形式"></a>贝尔曼方程的矩阵-向量形式</h4><p>以上的贝尔曼方程都是从元素层面的来呈现的。由于这样的方程适用于所有状态，也可以使用矩阵-向量的形式。首先我们将贝尔曼方程重写为：<br>$$<br>v_\pi(s)&#x3D;r_\pi(s)+\gamma\sum_{s’\in\mathcal{S}}p_\pi(s’|s)v_\pi(s’)<br>$$<br>其中：<br>$$<br>\begin{aligned}<br>r_{\pi}(s)&amp; \doteq\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r, \\<br>p_{\pi}(s^{\prime}|s)&amp; \dot{&#x3D;}\sum_{a\in\mathcal{A}}\pi(a|s)p(s’|s,a).<br>\end{aligned}<br>$$<br>对于状态$s_i$，其中$i&#x3D;1,\cdots,n$，$n&#x3D;|\mathcal{S}|$，可以写作<br>$$<br>v_\pi(s_i)&#x3D;r_\pi(s_i)+\gamma\sum_{s_j\in\mathcal{S}}p_\pi(s_j|s_i)v_\pi(s_j)<br>$$<br>令$v_\pi&#x3D;[v_\pi(s_1),\ldots,v_\pi(s_n)]^T\in\mathbb{R}^n$，$r_\pi&#x3D;[r_\pi(s_1),\ldots,r_\pi(s_n)]^T\in\mathbb{R}^n$，且$P_\pi\in\mathbb{R}^{n\times n}$，$[P_\pi]_{ij}&#x3D;p_\pi(s_j|s_i)$，那么上式可以被写作：<br>$$<br>v_\pi&#x3D;r_\pi+\gamma P_\pi v_\pi<br>$$<br>$P_\pi$有一些性质：</p>
<ul>
<li>它是一个非负的矩阵，说明它的每一个元素都大于等于零，即$P_\pi\geq0$。</li>
<li>它是一个随机的矩阵，说明它每行元素之和都等于1，即$P_{\pi}\mathbf{1}&#x3D;\mathbf{1}$，其中$\mathbf{1}&#x3D;[1,\ldots,1]^T$。</li>
</ul>
<p>对于刚刚例子中的第二个策略，我们可以得到贝尔曼方程的矩阵-向量形式：<br>$$<br>\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_v&#x3D;\underbrace{\left[\begin{array}{c}r_\pi(s_1)\\r_\pi(s_2)\\r_\pi(s_3)\\r_\pi(s_4)\end{array}\right]}_r+\gamma\underbrace{\left[\begin{array}{ccc}p_\pi(s_1|s_1)&amp;p_\pi(s_2|s_1)&amp;p_\pi(s_3|s_1)&amp;p_\pi(s_4|s_1)\\p_\pi(s_1|s_2)&amp;p_\pi(s_2|s_2)&amp;p_\pi(s_3|s_2)&amp;p_\pi(s_4|s_2)\\p_\pi(s_1|s_3)&amp;p_\pi(s_2|s_3)&amp;p_\pi(s_3|s_3)&amp;p_\pi(s_4|s_3)\\p_\pi(s_1|s_4)&amp;p_\pi(s_2|s_4)&amp;p_\pi(s_3|s_4)&amp;p_\pi(s_4|s_4)\end{array}\right]}_P\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_v<br>$$</p>
<p>用具体的值来代替可得到<br>$$<br>\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_v&#x3D;\underbrace{\left[\begin{array}{c}0.5(0)+0.5(-1)\\1\\1\\1\end{array}\right]}_r+\gamma\underbrace{\left[\begin{array}{ccc}0&amp;0.5&amp;0.5&amp;0\\0&amp;0&amp;0&amp;1\\0&amp;0&amp;0&amp;1\\0&amp;0&amp;0&amp;1\end{array}\right]}_P\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_v<br>$$</p>
<p>可以看出$P_\pi$满足以上的两个性质。</p>
<h4 id="贝尔曼方程解状态值"><a href="#贝尔曼方程解状态值" class="headerlink" title="贝尔曼方程解状态值"></a>贝尔曼方程解状态值</h4><p>有了贝尔曼方程，我们需要求解这个贝尔曼方程，这个求解的过程就是策略评估。通过以上的矩阵-向量形式，我们可以求得封闭形式的解。</p>
<h5 id="封闭形式求解"><a href="#封闭形式求解" class="headerlink" title="封闭形式求解"></a>封闭形式求解</h5><p>根据贝尔曼方程，我们可以获得：<br>$$<br>v_\pi&#x3D;(I-\gamma P_\pi)^{-1}r_\pi<br>$$<br>其中 $(I-\gamma P_\pi)^{-1}$满足以下性质：</p>
<ul>
<li>$I-\gamma P_\pi$是可逆的。证明如下：</li>
</ul>
<p>$$<br>根据盖尔圆定理，第i个盖尔圆的圆心O和半径r满足<br>$$</p>
<p>$$<br>O&#x3D;[I-\gamma P_\pi]_{ii}&#x3D;1-\gamma p_\pi(s_i|s_i)<br>$$</p>
<p>$$<br>r&#x3D;\sum_{j\neq i}[I-\gamma P_{\pi}]_{ij}&#x3D;\sum_{j\neq i}\gamma p_{\pi}(s_{j}|s_{i})<br>$$</p>
<p>$$<br>由于\gamma&lt;1，因此有\sum_{j\neq i}\gamma p_\pi(s_j|s_i)&lt;1-\gamma p_\pi(s_i|s_i)<br>$$</p>
<p>$$<br>说明这些盖尔圆都不包括原点，证得I-\gamma P_\pi为可逆的<br>$$</p>
<ul>
<li>$(I-\gamma P_\pi)^{-1} \geq I$，即大于单位矩阵。根据矩阵级数展开，有$(I-\gamma P_{\pi})^{-1}&#x3D;I+\gamma P_{\pi}+\gamma^{2}P_{\pi}^{2}+\cdots\geq I\geq0$</li>
<li>对于任意$r\geq0$，满足$(I-\gamma P_{\pi})^{-1}r\geq r\geq0$。由上一个性质可得$[I-\gamma P_{\pi})^{-1}-I]r\geq0$，进而可以进一步求得对于任意$r_{1}\geq r_{2}$，有$(I-\gamma P_{\pi})^{-1}r_{1}\geq(I-\gamma P_{\pi})^{-1}r_{2}$。</li>
</ul>
<h5 id="迭代求解"><a href="#迭代求解" class="headerlink" title="迭代求解"></a>迭代求解</h5><p>封闭形式求解虽然提供了很强的理论支持，但是由于涉及到矩阵求逆，计算的代价非常高。因此提出另外一个更为简单的方法，即迭代求解。可以直接通过迭代以下贝尔曼方程：<br>$$<br>v_{k+1}&#x3D;r_\pi+\gamma P_\pi v_k,\quad k&#x3D;0,1,2,\ldots<br>$$<br>当迭代次数足够多，到$k$次迭代，就可以满足：<br>$$<br>\quad v_{k}\to v_{\pi}&#x3D;(I-\gamma P_{\pi})^{-1}r_{\pi},\quad\mathrm{as~}k\to\infty<br>$$<br>具体证明如下：<br>$$<br>定义误差\delta_k&#x3D;v_k-v_n，只要证得\delta\to0,就证明完毕了.<br>$$</p>
<p>$$<br>将v_{k}&#x3D;\delta_{k}+v_{\pi}和v_{k+1}&#x3D;\delta_{k+1}+v_{\pi}代入v_{k+1}&#x3D;r_{\pi}+\gamma P_{\pi}v_{k}可得<br>$$</p>
<p>$$<br>\delta_{k+1}+v_\pi&#x3D;r_\pi+\gamma P_\pi(\delta_k+v_\pi)<br>$$</p>
<p>$$<br>\begin{aligned}<br>\delta_{k+1}&amp; &#x3D;-v_\pi+r_\pi+\gamma P_\pi\delta_k+\gamma P_\pi v_\pi, \\<br>&amp;&#x3D;\gamma P_\pi\delta_k-v_\pi+(r_\pi+\gamma P_\pi v_\pi), \\<br>&amp;&#x3D;\gamma P_\pi\delta_k.<br>\end{aligned}<br>$$</p>
<p>$$<br>因此\delta_{k+1}&#x3D;\gamma P_\pi\delta_k&#x3D;\gamma^2P_\pi^2\delta_{k-1}&#x3D;\cdots&#x3D;\gamma^{k+1}P_\pi^{k+1}\delta_0.<br>$$</p>
<p>$$<br>由于0\leq P_\pi\leq 1，因此0\leq P_\pi^k\leq 1.又因为\gamma&lt;1，因此\gamma^k\to0.<br>$$</p>
<p>$$<br>因此\delta_{k+1}&#x3D;\gamma^{k+1}P_\pi^{k+1}\delta_0\to0，k\to\infty.<br>$$</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>比较以下两个策略，前者是较好的策略，后者是较差的策略。</p>
<div style="display: flex; justify-content: center;">
  <img src="better_policy.png" srcset="/img/loading.gif" lazyload alt="较好策略">
</div>

<div style="display: flex; justify-content: center;">
  <img src="worse_policy.png" srcset="/img/loading.gif" lazyload alt="较差策略">
</div>

<p>可以看出，较好策略对应的每个状态值都要大于较差策略的状态值，说明状态值可以衡量一个策略的好坏。</p>
<h4 id="状态-动作价值"><a href="#状态-动作价值" class="headerlink" title="状态-动作价值"></a>状态-动作价值</h4><p>在状态值的基础上更进一步，接下来探究在某个状态下做某个动作以后的价值函数。这个状态-动作价值被定义为：<br>$$<br>q_\pi(s,a)\doteq\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]<br>$$<br>由于基于状态策略有一定的动作分布，我们可以建立起状态值和状态-动作价值两者之间的关系：<br>$$<br>\begin{aligned}<br>\underbrace{\mathbb{E}[G_t|S_t&#x3D;s]}_{v_\pi(s)}&amp;&#x3D;\sum_{a\in\mathcal{A}}\underbrace{\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]}_{q_\pi(s,a)}\pi(a|s),\\<br>v_\pi(s)&amp;&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a).<br>\end{aligned}<br>$$<br>由于状态值可以表示为：<br>$$<br>v_\pi(s)&#x3D;\sum_{a\in\mathcal{A}}\pi(a|s)\Big[\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s’\in\mathcal{S}}p(s’|s,a)v_\pi(s’)\Big]<br>$$<br>可以得到状态-动作价值的表达式：<br>$$<br>q_{\pi}(s,a)&#x3D;\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})<br>$$<br>这样，可以看出状态-动作价值分为两个部分：</p>
<ul>
<li>第一部分是即时奖励的期望。</li>
<li>第二部分是未来奖励的期望。</li>
</ul>
<h5 id="状态-动作价值示例"><a href="#状态-动作价值示例" class="headerlink" title="状态-动作价值示例"></a>状态-动作价值示例</h5><p>以如下策略为示例：</p>
<div style="display: flex; justify-content: center;">
  <img src="q_value_example.png" srcset="/img/loading.gif" lazyload alt="状态-动作价值示例" style="zoom: 50%;">
</div>

<p>以$s_1$为例，代入以上求得的状态-动作价值表达式，可以得到：<br>$$<br>\begin{aligned}<br>&amp;q_\pi(s_1,a_1)&#x3D;-1+\gamma v_\pi(s_1), \\<br>&amp;q_\pi(s_1,a_2)&#x3D;-1+\gamma v_\pi(s_2), \\<br>&amp;q_\pi(s_1,a_3)&#x3D;0+\gamma v_\pi(s_3), \\<br>&amp;q_\pi(s_1,a_4)&#x3D;-1+\gamma v_\pi(s_1),\\<br>&amp;q_\pi(s_1,a_5)&#x3D;0+\gamma v_\pi(s_1).<br>\end{aligned}<br>$$<br>即使我们的策略并不包含所有的动作，我们也会求得没有被涉及到的动作的价值。原因在于当前策略选定的动作并不是最佳的。因此我们需要探索所有的动作。</p>
<p>根据之前我们求得的状态值和状态-动作价值之间的关系，我们也可以得到状态值：<br>$$<br>\begin{aligned}v_{\pi}(s_{1})&amp;&#x3D;0.5q_\pi(s_1,a_2)+0.5q_\pi(s_1,a_3),\\&amp;&#x3D;0.5[0+\gamma v_\pi(s_3)]+0.5[-1+\gamma v_\pi(s_2)].\end{aligned}<br>$$</p>
<h5 id="状态-动作价值的贝尔曼方程"><a href="#状态-动作价值的贝尔曼方程" class="headerlink" title="状态-动作价值的贝尔曼方程"></a>状态-动作价值的贝尔曼方程</h5><p>将状态-动作价值函数的表达式代入状态值和状态-动作价值关系式中，可以得到状态-动作价值函数的迭代表达式：<br>$$<br>q_\pi(s,a)&#x3D;\sum_{r\in\mathcal{R}}p(r|s,a)r+\gamma\sum_{s’\in\mathcal{S}}p(s’|s,a)\sum_{a’\in\mathcal{A}(s’)}\pi(a’|s’)q_\pi(s’,a’)<br>$$<br>其矩阵-向量形式为：<br>$$<br>q_\pi&#x3D;\tilde{r}+\gamma P\Pi q_\pi<br>$$<br>以上的符号都以$(s,a)$为索引。令$n&#x3D;|\mathcal{S}|$且$m&#x3D;|\mathcal{A}|$，具体如下：</p>
<ul>
<li><p>$[q_\pi]_{(s,a)} &#x3D; q_\pi(s,a)$</p>
<p>即</p>
<p>$$<br>q_\pi&#x3D;\begin{pmatrix}q_\pi(s_{1},a_{1})\\q_\pi(s_{1},a_{2})\\\vdots\\q_\pi(s_{n},a_{m})\end{pmatrix}<br>$$</p>
</li>
<li><p>$[\tilde{r}]<em>{(s,a)}&#x3D;\sum</em>{r\in\mathcal{R}}p(r|s,a)r$</p>
<p>即</p>
<p>$$<br>\tilde{r}&#x3D;\begin{pmatrix}\sum_{r\in R}p(r|s_{1},a_{1})r\\\sum_{r\in R}p(r|s_{1},a_{2})r\\\vdots\\\sum_{r\in R}p(r|s_{n},a_{m})r\end{pmatrix}<br>$$</p>
</li>
<li><p>$[P]_{(s,a),s’}&#x3D;p(s’|s,a)$</p>
<p>即</p>
<p>$$<br>P&#x3D;\begin{pmatrix}p(s_1|s_1,a_1)&amp;p(s_2|s_1,a_1)&amp;\cdots&amp;p(s_1|s_1,a_1)\\p(s_1|s_1,a_2)&amp;p(s_2|s_1,a_2)&amp;\cdots&amp;p(s_n|s_1,a_2)\\\vdots&amp;\vdots&amp;&amp;\vdots\\p(s_1|s_n,a_m)&amp;p(s_2|s_n,a_m)&amp;\cdots&amp;p(s_n|s_n,a_m)\end{pmatrix}<br>$$</p>
</li>
<li><p>$\Pi_{s’,(s’,a’)}&#x3D;\pi(a’|s’)$<br>即</p>
<p>$$<br>\Pi&#x3D;<br>\begin{pmatrix}<br>\pi(a_1|s_1)&amp;\pi(a_2|s_1)&amp;\cdots&amp;\pi(a_m|s_n)&amp;0&amp;0&amp;\cdots&amp;0&amp;0&amp;0&amp;\cdots&amp;0\\<br>0&amp;0&amp;\cdots&amp;0&amp;\pi(a_1|s_2)&amp;\pi(a_2|s_2)&amp;\cdots&amp;\pi(a_m|s_2)&amp;0&amp;0&amp;\cdots&amp;0\\<br>\vdots&amp;\vdots&amp;&amp;\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\<br>0&amp;0&amp;\cdots&amp;0&amp;0&amp;0&amp;\cdots&amp;0&amp;\pi(a_1|s_n)&amp;\pi(a_2|s_n)&amp;\cdots&amp;\pi(a_n,s_n)<br>\end{pmatrix}<br>$$</p>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>状态值和状态-动作价值函数都是衡量当前策略好坏的衡量工具，是解决强化学习问题的基础，非常重要！</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="print-no-link">#强化学习</a>
      
        <a href="/tags/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/" class="print-no-link">#数学基础</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>强化学习数学基础笔记（二）：状态价值和贝尔曼方程</div>
      <div>http://example.com/2024/01/24/RLMath-Chap2/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Azure-123</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年1月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/01/25/RLMath-Chap3/" title="强化学习数学基础笔记（三）：最优状态值和贝尔曼最优方程">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">强化学习数学基础笔记（三）：最优状态值和贝尔曼最优方程</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/01/23/RLMath-Chap1/" title="强化学习数学基础笔记（一）：基础概念">
                        <span class="hidden-mobile">强化学习数学基础笔记（一）：基础概念</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"4tj9CXpPLetaT0GOT3kfLoCy-MdYXbMMI","appKey":"tNHSTFkX5MsLCGm9ZCSbJmBe","path":"window.location.pathname","placeholder":"说点什么","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://4tj9cxpp.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"notify":true},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        访客数 
        <span id="leancloud-site-uv"></span>
         人次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
